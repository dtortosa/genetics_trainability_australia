
#######################################
#######################################
checking function to print nicely: header 1
#######################################
#######################################

###### checking function to print nicely: header 2 ######

## checking function to print nicely: header 3 ##

# checking function to print nicely: header 4 #

#######################################
#######################################
check behaviour run_bash
#######################################
#######################################

###### see working directory ######
/home/UGR002/dsalazar/climahealth/combat_genes


###### list files/folders there ######
03a_association_analyses.sif
03b_prs_calculation_100_iter_large_set_predictors_beep_change.out
03b_prs_calculation_100_iter_large_set_predictors_distance_change.out
03b_prs_calculation_100_iter_large_set_predictors_vo2_change.out
03b_prs_calculation_100_iter_large_set_predictors_weight_change.out
03b_prs_calculation_100_iter_small_set_predictors_beep_change.out
03b_prs_calculation_100_iter_small_set_predictors_distance_change.out
03b_prs_calculation_100_iter_small_set_predictors_vo2_change.out
03b_prs_calculation_100_iter_small_set_predictors_weight_change.out
03d_processing_results.out
03e_manhattan_plots_beep_change.out
03e_manhattan_plots_distance_change.out
03e_manhattan_plots_vo2_change.out
03e_manhattan_plots_weight_change.out
data
results
scripts


#######################################
#######################################
For phenotype weight_change, and the small dataset of covariates
#######################################
#######################################

###### initial preparations ######

## create folders for results ##


## load the phenotype data ##
                family_id AGRF code  ...  Week 1 Body Mass  weight_change
0    combat_ILGSA24-17303  0200ADMM  ...         -0.041964      -0.278883
1    combat_ILGSA24-17303  0200ASJM  ...         -1.958177       0.619534
2    combat_ILGSA24-17303  0200BHNM  ...         -0.353319       0.444192
3    combat_ILGSA24-17303  0200CBOM  ...         -2.240113       1.663390
4    combat_ILGSA24-17303  0200CDFM  ...         -1.779037      -0.005224
..                    ...       ...  ...               ...            ...
954  combat_ILGSA24-17873  8098ATDN  ...          0.031469      -0.694374
955  combat_ILGSA24-17873  8098RSJN  ...         -0.197935       0.884236
956  combat_ILGSA24-17873  8098TSAN  ...         -0.880366      -0.654869
957  combat_ILGSA24-17873  8099AJNN  ...         -0.269758       0.948065
958  combat_ILGSA24-17873  8099COSN  ...         -0.632291      -0.389843

[959 rows x 10 columns]

## specify the covariates ##
Index(['PCA1', 'PCA2', 'PCA8', 'PCA11', 'Age', 'sex_code', 'Week 1 Body Mass'], dtype='object')

## decompress bim and bed files with all sample generated after NA cleaning ##


###### prepare LDAK inputs ######

###### calculate elastic PRS ######

## run the PRS with --elastic ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--elastic ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_elastic
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--LOCO NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Constructing elastic net PRS

Will consider five values for the predictor scaling (alpha = -1, -0.75, -0.5, -0.25 and 0); to instead specify the value, use "--power" (or use "--powerfile" to provide a range of values)

Will use the default prior parameter choices (saved in the file ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_elastic.parameters); to instead specify your own, use "--parameters"

Will select the best prior parameters via cross-validation, using 0.10 randomly-picked test samples (use "--cv-proportion" to change this proportion, "--cv-samples" to explicitly specify the test samples, or "--cv-skip" to turn off cross-validation)

Will always include the LOCO polygenic contribution, regardless of their estimated accuracy

When constructing PRS, will scan the data at most 10 times (change this using "--num-scans")

All heritability estimates must be between 0.01 and 0.8000 (change the upper bound using "--max-her")

Will use either three or ten random vectors for Monte Carlo operations (decided based on the number of samples); change this number using "--num-random-vectors"

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 959 samples and 3187483 predictors (will be using 959 and 3187483)

Reading phenotypes for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 6 covariates for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_elastic.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_elastic.combined")

When performing cross-validation, will use 864 samples to train models and 95 to test their accuracy

Warning, to process the data requires 6.1 Gb; sorry this can not be reduced

Warning, to perform the analysis requires approximately 2.1 Gb; sorry, this can not be reduced

Estimating per-predictor heritabilities using Randomized Haseman-Elston Regression with 10 random vectors

Will divide the predictors into 20 partitions (change this using "--num-divides")
Will exclude chunks containing a predictor with estimated variance explained greater than 0.1043 (change this using "--max-cor")

Calculating traces for Chunk 1 of 12452
Calculating traces for Chunk 201 of 12452
Calculating traces for Chunk 401 of 12452
Calculating traces for Chunk 601 of 12452
Calculating traces for Chunk 801 of 12452
Calculating traces for Chunk 1001 of 12452
Calculating traces for Chunk 1201 of 12452
Calculating traces for Chunk 1401 of 12452
Calculating traces for Chunk 1601 of 12452
Calculating traces for Chunk 1801 of 12452
Calculating traces for Chunk 2001 of 12452
Calculating traces for Chunk 2201 of 12452
Calculating traces for Chunk 2401 of 12452
Calculating traces for Chunk 2601 of 12452
Calculating traces for Chunk 2801 of 12452
Calculating traces for Chunk 3001 of 12452
Calculating traces for Chunk 3201 of 12452
Calculating traces for Chunk 3401 of 12452
Calculating traces for Chunk 3601 of 12452
Calculating traces for Chunk 3801 of 12452
Calculating traces for Chunk 4001 of 12452
Calculating traces for Chunk 4201 of 12452
Calculating traces for Chunk 4401 of 12452
Calculating traces for Chunk 4601 of 12452
Calculating traces for Chunk 4801 of 12452
Calculating traces for Chunk 5001 of 12452
Calculating traces for Chunk 5201 of 12452
Calculating traces for Chunk 5401 of 12452
Calculating traces for Chunk 5601 of 12452
Calculating traces for Chunk 5801 of 12452
Calculating traces for Chunk 6001 of 12452
Calculating traces for Chunk 6201 of 12452
Calculating traces for Chunk 6401 of 12452
Calculating traces for Chunk 6601 of 12452
Calculating traces for Chunk 6801 of 12452
Calculating traces for Chunk 7001 of 12452
Calculating traces for Chunk 7201 of 12452
Calculating traces for Chunk 7401 of 12452
Calculating traces for Chunk 7601 of 12452
Calculating traces for Chunk 7801 of 12452
Calculating traces for Chunk 8001 of 12452
Calculating traces for Chunk 8201 of 12452
Calculating traces for Chunk 8401 of 12452
Calculating traces for Chunk 8601 of 12452
Calculating traces for Chunk 8801 of 12452
Calculating traces for Chunk 9001 of 12452
Calculating traces for Chunk 9201 of 12452
Calculating traces for Chunk 9401 of 12452
Calculating traces for Chunk 9601 of 12452
Calculating traces for Chunk 9801 of 12452
Calculating traces for Chunk 10001 of 12452
Calculating traces for Chunk 10201 of 12452
Calculating traces for Chunk 10401 of 12452
Calculating traces for Chunk 10601 of 12452
Calculating traces for Chunk 10801 of 12452
Calculating traces for Chunk 11001 of 12452
Calculating traces for Chunk 11201 of 12452
Calculating traces for Chunk 11401 of 12452
Calculating traces for Chunk 11601 of 12452
Calculating traces for Chunk 11801 of 12452
Calculating traces for Chunk 12001 of 12452
Calculating traces for Chunk 12201 of 12452
Calculating traces for Chunk 12401 of 12452

Best power is 0.0000 and estimated heritability is 0.7791

Time check: have so far spent 0.02 hours

Constructing 10 PRS using training samples
Will also make 33 MCMC REML models (using all samples)

Scan 1: estimating training effect sizes for Chunk 1 of 12452
Scan 1: estimating training effect sizes for Chunk 201 of 12452
Scan 1: estimating training effect sizes for Chunk 401 of 12452
Scan 1: estimating training effect sizes for Chunk 601 of 12452
Scan 1: estimating training effect sizes for Chunk 801 of 12452
Scan 1: estimating training effect sizes for Chunk 1001 of 12452
Scan 1: estimating training effect sizes for Chunk 1201 of 12452
Scan 1: estimating training effect sizes for Chunk 1401 of 12452
Scan 1: estimating training effect sizes for Chunk 1601 of 12452
Scan 1: estimating training effect sizes for Chunk 1801 of 12452
Scan 1: estimating training effect sizes for Chunk 2001 of 12452
Scan 1: estimating training effect sizes for Chunk 2201 of 12452
Scan 1: estimating training effect sizes for Chunk 2401 of 12452
Scan 1: estimating training effect sizes for Chunk 2601 of 12452
Scan 1: estimating training effect sizes for Chunk 2801 of 12452
Scan 1: estimating training effect sizes for Chunk 3001 of 12452
Scan 1: estimating training effect sizes for Chunk 3201 of 12452
Scan 1: estimating training effect sizes for Chunk 3401 of 12452
Scan 1: estimating training effect sizes for Chunk 3601 of 12452
Scan 1: estimating training effect sizes for Chunk 3801 of 12452
Scan 1: estimating training effect sizes for Chunk 4001 of 12452
Scan 1: estimating training effect sizes for Chunk 4201 of 12452
Scan 1: estimating training effect sizes for Chunk 4401 of 12452
Scan 1: estimating training effect sizes for Chunk 4601 of 12452
Scan 1: estimating training effect sizes for Chunk 4801 of 12452
Scan 1: estimating training effect sizes for Chunk 5001 of 12452
Scan 1: estimating training effect sizes for Chunk 5201 of 12452
Scan 1: estimating training effect sizes for Chunk 5401 of 12452
Scan 1: estimating training effect sizes for Chunk 5601 of 12452
Scan 1: estimating training effect sizes for Chunk 5801 of 12452
Scan 1: estimating training effect sizes for Chunk 6001 of 12452
Scan 1: estimating training effect sizes for Chunk 6201 of 12452
Scan 1: estimating training effect sizes for Chunk 6401 of 12452
Scan 1: estimating training effect sizes for Chunk 6601 of 12452
Scan 1: estimating training effect sizes for Chunk 6801 of 12452
Scan 1: estimating training effect sizes for Chunk 7001 of 12452
Scan 1: estimating training effect sizes for Chunk 7201 of 12452
Scan 1: estimating training effect sizes for Chunk 7401 of 12452
Scan 1: estimating training effect sizes for Chunk 7601 of 12452
Scan 1: estimating training effect sizes for Chunk 7801 of 12452
Scan 1: estimating training effect sizes for Chunk 8001 of 12452
Scan 1: estimating training effect sizes for Chunk 8201 of 12452
Scan 1: estimating training effect sizes for Chunk 8401 of 12452
Scan 1: estimating training effect sizes for Chunk 8601 of 12452
Scan 1: estimating training effect sizes for Chunk 8801 of 12452
Scan 1: estimating training effect sizes for Chunk 9001 of 12452
Scan 1: estimating training effect sizes for Chunk 9201 of 12452
Scan 1: estimating training effect sizes for Chunk 9401 of 12452
Scan 1: estimating training effect sizes for Chunk 9601 of 12452
Scan 1: estimating training effect sizes for Chunk 9801 of 12452
Scan 1: estimating training effect sizes for Chunk 10001 of 12452
Scan 1: estimating training effect sizes for Chunk 10201 of 12452
Scan 1: estimating training effect sizes for Chunk 10401 of 12452
Scan 1: estimating training effect sizes for Chunk 10601 of 12452
Scan 1: estimating training effect sizes for Chunk 10801 of 12452
Scan 1: estimating training effect sizes for Chunk 11001 of 12452
Scan 1: estimating training effect sizes for Chunk 11201 of 12452
Scan 1: estimating training effect sizes for Chunk 11401 of 12452
Scan 1: estimating training effect sizes for Chunk 11601 of 12452
Scan 1: estimating training effect sizes for Chunk 11801 of 12452
Scan 1: estimating training effect sizes for Chunk 12001 of 12452
Scan 1: estimating training effect sizes for Chunk 12201 of 12452
Scan 1: estimating training effect sizes for Chunk 12401 of 12452
Average number of iterations per chunk: 3.71

Scan 2: estimating training effect sizes for Chunk 1 of 12452
Scan 2: estimating training effect sizes for Chunk 201 of 12452
Scan 2: estimating training effect sizes for Chunk 401 of 12452
Scan 2: estimating training effect sizes for Chunk 601 of 12452
Scan 2: estimating training effect sizes for Chunk 801 of 12452
Scan 2: estimating training effect sizes for Chunk 1001 of 12452
Scan 2: estimating training effect sizes for Chunk 1201 of 12452
Scan 2: estimating training effect sizes for Chunk 1401 of 12452
Scan 2: estimating training effect sizes for Chunk 1601 of 12452
Scan 2: estimating training effect sizes for Chunk 1801 of 12452
Scan 2: estimating training effect sizes for Chunk 2001 of 12452
Scan 2: estimating training effect sizes for Chunk 2201 of 12452
Scan 2: estimating training effect sizes for Chunk 2401 of 12452
Scan 2: estimating training effect sizes for Chunk 2601 of 12452
Scan 2: estimating training effect sizes for Chunk 2801 of 12452
Scan 2: estimating training effect sizes for Chunk 3001 of 12452
Scan 2: estimating training effect sizes for Chunk 3201 of 12452
Scan 2: estimating training effect sizes for Chunk 3401 of 12452
Scan 2: estimating training effect sizes for Chunk 3601 of 12452
Scan 2: estimating training effect sizes for Chunk 3801 of 12452
Scan 2: estimating training effect sizes for Chunk 4001 of 12452
Scan 2: estimating training effect sizes for Chunk 4201 of 12452
Scan 2: estimating training effect sizes for Chunk 4401 of 12452
Scan 2: estimating training effect sizes for Chunk 4601 of 12452
Scan 2: estimating training effect sizes for Chunk 4801 of 12452
Scan 2: estimating training effect sizes for Chunk 5001 of 12452
Scan 2: estimating training effect sizes for Chunk 5201 of 12452
Scan 2: estimating training effect sizes for Chunk 5401 of 12452
Scan 2: estimating training effect sizes for Chunk 5601 of 12452
Scan 2: estimating training effect sizes for Chunk 5801 of 12452
Scan 2: estimating training effect sizes for Chunk 6001 of 12452
Scan 2: estimating training effect sizes for Chunk 6201 of 12452
Scan 2: estimating training effect sizes for Chunk 6401 of 12452
Scan 2: estimating training effect sizes for Chunk 6601 of 12452
Scan 2: estimating training effect sizes for Chunk 6801 of 12452
Scan 2: estimating training effect sizes for Chunk 7001 of 12452
Scan 2: estimating training effect sizes for Chunk 7201 of 12452
Scan 2: estimating training effect sizes for Chunk 7401 of 12452
Scan 2: estimating training effect sizes for Chunk 7601 of 12452
Scan 2: estimating training effect sizes for Chunk 7801 of 12452
Scan 2: estimating training effect sizes for Chunk 8001 of 12452
Scan 2: estimating training effect sizes for Chunk 8201 of 12452
Scan 2: estimating training effect sizes for Chunk 8401 of 12452
Scan 2: estimating training effect sizes for Chunk 8601 of 12452
Scan 2: estimating training effect sizes for Chunk 8801 of 12452
Scan 2: estimating training effect sizes for Chunk 9001 of 12452
Scan 2: estimating training effect sizes for Chunk 9201 of 12452
Scan 2: estimating training effect sizes for Chunk 9401 of 12452
Scan 2: estimating training effect sizes for Chunk 9601 of 12452
Scan 2: estimating training effect sizes for Chunk 9801 of 12452
Scan 2: estimating training effect sizes for Chunk 10001 of 12452
Scan 2: estimating training effect sizes for Chunk 10201 of 12452
Scan 2: estimating training effect sizes for Chunk 10401 of 12452
Scan 2: estimating training effect sizes for Chunk 10601 of 12452
Scan 2: estimating training effect sizes for Chunk 10801 of 12452
Scan 2: estimating training effect sizes for Chunk 11001 of 12452
Scan 2: estimating training effect sizes for Chunk 11201 of 12452
Scan 2: estimating training effect sizes for Chunk 11401 of 12452
Scan 2: estimating training effect sizes for Chunk 11601 of 12452
Scan 2: estimating training effect sizes for Chunk 11801 of 12452
Scan 2: estimating training effect sizes for Chunk 12001 of 12452
Scan 2: estimating training effect sizes for Chunk 12201 of 12452
Scan 2: estimating training effect sizes for Chunk 12401 of 12452
Average number of iterations per chunk: 3.51

Scan 3: estimating training effect sizes for Chunk 1 of 12452
Scan 3: estimating training effect sizes for Chunk 201 of 12452
Scan 3: estimating training effect sizes for Chunk 401 of 12452
Scan 3: estimating training effect sizes for Chunk 601 of 12452
Scan 3: estimating training effect sizes for Chunk 801 of 12452
Scan 3: estimating training effect sizes for Chunk 1001 of 12452
Scan 3: estimating training effect sizes for Chunk 1201 of 12452
Scan 3: estimating training effect sizes for Chunk 1401 of 12452
Scan 3: estimating training effect sizes for Chunk 1601 of 12452
Scan 3: estimating training effect sizes for Chunk 1801 of 12452
Scan 3: estimating training effect sizes for Chunk 2001 of 12452
Scan 3: estimating training effect sizes for Chunk 2201 of 12452
Scan 3: estimating training effect sizes for Chunk 2401 of 12452
Scan 3: estimating training effect sizes for Chunk 2601 of 12452
Scan 3: estimating training effect sizes for Chunk 2801 of 12452
Scan 3: estimating training effect sizes for Chunk 3001 of 12452
Scan 3: estimating training effect sizes for Chunk 3201 of 12452
Scan 3: estimating training effect sizes for Chunk 3401 of 12452
Scan 3: estimating training effect sizes for Chunk 3601 of 12452
Scan 3: estimating training effect sizes for Chunk 3801 of 12452
Scan 3: estimating training effect sizes for Chunk 4001 of 12452
Scan 3: estimating training effect sizes for Chunk 4201 of 12452
Scan 3: estimating training effect sizes for Chunk 4401 of 12452
Scan 3: estimating training effect sizes for Chunk 4601 of 12452
Scan 3: estimating training effect sizes for Chunk 4801 of 12452
Scan 3: estimating training effect sizes for Chunk 5001 of 12452
Scan 3: estimating training effect sizes for Chunk 5201 of 12452
Scan 3: estimating training effect sizes for Chunk 5401 of 12452
Scan 3: estimating training effect sizes for Chunk 5601 of 12452
Scan 3: estimating training effect sizes for Chunk 5801 of 12452
Scan 3: estimating training effect sizes for Chunk 6001 of 12452
Scan 3: estimating training effect sizes for Chunk 6201 of 12452
Scan 3: estimating training effect sizes for Chunk 6401 of 12452
Scan 3: estimating training effect sizes for Chunk 6601 of 12452
Scan 3: estimating training effect sizes for Chunk 6801 of 12452
Scan 3: estimating training effect sizes for Chunk 7001 of 12452
Scan 3: estimating training effect sizes for Chunk 7201 of 12452
Scan 3: estimating training effect sizes for Chunk 7401 of 12452
Scan 3: estimating training effect sizes for Chunk 7601 of 12452
Scan 3: estimating training effect sizes for Chunk 7801 of 12452
Scan 3: estimating training effect sizes for Chunk 8001 of 12452
Scan 3: estimating training effect sizes for Chunk 8201 of 12452
Scan 3: estimating training effect sizes for Chunk 8401 of 12452
Scan 3: estimating training effect sizes for Chunk 8601 of 12452
Scan 3: estimating training effect sizes for Chunk 8801 of 12452
Scan 3: estimating training effect sizes for Chunk 9001 of 12452
Scan 3: estimating training effect sizes for Chunk 9201 of 12452
Scan 3: estimating training effect sizes for Chunk 9401 of 12452
Scan 3: estimating training effect sizes for Chunk 9601 of 12452
Scan 3: estimating training effect sizes for Chunk 9801 of 12452
Scan 3: estimating training effect sizes for Chunk 10001 of 12452
Scan 3: estimating training effect sizes for Chunk 10201 of 12452
Scan 3: estimating training effect sizes for Chunk 10401 of 12452
Scan 3: estimating training effect sizes for Chunk 10601 of 12452
Scan 3: estimating training effect sizes for Chunk 10801 of 12452
Scan 3: estimating training effect sizes for Chunk 11001 of 12452
Scan 3: estimating training effect sizes for Chunk 11201 of 12452
Scan 3: estimating training effect sizes for Chunk 11401 of 12452
Scan 3: estimating training effect sizes for Chunk 11601 of 12452
Scan 3: estimating training effect sizes for Chunk 11801 of 12452
Scan 3: estimating training effect sizes for Chunk 12001 of 12452
Scan 3: estimating training effect sizes for Chunk 12201 of 12452
Scan 3: estimating training effect sizes for Chunk 12401 of 12452
Average number of iterations per chunk: 3.30

Scan 4: estimating training effect sizes for Chunk 1 of 12452
Scan 4: estimating training effect sizes for Chunk 201 of 12452
Scan 4: estimating training effect sizes for Chunk 401 of 12452
Scan 4: estimating training effect sizes for Chunk 601 of 12452
Scan 4: estimating training effect sizes for Chunk 801 of 12452
Scan 4: estimating training effect sizes for Chunk 1001 of 12452
Scan 4: estimating training effect sizes for Chunk 1201 of 12452
Scan 4: estimating training effect sizes for Chunk 1401 of 12452
Scan 4: estimating training effect sizes for Chunk 1601 of 12452
Scan 4: estimating training effect sizes for Chunk 1801 of 12452
Scan 4: estimating training effect sizes for Chunk 2001 of 12452
Scan 4: estimating training effect sizes for Chunk 2201 of 12452
Scan 4: estimating training effect sizes for Chunk 2401 of 12452
Scan 4: estimating training effect sizes for Chunk 2601 of 12452
Scan 4: estimating training effect sizes for Chunk 2801 of 12452
Scan 4: estimating training effect sizes for Chunk 3001 of 12452
Scan 4: estimating training effect sizes for Chunk 3201 of 12452
Scan 4: estimating training effect sizes for Chunk 3401 of 12452
Scan 4: estimating training effect sizes for Chunk 3601 of 12452
Scan 4: estimating training effect sizes for Chunk 3801 of 12452
Scan 4: estimating training effect sizes for Chunk 4001 of 12452
Scan 4: estimating training effect sizes for Chunk 4201 of 12452
Scan 4: estimating training effect sizes for Chunk 4401 of 12452
Scan 4: estimating training effect sizes for Chunk 4601 of 12452
Scan 4: estimating training effect sizes for Chunk 4801 of 12452
Scan 4: estimating training effect sizes for Chunk 5001 of 12452
Scan 4: estimating training effect sizes for Chunk 5201 of 12452
Scan 4: estimating training effect sizes for Chunk 5401 of 12452
Scan 4: estimating training effect sizes for Chunk 5601 of 12452
Scan 4: estimating training effect sizes for Chunk 5801 of 12452
Scan 4: estimating training effect sizes for Chunk 6001 of 12452
Scan 4: estimating training effect sizes for Chunk 6201 of 12452
Scan 4: estimating training effect sizes for Chunk 6401 of 12452
Scan 4: estimating training effect sizes for Chunk 6601 of 12452
Scan 4: estimating training effect sizes for Chunk 6801 of 12452
Scan 4: estimating training effect sizes for Chunk 7001 of 12452
Scan 4: estimating training effect sizes for Chunk 7201 of 12452
Scan 4: estimating training effect sizes for Chunk 7401 of 12452
Scan 4: estimating training effect sizes for Chunk 7601 of 12452
Scan 4: estimating training effect sizes for Chunk 7801 of 12452
Scan 4: estimating training effect sizes for Chunk 8001 of 12452
Scan 4: estimating training effect sizes for Chunk 8201 of 12452
Scan 4: estimating training effect sizes for Chunk 8401 of 12452
Scan 4: estimating training effect sizes for Chunk 8601 of 12452
Scan 4: estimating training effect sizes for Chunk 8801 of 12452
Scan 4: estimating training effect sizes for Chunk 9001 of 12452
Scan 4: estimating training effect sizes for Chunk 9201 of 12452
Scan 4: estimating training effect sizes for Chunk 9401 of 12452
Scan 4: estimating training effect sizes for Chunk 9601 of 12452
Scan 4: estimating training effect sizes for Chunk 9801 of 12452
Scan 4: estimating training effect sizes for Chunk 10001 of 12452
Scan 4: estimating training effect sizes for Chunk 10201 of 12452
Scan 4: estimating training effect sizes for Chunk 10401 of 12452
Scan 4: estimating training effect sizes for Chunk 10601 of 12452
Scan 4: estimating training effect sizes for Chunk 10801 of 12452
Scan 4: estimating training effect sizes for Chunk 11001 of 12452
Scan 4: estimating training effect sizes for Chunk 11201 of 12452
Scan 4: estimating training effect sizes for Chunk 11401 of 12452
Scan 4: estimating training effect sizes for Chunk 11601 of 12452
Scan 4: estimating training effect sizes for Chunk 11801 of 12452
Scan 4: estimating training effect sizes for Chunk 12001 of 12452
Scan 4: estimating training effect sizes for Chunk 12201 of 12452
Scan 4: estimating training effect sizes for Chunk 12401 of 12452
Average number of iterations per chunk: 3.08

Scan 5: estimating training effect sizes for Chunk 1 of 12452
Scan 5: estimating training effect sizes for Chunk 201 of 12452
Scan 5: estimating training effect sizes for Chunk 401 of 12452
Scan 5: estimating training effect sizes for Chunk 601 of 12452
Scan 5: estimating training effect sizes for Chunk 801 of 12452
Scan 5: estimating training effect sizes for Chunk 1001 of 12452
Scan 5: estimating training effect sizes for Chunk 1201 of 12452
Scan 5: estimating training effect sizes for Chunk 1401 of 12452
Scan 5: estimating training effect sizes for Chunk 1601 of 12452
Scan 5: estimating training effect sizes for Chunk 1801 of 12452
Scan 5: estimating training effect sizes for Chunk 2001 of 12452
Scan 5: estimating training effect sizes for Chunk 2201 of 12452
Scan 5: estimating training effect sizes for Chunk 2401 of 12452
Scan 5: estimating training effect sizes for Chunk 2601 of 12452
Scan 5: estimating training effect sizes for Chunk 2801 of 12452
Scan 5: estimating training effect sizes for Chunk 3001 of 12452
Scan 5: estimating training effect sizes for Chunk 3201 of 12452
Scan 5: estimating training effect sizes for Chunk 3401 of 12452
Scan 5: estimating training effect sizes for Chunk 3601 of 12452
Scan 5: estimating training effect sizes for Chunk 3801 of 12452
Scan 5: estimating training effect sizes for Chunk 4001 of 12452
Scan 5: estimating training effect sizes for Chunk 4201 of 12452
Scan 5: estimating training effect sizes for Chunk 4401 of 12452
Scan 5: estimating training effect sizes for Chunk 4601 of 12452
Scan 5: estimating training effect sizes for Chunk 4801 of 12452
Scan 5: estimating training effect sizes for Chunk 5001 of 12452
Scan 5: estimating training effect sizes for Chunk 5201 of 12452
Scan 5: estimating training effect sizes for Chunk 5401 of 12452
Scan 5: estimating training effect sizes for Chunk 5601 of 12452
Scan 5: estimating training effect sizes for Chunk 5801 of 12452
Scan 5: estimating training effect sizes for Chunk 6001 of 12452
Scan 5: estimating training effect sizes for Chunk 6201 of 12452
Scan 5: estimating training effect sizes for Chunk 6401 of 12452
Scan 5: estimating training effect sizes for Chunk 6601 of 12452
Scan 5: estimating training effect sizes for Chunk 6801 of 12452
Scan 5: estimating training effect sizes for Chunk 7001 of 12452
Scan 5: estimating training effect sizes for Chunk 7201 of 12452
Scan 5: estimating training effect sizes for Chunk 7401 of 12452
Scan 5: estimating training effect sizes for Chunk 7601 of 12452
Scan 5: estimating training effect sizes for Chunk 7801 of 12452
Scan 5: estimating training effect sizes for Chunk 8001 of 12452
Scan 5: estimating training effect sizes for Chunk 8201 of 12452
Scan 5: estimating training effect sizes for Chunk 8401 of 12452
Scan 5: estimating training effect sizes for Chunk 8601 of 12452
Scan 5: estimating training effect sizes for Chunk 8801 of 12452
Scan 5: estimating training effect sizes for Chunk 9001 of 12452
Scan 5: estimating training effect sizes for Chunk 9201 of 12452
Scan 5: estimating training effect sizes for Chunk 9401 of 12452
Scan 5: estimating training effect sizes for Chunk 9601 of 12452
Scan 5: estimating training effect sizes for Chunk 9801 of 12452
Scan 5: estimating training effect sizes for Chunk 10001 of 12452
Scan 5: estimating training effect sizes for Chunk 10201 of 12452
Scan 5: estimating training effect sizes for Chunk 10401 of 12452
Scan 5: estimating training effect sizes for Chunk 10601 of 12452
Scan 5: estimating training effect sizes for Chunk 10801 of 12452
Scan 5: estimating training effect sizes for Chunk 11001 of 12452
Scan 5: estimating training effect sizes for Chunk 11201 of 12452
Scan 5: estimating training effect sizes for Chunk 11401 of 12452
Scan 5: estimating training effect sizes for Chunk 11601 of 12452
Scan 5: estimating training effect sizes for Chunk 11801 of 12452
Scan 5: estimating training effect sizes for Chunk 12001 of 12452
Scan 5: estimating training effect sizes for Chunk 12201 of 12452
Scan 5: estimating training effect sizes for Chunk 12401 of 12452
Average number of iterations per chunk: 2.90

Scan 6: estimating training effect sizes for Chunk 1 of 12452
Scan 6: estimating training effect sizes for Chunk 201 of 12452
Scan 6: estimating training effect sizes for Chunk 401 of 12452
Scan 6: estimating training effect sizes for Chunk 601 of 12452
Scan 6: estimating training effect sizes for Chunk 801 of 12452
Scan 6: estimating training effect sizes for Chunk 1001 of 12452
Scan 6: estimating training effect sizes for Chunk 1201 of 12452
Scan 6: estimating training effect sizes for Chunk 1401 of 12452
Scan 6: estimating training effect sizes for Chunk 1601 of 12452
Scan 6: estimating training effect sizes for Chunk 1801 of 12452
Scan 6: estimating training effect sizes for Chunk 2001 of 12452
Scan 6: estimating training effect sizes for Chunk 2201 of 12452
Scan 6: estimating training effect sizes for Chunk 2401 of 12452
Scan 6: estimating training effect sizes for Chunk 2601 of 12452
Scan 6: estimating training effect sizes for Chunk 2801 of 12452
Scan 6: estimating training effect sizes for Chunk 3001 of 12452
Scan 6: estimating training effect sizes for Chunk 3201 of 12452
Scan 6: estimating training effect sizes for Chunk 3401 of 12452
Scan 6: estimating training effect sizes for Chunk 3601 of 12452
Scan 6: estimating training effect sizes for Chunk 3801 of 12452
Scan 6: estimating training effect sizes for Chunk 4001 of 12452
Scan 6: estimating training effect sizes for Chunk 4201 of 12452
Scan 6: estimating training effect sizes for Chunk 4401 of 12452
Scan 6: estimating training effect sizes for Chunk 4601 of 12452
Scan 6: estimating training effect sizes for Chunk 4801 of 12452
Scan 6: estimating training effect sizes for Chunk 5001 of 12452
Scan 6: estimating training effect sizes for Chunk 5201 of 12452
Scan 6: estimating training effect sizes for Chunk 5401 of 12452
Scan 6: estimating training effect sizes for Chunk 5601 of 12452
Scan 6: estimating training effect sizes for Chunk 5801 of 12452
Scan 6: estimating training effect sizes for Chunk 6001 of 12452
Scan 6: estimating training effect sizes for Chunk 6201 of 12452
Scan 6: estimating training effect sizes for Chunk 6401 of 12452
Scan 6: estimating training effect sizes for Chunk 6601 of 12452
Scan 6: estimating training effect sizes for Chunk 6801 of 12452
Scan 6: estimating training effect sizes for Chunk 7001 of 12452
Scan 6: estimating training effect sizes for Chunk 7201 of 12452
Scan 6: estimating training effect sizes for Chunk 7401 of 12452
Scan 6: estimating training effect sizes for Chunk 7601 of 12452
Scan 6: estimating training effect sizes for Chunk 7801 of 12452
Scan 6: estimating training effect sizes for Chunk 8001 of 12452
Scan 6: estimating training effect sizes for Chunk 8201 of 12452
Scan 6: estimating training effect sizes for Chunk 8401 of 12452
Scan 6: estimating training effect sizes for Chunk 8601 of 12452
Scan 6: estimating training effect sizes for Chunk 8801 of 12452
Scan 6: estimating training effect sizes for Chunk 9001 of 12452
Scan 6: estimating training effect sizes for Chunk 9201 of 12452
Scan 6: estimating training effect sizes for Chunk 9401 of 12452
Scan 6: estimating training effect sizes for Chunk 9601 of 12452
Scan 6: estimating training effect sizes for Chunk 9801 of 12452
Scan 6: estimating training effect sizes for Chunk 10001 of 12452
Scan 6: estimating training effect sizes for Chunk 10201 of 12452
Scan 6: estimating training effect sizes for Chunk 10401 of 12452
Scan 6: estimating training effect sizes for Chunk 10601 of 12452
Scan 6: estimating training effect sizes for Chunk 10801 of 12452
Scan 6: estimating training effect sizes for Chunk 11001 of 12452
Scan 6: estimating training effect sizes for Chunk 11201 of 12452
Scan 6: estimating training effect sizes for Chunk 11401 of 12452
Scan 6: estimating training effect sizes for Chunk 11601 of 12452
Scan 6: estimating training effect sizes for Chunk 11801 of 12452
Scan 6: estimating training effect sizes for Chunk 12001 of 12452
Scan 6: estimating training effect sizes for Chunk 12201 of 12452
Scan 6: estimating training effect sizes for Chunk 12401 of 12452
Average number of iterations per chunk: 2.74

Scan 7: estimating training effect sizes for Chunk 1 of 12452
Scan 7: estimating training effect sizes for Chunk 201 of 12452
Scan 7: estimating training effect sizes for Chunk 401 of 12452
Scan 7: estimating training effect sizes for Chunk 601 of 12452
Scan 7: estimating training effect sizes for Chunk 801 of 12452
Scan 7: estimating training effect sizes for Chunk 1001 of 12452
Scan 7: estimating training effect sizes for Chunk 1201 of 12452
Scan 7: estimating training effect sizes for Chunk 1401 of 12452
Scan 7: estimating training effect sizes for Chunk 1601 of 12452
Scan 7: estimating training effect sizes for Chunk 1801 of 12452
Scan 7: estimating training effect sizes for Chunk 2001 of 12452
Scan 7: estimating training effect sizes for Chunk 2201 of 12452
Scan 7: estimating training effect sizes for Chunk 2401 of 12452
Scan 7: estimating training effect sizes for Chunk 2601 of 12452
Scan 7: estimating training effect sizes for Chunk 2801 of 12452
Scan 7: estimating training effect sizes for Chunk 3001 of 12452
Scan 7: estimating training effect sizes for Chunk 3201 of 12452
Scan 7: estimating training effect sizes for Chunk 3401 of 12452
Scan 7: estimating training effect sizes for Chunk 3601 of 12452
Scan 7: estimating training effect sizes for Chunk 3801 of 12452
Scan 7: estimating training effect sizes for Chunk 4001 of 12452
Scan 7: estimating training effect sizes for Chunk 4201 of 12452
Scan 7: estimating training effect sizes for Chunk 4401 of 12452
Scan 7: estimating training effect sizes for Chunk 4601 of 12452
Scan 7: estimating training effect sizes for Chunk 4801 of 12452
Scan 7: estimating training effect sizes for Chunk 5001 of 12452
Scan 7: estimating training effect sizes for Chunk 5201 of 12452
Scan 7: estimating training effect sizes for Chunk 5401 of 12452
Scan 7: estimating training effect sizes for Chunk 5601 of 12452
Scan 7: estimating training effect sizes for Chunk 5801 of 12452
Scan 7: estimating training effect sizes for Chunk 6001 of 12452
Scan 7: estimating training effect sizes for Chunk 6201 of 12452
Scan 7: estimating training effect sizes for Chunk 6401 of 12452
Scan 7: estimating training effect sizes for Chunk 6601 of 12452
Scan 7: estimating training effect sizes for Chunk 6801 of 12452
Scan 7: estimating training effect sizes for Chunk 7001 of 12452
Scan 7: estimating training effect sizes for Chunk 7201 of 12452
Scan 7: estimating training effect sizes for Chunk 7401 of 12452
Scan 7: estimating training effect sizes for Chunk 7601 of 12452
Scan 7: estimating training effect sizes for Chunk 7801 of 12452
Scan 7: estimating training effect sizes for Chunk 8001 of 12452
Scan 7: estimating training effect sizes for Chunk 8201 of 12452
Scan 7: estimating training effect sizes for Chunk 8401 of 12452
Scan 7: estimating training effect sizes for Chunk 8601 of 12452
Scan 7: estimating training effect sizes for Chunk 8801 of 12452
Scan 7: estimating training effect sizes for Chunk 9001 of 12452
Scan 7: estimating training effect sizes for Chunk 9201 of 12452
Scan 7: estimating training effect sizes for Chunk 9401 of 12452
Scan 7: estimating training effect sizes for Chunk 9601 of 12452
Scan 7: estimating training effect sizes for Chunk 9801 of 12452
Scan 7: estimating training effect sizes for Chunk 10001 of 12452
Scan 7: estimating training effect sizes for Chunk 10201 of 12452
Scan 7: estimating training effect sizes for Chunk 10401 of 12452
Scan 7: estimating training effect sizes for Chunk 10601 of 12452
Scan 7: estimating training effect sizes for Chunk 10801 of 12452
Scan 7: estimating training effect sizes for Chunk 11001 of 12452
Scan 7: estimating training effect sizes for Chunk 11201 of 12452
Scan 7: estimating training effect sizes for Chunk 11401 of 12452
Scan 7: estimating training effect sizes for Chunk 11601 of 12452
Scan 7: estimating training effect sizes for Chunk 11801 of 12452
Scan 7: estimating training effect sizes for Chunk 12001 of 12452
Scan 7: estimating training effect sizes for Chunk 12201 of 12452
Scan 7: estimating training effect sizes for Chunk 12401 of 12452
Average number of iterations per chunk: 2.59

Scan 8: estimating training effect sizes for Chunk 1 of 12452
Scan 8: estimating training effect sizes for Chunk 201 of 12452
Scan 8: estimating training effect sizes for Chunk 401 of 12452
Scan 8: estimating training effect sizes for Chunk 601 of 12452
Scan 8: estimating training effect sizes for Chunk 801 of 12452
Scan 8: estimating training effect sizes for Chunk 1001 of 12452
Scan 8: estimating training effect sizes for Chunk 1201 of 12452
Scan 8: estimating training effect sizes for Chunk 1401 of 12452
Scan 8: estimating training effect sizes for Chunk 1601 of 12452
Scan 8: estimating training effect sizes for Chunk 1801 of 12452
Scan 8: estimating training effect sizes for Chunk 2001 of 12452
Scan 8: estimating training effect sizes for Chunk 2201 of 12452
Scan 8: estimating training effect sizes for Chunk 2401 of 12452
Scan 8: estimating training effect sizes for Chunk 2601 of 12452
Scan 8: estimating training effect sizes for Chunk 2801 of 12452
Scan 8: estimating training effect sizes for Chunk 3001 of 12452
Scan 8: estimating training effect sizes for Chunk 3201 of 12452
Scan 8: estimating training effect sizes for Chunk 3401 of 12452
Scan 8: estimating training effect sizes for Chunk 3601 of 12452
Scan 8: estimating training effect sizes for Chunk 3801 of 12452
Scan 8: estimating training effect sizes for Chunk 4001 of 12452
Scan 8: estimating training effect sizes for Chunk 4201 of 12452
Scan 8: estimating training effect sizes for Chunk 4401 of 12452
Scan 8: estimating training effect sizes for Chunk 4601 of 12452
Scan 8: estimating training effect sizes for Chunk 4801 of 12452
Scan 8: estimating training effect sizes for Chunk 5001 of 12452
Scan 8: estimating training effect sizes for Chunk 5201 of 12452
Scan 8: estimating training effect sizes for Chunk 5401 of 12452
Scan 8: estimating training effect sizes for Chunk 5601 of 12452
Scan 8: estimating training effect sizes for Chunk 5801 of 12452
Scan 8: estimating training effect sizes for Chunk 6001 of 12452
Scan 8: estimating training effect sizes for Chunk 6201 of 12452
Scan 8: estimating training effect sizes for Chunk 6401 of 12452
Scan 8: estimating training effect sizes for Chunk 6601 of 12452
Scan 8: estimating training effect sizes for Chunk 6801 of 12452
Scan 8: estimating training effect sizes for Chunk 7001 of 12452
Scan 8: estimating training effect sizes for Chunk 7201 of 12452
Scan 8: estimating training effect sizes for Chunk 7401 of 12452
Scan 8: estimating training effect sizes for Chunk 7601 of 12452
Scan 8: estimating training effect sizes for Chunk 7801 of 12452
Scan 8: estimating training effect sizes for Chunk 8001 of 12452
Scan 8: estimating training effect sizes for Chunk 8201 of 12452
Scan 8: estimating training effect sizes for Chunk 8401 of 12452
Scan 8: estimating training effect sizes for Chunk 8601 of 12452
Scan 8: estimating training effect sizes for Chunk 8801 of 12452
Scan 8: estimating training effect sizes for Chunk 9001 of 12452
Scan 8: estimating training effect sizes for Chunk 9201 of 12452
Scan 8: estimating training effect sizes for Chunk 9401 of 12452
Scan 8: estimating training effect sizes for Chunk 9601 of 12452
Scan 8: estimating training effect sizes for Chunk 9801 of 12452
Scan 8: estimating training effect sizes for Chunk 10001 of 12452
Scan 8: estimating training effect sizes for Chunk 10201 of 12452
Scan 8: estimating training effect sizes for Chunk 10401 of 12452
Scan 8: estimating training effect sizes for Chunk 10601 of 12452
Scan 8: estimating training effect sizes for Chunk 10801 of 12452
Scan 8: estimating training effect sizes for Chunk 11001 of 12452
Scan 8: estimating training effect sizes for Chunk 11201 of 12452
Scan 8: estimating training effect sizes for Chunk 11401 of 12452
Scan 8: estimating training effect sizes for Chunk 11601 of 12452
Scan 8: estimating training effect sizes for Chunk 11801 of 12452
Scan 8: estimating training effect sizes for Chunk 12001 of 12452
Scan 8: estimating training effect sizes for Chunk 12201 of 12452
Scan 8: estimating training effect sizes for Chunk 12401 of 12452
Average number of iterations per chunk: 2.46

Scan 9: estimating training effect sizes for Chunk 1 of 12452
Scan 9: estimating training effect sizes for Chunk 201 of 12452
Scan 9: estimating training effect sizes for Chunk 401 of 12452
Scan 9: estimating training effect sizes for Chunk 601 of 12452
Scan 9: estimating training effect sizes for Chunk 801 of 12452
Scan 9: estimating training effect sizes for Chunk 1001 of 12452
Scan 9: estimating training effect sizes for Chunk 1201 of 12452
Scan 9: estimating training effect sizes for Chunk 1401 of 12452
Scan 9: estimating training effect sizes for Chunk 1601 of 12452
Scan 9: estimating training effect sizes for Chunk 1801 of 12452
Scan 9: estimating training effect sizes for Chunk 2001 of 12452
Scan 9: estimating training effect sizes for Chunk 2201 of 12452
Scan 9: estimating training effect sizes for Chunk 2401 of 12452
Scan 9: estimating training effect sizes for Chunk 2601 of 12452
Scan 9: estimating training effect sizes for Chunk 2801 of 12452
Scan 9: estimating training effect sizes for Chunk 3001 of 12452
Scan 9: estimating training effect sizes for Chunk 3201 of 12452
Scan 9: estimating training effect sizes for Chunk 3401 of 12452
Scan 9: estimating training effect sizes for Chunk 3601 of 12452
Scan 9: estimating training effect sizes for Chunk 3801 of 12452
Scan 9: estimating training effect sizes for Chunk 4001 of 12452
Scan 9: estimating training effect sizes for Chunk 4201 of 12452
Scan 9: estimating training effect sizes for Chunk 4401 of 12452
Scan 9: estimating training effect sizes for Chunk 4601 of 12452
Scan 9: estimating training effect sizes for Chunk 4801 of 12452
Scan 9: estimating training effect sizes for Chunk 5001 of 12452
Scan 9: estimating training effect sizes for Chunk 5201 of 12452
Scan 9: estimating training effect sizes for Chunk 5401 of 12452
Scan 9: estimating training effect sizes for Chunk 5601 of 12452
Scan 9: estimating training effect sizes for Chunk 5801 of 12452
Scan 9: estimating training effect sizes for Chunk 6001 of 12452
Scan 9: estimating training effect sizes for Chunk 6201 of 12452
Scan 9: estimating training effect sizes for Chunk 6401 of 12452
Scan 9: estimating training effect sizes for Chunk 6601 of 12452
Scan 9: estimating training effect sizes for Chunk 6801 of 12452
Scan 9: estimating training effect sizes for Chunk 7001 of 12452
Scan 9: estimating training effect sizes for Chunk 7201 of 12452
Scan 9: estimating training effect sizes for Chunk 7401 of 12452
Scan 9: estimating training effect sizes for Chunk 7601 of 12452
Scan 9: estimating training effect sizes for Chunk 7801 of 12452
Scan 9: estimating training effect sizes for Chunk 8001 of 12452
Scan 9: estimating training effect sizes for Chunk 8201 of 12452
Scan 9: estimating training effect sizes for Chunk 8401 of 12452
Scan 9: estimating training effect sizes for Chunk 8601 of 12452
Scan 9: estimating training effect sizes for Chunk 8801 of 12452
Scan 9: estimating training effect sizes for Chunk 9001 of 12452
Scan 9: estimating training effect sizes for Chunk 9201 of 12452
Scan 9: estimating training effect sizes for Chunk 9401 of 12452
Scan 9: estimating training effect sizes for Chunk 9601 of 12452
Scan 9: estimating training effect sizes for Chunk 9801 of 12452
Scan 9: estimating training effect sizes for Chunk 10001 of 12452
Scan 9: estimating training effect sizes for Chunk 10201 of 12452
Scan 9: estimating training effect sizes for Chunk 10401 of 12452
Scan 9: estimating training effect sizes for Chunk 10601 of 12452
Scan 9: estimating training effect sizes for Chunk 10801 of 12452
Scan 9: estimating training effect sizes for Chunk 11001 of 12452
Scan 9: estimating training effect sizes for Chunk 11201 of 12452
Scan 9: estimating training effect sizes for Chunk 11401 of 12452
Scan 9: estimating training effect sizes for Chunk 11601 of 12452
Scan 9: estimating training effect sizes for Chunk 11801 of 12452
Scan 9: estimating training effect sizes for Chunk 12001 of 12452
Scan 9: estimating training effect sizes for Chunk 12201 of 12452
Scan 9: estimating training effect sizes for Chunk 12401 of 12452
Average number of iterations per chunk: 2.36

Scan 10: estimating training effect sizes for Chunk 1 of 12452
Scan 10: estimating training effect sizes for Chunk 201 of 12452
Scan 10: estimating training effect sizes for Chunk 401 of 12452
Scan 10: estimating training effect sizes for Chunk 601 of 12452
Scan 10: estimating training effect sizes for Chunk 801 of 12452
Scan 10: estimating training effect sizes for Chunk 1001 of 12452
Scan 10: estimating training effect sizes for Chunk 1201 of 12452
Scan 10: estimating training effect sizes for Chunk 1401 of 12452
Scan 10: estimating training effect sizes for Chunk 1601 of 12452
Scan 10: estimating training effect sizes for Chunk 1801 of 12452
Scan 10: estimating training effect sizes for Chunk 2001 of 12452
Scan 10: estimating training effect sizes for Chunk 2201 of 12452
Scan 10: estimating training effect sizes for Chunk 2401 of 12452
Scan 10: estimating training effect sizes for Chunk 2601 of 12452
Scan 10: estimating training effect sizes for Chunk 2801 of 12452
Scan 10: estimating training effect sizes for Chunk 3001 of 12452
Scan 10: estimating training effect sizes for Chunk 3201 of 12452
Scan 10: estimating training effect sizes for Chunk 3401 of 12452
Scan 10: estimating training effect sizes for Chunk 3601 of 12452
Scan 10: estimating training effect sizes for Chunk 3801 of 12452
Scan 10: estimating training effect sizes for Chunk 4001 of 12452
Scan 10: estimating training effect sizes for Chunk 4201 of 12452
Scan 10: estimating training effect sizes for Chunk 4401 of 12452
Scan 10: estimating training effect sizes for Chunk 4601 of 12452
Scan 10: estimating training effect sizes for Chunk 4801 of 12452
Scan 10: estimating training effect sizes for Chunk 5001 of 12452
Scan 10: estimating training effect sizes for Chunk 5201 of 12452
Scan 10: estimating training effect sizes for Chunk 5401 of 12452
Scan 10: estimating training effect sizes for Chunk 5601 of 12452
Scan 10: estimating training effect sizes for Chunk 5801 of 12452
Scan 10: estimating training effect sizes for Chunk 6001 of 12452
Scan 10: estimating training effect sizes for Chunk 6201 of 12452
Scan 10: estimating training effect sizes for Chunk 6401 of 12452
Scan 10: estimating training effect sizes for Chunk 6601 of 12452
Scan 10: estimating training effect sizes for Chunk 6801 of 12452
Scan 10: estimating training effect sizes for Chunk 7001 of 12452
Scan 10: estimating training effect sizes for Chunk 7201 of 12452
Scan 10: estimating training effect sizes for Chunk 7401 of 12452
Scan 10: estimating training effect sizes for Chunk 7601 of 12452
Scan 10: estimating training effect sizes for Chunk 7801 of 12452
Scan 10: estimating training effect sizes for Chunk 8001 of 12452
Scan 10: estimating training effect sizes for Chunk 8201 of 12452
Scan 10: estimating training effect sizes for Chunk 8401 of 12452
Scan 10: estimating training effect sizes for Chunk 8601 of 12452
Scan 10: estimating training effect sizes for Chunk 8801 of 12452
Scan 10: estimating training effect sizes for Chunk 9001 of 12452
Scan 10: estimating training effect sizes for Chunk 9201 of 12452
Scan 10: estimating training effect sizes for Chunk 9401 of 12452
Scan 10: estimating training effect sizes for Chunk 9601 of 12452
Scan 10: estimating training effect sizes for Chunk 9801 of 12452
Scan 10: estimating training effect sizes for Chunk 10001 of 12452
Scan 10: estimating training effect sizes for Chunk 10201 of 12452
Scan 10: estimating training effect sizes for Chunk 10401 of 12452
Scan 10: estimating training effect sizes for Chunk 10601 of 12452
Scan 10: estimating training effect sizes for Chunk 10801 of 12452
Scan 10: estimating training effect sizes for Chunk 11001 of 12452
Scan 10: estimating training effect sizes for Chunk 11201 of 12452
Scan 10: estimating training effect sizes for Chunk 11401 of 12452
Scan 10: estimating training effect sizes for Chunk 11601 of 12452
Scan 10: estimating training effect sizes for Chunk 11801 of 12452
Scan 10: estimating training effect sizes for Chunk 12001 of 12452
Scan 10: estimating training effect sizes for Chunk 12201 of 12452
Scan 10: estimating training effect sizes for Chunk 12401 of 12452
Average number of iterations per chunk: 2.26

Warning, Variational Bayes did not converge after 10 scans (this is not normally a problem)

The revised estimate of heritability is 0.0100

Measuring accuracy of each model
Model 1: heritability 0.0100, p 0.0000, f2 1.0000, mean squared error 1.0104
Model 2: heritability 0.0100, p 0.5000, f2 0.5000, mean squared error 1.0118
Model 3: heritability 0.0100, p 0.5000, f2 0.3000, mean squared error 1.0190
Model 4: heritability 0.0100, p 0.5000, f2 0.1000, mean squared error 1.0269
Model 5: heritability 0.0100, p 0.1000, f2 0.5000, mean squared error 1.0159
Model 6: heritability 0.0100, p 0.1000, f2 0.3000, mean squared error 1.0202
Model 7: heritability 0.0100, p 0.1000, f2 0.1000, mean squared error 1.0226
Model 8: heritability 0.0100, p 0.0100, f2 0.5000, mean squared error 1.0107
Model 9: heritability 0.0100, p 0.0100, f2 0.3000, mean squared error 1.0115
Model 10: heritability 0.0100, p 0.0100, f2 0.1000, mean squared error 1.0122

Time check: have so far spent 0.32 hours

Constructing final PRS (heritability 0.0100, p 0.0000, f2 1.0000) using all samples

Scan 1: estimating final effect sizes for Chunk 1 of 12452
Scan 1: estimating final effect sizes for Chunk 201 of 12452
Scan 1: estimating final effect sizes for Chunk 401 of 12452
Scan 1: estimating final effect sizes for Chunk 601 of 12452
Scan 1: estimating final effect sizes for Chunk 801 of 12452
Scan 1: estimating final effect sizes for Chunk 1001 of 12452
Scan 1: estimating final effect sizes for Chunk 1201 of 12452
Scan 1: estimating final effect sizes for Chunk 1401 of 12452
Scan 1: estimating final effect sizes for Chunk 1601 of 12452
Scan 1: estimating final effect sizes for Chunk 1801 of 12452
Scan 1: estimating final effect sizes for Chunk 2001 of 12452
Scan 1: estimating final effect sizes for Chunk 2201 of 12452
Scan 1: estimating final effect sizes for Chunk 2401 of 12452
Scan 1: estimating final effect sizes for Chunk 2601 of 12452
Scan 1: estimating final effect sizes for Chunk 2801 of 12452
Scan 1: estimating final effect sizes for Chunk 3001 of 12452
Scan 1: estimating final effect sizes for Chunk 3201 of 12452
Scan 1: estimating final effect sizes for Chunk 3401 of 12452
Scan 1: estimating final effect sizes for Chunk 3601 of 12452
Scan 1: estimating final effect sizes for Chunk 3801 of 12452
Scan 1: estimating final effect sizes for Chunk 4001 of 12452
Scan 1: estimating final effect sizes for Chunk 4201 of 12452
Scan 1: estimating final effect sizes for Chunk 4401 of 12452
Scan 1: estimating final effect sizes for Chunk 4601 of 12452
Scan 1: estimating final effect sizes for Chunk 4801 of 12452
Scan 1: estimating final effect sizes for Chunk 5001 of 12452
Scan 1: estimating final effect sizes for Chunk 5201 of 12452
Scan 1: estimating final effect sizes for Chunk 5401 of 12452
Scan 1: estimating final effect sizes for Chunk 5601 of 12452
Scan 1: estimating final effect sizes for Chunk 5801 of 12452
Scan 1: estimating final effect sizes for Chunk 6001 of 12452
Scan 1: estimating final effect sizes for Chunk 6201 of 12452
Scan 1: estimating final effect sizes for Chunk 6401 of 12452
Scan 1: estimating final effect sizes for Chunk 6601 of 12452
Scan 1: estimating final effect sizes for Chunk 6801 of 12452
Scan 1: estimating final effect sizes for Chunk 7001 of 12452
Scan 1: estimating final effect sizes for Chunk 7201 of 12452
Scan 1: estimating final effect sizes for Chunk 7401 of 12452
Scan 1: estimating final effect sizes for Chunk 7601 of 12452
Scan 1: estimating final effect sizes for Chunk 7801 of 12452
Scan 1: estimating final effect sizes for Chunk 8001 of 12452
Scan 1: estimating final effect sizes for Chunk 8201 of 12452
Scan 1: estimating final effect sizes for Chunk 8401 of 12452
Scan 1: estimating final effect sizes for Chunk 8601 of 12452
Scan 1: estimating final effect sizes for Chunk 8801 of 12452
Scan 1: estimating final effect sizes for Chunk 9001 of 12452
Scan 1: estimating final effect sizes for Chunk 9201 of 12452
Scan 1: estimating final effect sizes for Chunk 9401 of 12452
Scan 1: estimating final effect sizes for Chunk 9601 of 12452
Scan 1: estimating final effect sizes for Chunk 9801 of 12452
Scan 1: estimating final effect sizes for Chunk 10001 of 12452
Scan 1: estimating final effect sizes for Chunk 10201 of 12452
Scan 1: estimating final effect sizes for Chunk 10401 of 12452
Scan 1: estimating final effect sizes for Chunk 10601 of 12452
Scan 1: estimating final effect sizes for Chunk 10801 of 12452
Scan 1: estimating final effect sizes for Chunk 11001 of 12452
Scan 1: estimating final effect sizes for Chunk 11201 of 12452
Scan 1: estimating final effect sizes for Chunk 11401 of 12452
Scan 1: estimating final effect sizes for Chunk 11601 of 12452
Scan 1: estimating final effect sizes for Chunk 11801 of 12452
Scan 1: estimating final effect sizes for Chunk 12001 of 12452
Scan 1: estimating final effect sizes for Chunk 12201 of 12452
Scan 1: estimating final effect sizes for Chunk 12401 of 12452
Average number of iterations per chunk: 1.99

Scan 2: estimating final effect sizes for Chunk 1 of 12359
Scan 2: estimating final effect sizes for Chunk 201 of 12359
Scan 2: estimating final effect sizes for Chunk 401 of 12359
Scan 2: estimating final effect sizes for Chunk 601 of 12359
Scan 2: estimating final effect sizes for Chunk 801 of 12359
Scan 2: estimating final effect sizes for Chunk 1001 of 12359
Scan 2: estimating final effect sizes for Chunk 1201 of 12359
Scan 2: estimating final effect sizes for Chunk 1401 of 12359
Scan 2: estimating final effect sizes for Chunk 1601 of 12359
Scan 2: estimating final effect sizes for Chunk 1801 of 12359
Scan 2: estimating final effect sizes for Chunk 2001 of 12359
Scan 2: estimating final effect sizes for Chunk 2201 of 12359
Scan 2: estimating final effect sizes for Chunk 2401 of 12359
Scan 2: estimating final effect sizes for Chunk 2601 of 12359
Scan 2: estimating final effect sizes for Chunk 2801 of 12359
Scan 2: estimating final effect sizes for Chunk 3001 of 12359
Scan 2: estimating final effect sizes for Chunk 3201 of 12359
Scan 2: estimating final effect sizes for Chunk 3401 of 12359
Scan 2: estimating final effect sizes for Chunk 3601 of 12359
Scan 2: estimating final effect sizes for Chunk 3801 of 12359
Scan 2: estimating final effect sizes for Chunk 4001 of 12359
Scan 2: estimating final effect sizes for Chunk 4201 of 12359
Scan 2: estimating final effect sizes for Chunk 4401 of 12359
Scan 2: estimating final effect sizes for Chunk 4601 of 12359
Scan 2: estimating final effect sizes for Chunk 4801 of 12359
Scan 2: estimating final effect sizes for Chunk 5001 of 12359
Scan 2: estimating final effect sizes for Chunk 5201 of 12359
Scan 2: estimating final effect sizes for Chunk 5401 of 12359
Scan 2: estimating final effect sizes for Chunk 5601 of 12359
Scan 2: estimating final effect sizes for Chunk 5801 of 12359
Scan 2: estimating final effect sizes for Chunk 6001 of 12359
Scan 2: estimating final effect sizes for Chunk 6201 of 12359
Scan 2: estimating final effect sizes for Chunk 6401 of 12359
Scan 2: estimating final effect sizes for Chunk 6601 of 12359
Scan 2: estimating final effect sizes for Chunk 6801 of 12359
Scan 2: estimating final effect sizes for Chunk 7001 of 12359
Scan 2: estimating final effect sizes for Chunk 7201 of 12359
Scan 2: estimating final effect sizes for Chunk 7401 of 12359
Scan 2: estimating final effect sizes for Chunk 7601 of 12359
Scan 2: estimating final effect sizes for Chunk 7801 of 12359
Scan 2: estimating final effect sizes for Chunk 8001 of 12359
Scan 2: estimating final effect sizes for Chunk 8201 of 12359
Scan 2: estimating final effect sizes for Chunk 8401 of 12359
Scan 2: estimating final effect sizes for Chunk 8601 of 12359
Scan 2: estimating final effect sizes for Chunk 8801 of 12359
Scan 2: estimating final effect sizes for Chunk 9001 of 12359
Scan 2: estimating final effect sizes for Chunk 9201 of 12359
Scan 2: estimating final effect sizes for Chunk 9401 of 12359
Scan 2: estimating final effect sizes for Chunk 9601 of 12359
Scan 2: estimating final effect sizes for Chunk 9801 of 12359
Scan 2: estimating final effect sizes for Chunk 10001 of 12359
Scan 2: estimating final effect sizes for Chunk 10201 of 12359
Scan 2: estimating final effect sizes for Chunk 10401 of 12359
Scan 2: estimating final effect sizes for Chunk 10601 of 12359
Scan 2: estimating final effect sizes for Chunk 10801 of 12359
Scan 2: estimating final effect sizes for Chunk 11001 of 12359
Scan 2: estimating final effect sizes for Chunk 11201 of 12359
Scan 2: estimating final effect sizes for Chunk 11401 of 12359
Scan 2: estimating final effect sizes for Chunk 11601 of 12359
Scan 2: estimating final effect sizes for Chunk 11801 of 12359
Scan 2: estimating final effect sizes for Chunk 12001 of 12359
Scan 2: estimating final effect sizes for Chunk 12201 of 12359
Average number of iterations per chunk: 1.00

Scan 3: estimating final effect sizes for Chunk 1 of 1
Average number of iterations per chunk: 1.00

Best-fitting model saved in ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_elastic.effects, with posterior probabilities in ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_elastic.probs

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 08:45:25 2025 and ended at Sun May 25 09:04:46 2025
The elapsed time was 0.32 hours
Given the command used one thread, this means the CPU time was also 0.32 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_elastic_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_elastic.effects
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for one profile

Please note that ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_elastic.effects is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 959 samples and 3187483 predictors (will be using 959 and 3187483)

Reading details for 3187483 predictors from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_elastic.effects

Reading phenotypes for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 3197
Calculating scores for Chunk 51 of 3197
Calculating scores for Chunk 101 of 3197
Calculating scores for Chunk 151 of 3197
Calculating scores for Chunk 201 of 3197
Calculating scores for Chunk 251 of 3197
Calculating scores for Chunk 301 of 3197
Calculating scores for Chunk 351 of 3197
Calculating scores for Chunk 401 of 3197
Calculating scores for Chunk 451 of 3197
Calculating scores for Chunk 501 of 3197
Calculating scores for Chunk 551 of 3197
Calculating scores for Chunk 601 of 3197
Calculating scores for Chunk 651 of 3197
Calculating scores for Chunk 701 of 3197
Calculating scores for Chunk 751 of 3197
Calculating scores for Chunk 801 of 3197
Calculating scores for Chunk 851 of 3197
Calculating scores for Chunk 901 of 3197
Calculating scores for Chunk 951 of 3197
Calculating scores for Chunk 1001 of 3197
Calculating scores for Chunk 1051 of 3197
Calculating scores for Chunk 1101 of 3197
Calculating scores for Chunk 1151 of 3197
Calculating scores for Chunk 1201 of 3197
Calculating scores for Chunk 1251 of 3197
Calculating scores for Chunk 1301 of 3197
Calculating scores for Chunk 1351 of 3197
Calculating scores for Chunk 1401 of 3197
Calculating scores for Chunk 1451 of 3197
Calculating scores for Chunk 1501 of 3197
Calculating scores for Chunk 1551 of 3197
Calculating scores for Chunk 1601 of 3197
Calculating scores for Chunk 1651 of 3197
Calculating scores for Chunk 1701 of 3197
Calculating scores for Chunk 1751 of 3197
Calculating scores for Chunk 1801 of 3197
Calculating scores for Chunk 1851 of 3197
Calculating scores for Chunk 1901 of 3197
Calculating scores for Chunk 1951 of 3197
Calculating scores for Chunk 2001 of 3197
Calculating scores for Chunk 2051 of 3197
Calculating scores for Chunk 2101 of 3197
Calculating scores for Chunk 2151 of 3197
Calculating scores for Chunk 2201 of 3197
Calculating scores for Chunk 2251 of 3197
Calculating scores for Chunk 2301 of 3197
Calculating scores for Chunk 2351 of 3197
Calculating scores for Chunk 2401 of 3197
Calculating scores for Chunk 2451 of 3197
Calculating scores for Chunk 2501 of 3197
Calculating scores for Chunk 2551 of 3197
Calculating scores for Chunk 2601 of 3197
Calculating scores for Chunk 2651 of 3197
Calculating scores for Chunk 2701 of 3197
Calculating scores for Chunk 2751 of 3197
Calculating scores for Chunk 2801 of 3197
Calculating scores for Chunk 2851 of 3197
Calculating scores for Chunk 2901 of 3197
Calculating scores for Chunk 2951 of 3197
Calculating scores for Chunk 3001 of 3197
Calculating scores for Chunk 3051 of 3197
Calculating scores for Chunk 3101 of 3197
Calculating scores for Chunk 3151 of 3197

Profile saved in ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_elastic_prs_calc_with_pheno.profile

Correlation between score and phenotype is 0.8357, saved in ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_elastic_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:04:46 2025 and ended at Sun May 25 09:05:19 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_elastic_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_elastic.effects
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for one profile

Please note that ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_elastic.effects is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 959 samples and 3187483 predictors (will be using 959 and 3187483)

Reading details for 3187483 predictors from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_elastic.effects

Calculating scores for Chunk 1 of 3197
Calculating scores for Chunk 51 of 3197
Calculating scores for Chunk 101 of 3197
Calculating scores for Chunk 151 of 3197
Calculating scores for Chunk 201 of 3197
Calculating scores for Chunk 251 of 3197
Calculating scores for Chunk 301 of 3197
Calculating scores for Chunk 351 of 3197
Calculating scores for Chunk 401 of 3197
Calculating scores for Chunk 451 of 3197
Calculating scores for Chunk 501 of 3197
Calculating scores for Chunk 551 of 3197
Calculating scores for Chunk 601 of 3197
Calculating scores for Chunk 651 of 3197
Calculating scores for Chunk 701 of 3197
Calculating scores for Chunk 751 of 3197
Calculating scores for Chunk 801 of 3197
Calculating scores for Chunk 851 of 3197
Calculating scores for Chunk 901 of 3197
Calculating scores for Chunk 951 of 3197
Calculating scores for Chunk 1001 of 3197
Calculating scores for Chunk 1051 of 3197
Calculating scores for Chunk 1101 of 3197
Calculating scores for Chunk 1151 of 3197
Calculating scores for Chunk 1201 of 3197
Calculating scores for Chunk 1251 of 3197
Calculating scores for Chunk 1301 of 3197
Calculating scores for Chunk 1351 of 3197
Calculating scores for Chunk 1401 of 3197
Calculating scores for Chunk 1451 of 3197
Calculating scores for Chunk 1501 of 3197
Calculating scores for Chunk 1551 of 3197
Calculating scores for Chunk 1601 of 3197
Calculating scores for Chunk 1651 of 3197
Calculating scores for Chunk 1701 of 3197
Calculating scores for Chunk 1751 of 3197
Calculating scores for Chunk 1801 of 3197
Calculating scores for Chunk 1851 of 3197
Calculating scores for Chunk 1901 of 3197
Calculating scores for Chunk 1951 of 3197
Calculating scores for Chunk 2001 of 3197
Calculating scores for Chunk 2051 of 3197
Calculating scores for Chunk 2101 of 3197
Calculating scores for Chunk 2151 of 3197
Calculating scores for Chunk 2201 of 3197
Calculating scores for Chunk 2251 of 3197
Calculating scores for Chunk 2301 of 3197
Calculating scores for Chunk 2351 of 3197
Calculating scores for Chunk 2401 of 3197
Calculating scores for Chunk 2451 of 3197
Calculating scores for Chunk 2501 of 3197
Calculating scores for Chunk 2551 of 3197
Calculating scores for Chunk 2601 of 3197
Calculating scores for Chunk 2651 of 3197
Calculating scores for Chunk 2701 of 3197
Calculating scores for Chunk 2751 of 3197
Calculating scores for Chunk 2801 of 3197
Calculating scores for Chunk 2851 of 3197
Calculating scores for Chunk 2901 of 3197
Calculating scores for Chunk 2951 of 3197
Calculating scores for Chunk 3001 of 3197
Calculating scores for Chunk 3051 of 3197
Calculating scores for Chunk 3101 of 3197
Calculating scores for Chunk 3151 of 3197

Profile saved in ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_elastic_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:05:19 2025 and ended at Sun May 25 09:05:50 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## check the PRS is the same ##


###### calculate a PRS using the classical approach ######

## calculate linear association between SNPs and the phenotype ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--linear ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_linear_raw
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 959 samples and 3187483 predictors (will be using 959 and 3187483)

Reading phenotypes for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 6 covariates for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_linear_raw.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_linear_raw.combined")

Performing linear regression for Chunk 1 of 3197
Performing linear regression for Chunk 11 of 3197
Performing linear regression for Chunk 21 of 3197
Performing linear regression for Chunk 31 of 3197
Performing linear regression for Chunk 41 of 3197
Performing linear regression for Chunk 51 of 3197
Performing linear regression for Chunk 61 of 3197
Performing linear regression for Chunk 71 of 3197
Performing linear regression for Chunk 81 of 3197
Performing linear regression for Chunk 91 of 3197
Performing linear regression for Chunk 101 of 3197
Performing linear regression for Chunk 111 of 3197
Performing linear regression for Chunk 121 of 3197
Performing linear regression for Chunk 131 of 3197
Performing linear regression for Chunk 141 of 3197
Performing linear regression for Chunk 151 of 3197
Performing linear regression for Chunk 161 of 3197
Performing linear regression for Chunk 171 of 3197
Performing linear regression for Chunk 181 of 3197
Performing linear regression for Chunk 191 of 3197
Performing linear regression for Chunk 201 of 3197
Performing linear regression for Chunk 211 of 3197
Performing linear regression for Chunk 221 of 3197
Performing linear regression for Chunk 231 of 3197
Performing linear regression for Chunk 241 of 3197
Performing linear regression for Chunk 251 of 3197
Performing linear regression for Chunk 261 of 3197
Performing linear regression for Chunk 271 of 3197
Performing linear regression for Chunk 281 of 3197
Performing linear regression for Chunk 291 of 3197
Performing linear regression for Chunk 301 of 3197
Performing linear regression for Chunk 311 of 3197
Performing linear regression for Chunk 321 of 3197
Performing linear regression for Chunk 331 of 3197
Performing linear regression for Chunk 341 of 3197
Performing linear regression for Chunk 351 of 3197
Performing linear regression for Chunk 361 of 3197
Performing linear regression for Chunk 371 of 3197
Performing linear regression for Chunk 381 of 3197
Performing linear regression for Chunk 391 of 3197
Performing linear regression for Chunk 401 of 3197
Performing linear regression for Chunk 411 of 3197
Performing linear regression for Chunk 421 of 3197
Performing linear regression for Chunk 431 of 3197
Performing linear regression for Chunk 441 of 3197
Performing linear regression for Chunk 451 of 3197
Performing linear regression for Chunk 461 of 3197
Performing linear regression for Chunk 471 of 3197
Performing linear regression for Chunk 481 of 3197
Performing linear regression for Chunk 491 of 3197
Performing linear regression for Chunk 501 of 3197
Performing linear regression for Chunk 511 of 3197
Performing linear regression for Chunk 521 of 3197
Performing linear regression for Chunk 531 of 3197
Performing linear regression for Chunk 541 of 3197
Performing linear regression for Chunk 551 of 3197
Performing linear regression for Chunk 561 of 3197
Performing linear regression for Chunk 571 of 3197
Performing linear regression for Chunk 581 of 3197
Performing linear regression for Chunk 591 of 3197
Performing linear regression for Chunk 601 of 3197
Performing linear regression for Chunk 611 of 3197
Performing linear regression for Chunk 621 of 3197
Performing linear regression for Chunk 631 of 3197
Performing linear regression for Chunk 641 of 3197
Performing linear regression for Chunk 651 of 3197
Performing linear regression for Chunk 661 of 3197
Performing linear regression for Chunk 671 of 3197
Performing linear regression for Chunk 681 of 3197
Performing linear regression for Chunk 691 of 3197
Performing linear regression for Chunk 701 of 3197
Performing linear regression for Chunk 711 of 3197
Performing linear regression for Chunk 721 of 3197
Performing linear regression for Chunk 731 of 3197
Performing linear regression for Chunk 741 of 3197
Performing linear regression for Chunk 751 of 3197
Performing linear regression for Chunk 761 of 3197
Performing linear regression for Chunk 771 of 3197
Performing linear regression for Chunk 781 of 3197
Performing linear regression for Chunk 791 of 3197
Performing linear regression for Chunk 801 of 3197
Performing linear regression for Chunk 811 of 3197
Performing linear regression for Chunk 821 of 3197
Performing linear regression for Chunk 831 of 3197
Performing linear regression for Chunk 841 of 3197
Performing linear regression for Chunk 851 of 3197
Performing linear regression for Chunk 861 of 3197
Performing linear regression for Chunk 871 of 3197
Performing linear regression for Chunk 881 of 3197
Performing linear regression for Chunk 891 of 3197
Performing linear regression for Chunk 901 of 3197
Performing linear regression for Chunk 911 of 3197
Performing linear regression for Chunk 921 of 3197
Performing linear regression for Chunk 931 of 3197
Performing linear regression for Chunk 941 of 3197
Performing linear regression for Chunk 951 of 3197
Performing linear regression for Chunk 961 of 3197
Performing linear regression for Chunk 971 of 3197
Performing linear regression for Chunk 981 of 3197
Performing linear regression for Chunk 991 of 3197
Performing linear regression for Chunk 1001 of 3197
Performing linear regression for Chunk 1011 of 3197
Performing linear regression for Chunk 1021 of 3197
Performing linear regression for Chunk 1031 of 3197
Performing linear regression for Chunk 1041 of 3197
Performing linear regression for Chunk 1051 of 3197
Performing linear regression for Chunk 1061 of 3197
Performing linear regression for Chunk 1071 of 3197
Performing linear regression for Chunk 1081 of 3197
Performing linear regression for Chunk 1091 of 3197
Performing linear regression for Chunk 1101 of 3197
Performing linear regression for Chunk 1111 of 3197
Performing linear regression for Chunk 1121 of 3197
Performing linear regression for Chunk 1131 of 3197
Performing linear regression for Chunk 1141 of 3197
Performing linear regression for Chunk 1151 of 3197
Performing linear regression for Chunk 1161 of 3197
Performing linear regression for Chunk 1171 of 3197
Performing linear regression for Chunk 1181 of 3197
Performing linear regression for Chunk 1191 of 3197
Performing linear regression for Chunk 1201 of 3197
Performing linear regression for Chunk 1211 of 3197
Performing linear regression for Chunk 1221 of 3197
Performing linear regression for Chunk 1231 of 3197
Performing linear regression for Chunk 1241 of 3197
Performing linear regression for Chunk 1251 of 3197
Performing linear regression for Chunk 1261 of 3197
Performing linear regression for Chunk 1271 of 3197
Performing linear regression for Chunk 1281 of 3197
Performing linear regression for Chunk 1291 of 3197
Performing linear regression for Chunk 1301 of 3197
Performing linear regression for Chunk 1311 of 3197
Performing linear regression for Chunk 1321 of 3197
Performing linear regression for Chunk 1331 of 3197
Performing linear regression for Chunk 1341 of 3197
Performing linear regression for Chunk 1351 of 3197
Performing linear regression for Chunk 1361 of 3197
Performing linear regression for Chunk 1371 of 3197
Performing linear regression for Chunk 1381 of 3197
Performing linear regression for Chunk 1391 of 3197
Performing linear regression for Chunk 1401 of 3197
Performing linear regression for Chunk 1411 of 3197
Performing linear regression for Chunk 1421 of 3197
Performing linear regression for Chunk 1431 of 3197
Performing linear regression for Chunk 1441 of 3197
Performing linear regression for Chunk 1451 of 3197
Performing linear regression for Chunk 1461 of 3197
Performing linear regression for Chunk 1471 of 3197
Performing linear regression for Chunk 1481 of 3197
Performing linear regression for Chunk 1491 of 3197
Performing linear regression for Chunk 1501 of 3197
Performing linear regression for Chunk 1511 of 3197
Performing linear regression for Chunk 1521 of 3197
Performing linear regression for Chunk 1531 of 3197
Performing linear regression for Chunk 1541 of 3197
Performing linear regression for Chunk 1551 of 3197
Performing linear regression for Chunk 1561 of 3197
Performing linear regression for Chunk 1571 of 3197
Performing linear regression for Chunk 1581 of 3197
Performing linear regression for Chunk 1591 of 3197
Performing linear regression for Chunk 1601 of 3197
Performing linear regression for Chunk 1611 of 3197
Performing linear regression for Chunk 1621 of 3197
Performing linear regression for Chunk 1631 of 3197
Performing linear regression for Chunk 1641 of 3197
Performing linear regression for Chunk 1651 of 3197
Performing linear regression for Chunk 1661 of 3197
Performing linear regression for Chunk 1671 of 3197
Performing linear regression for Chunk 1681 of 3197
Performing linear regression for Chunk 1691 of 3197
Performing linear regression for Chunk 1701 of 3197
Performing linear regression for Chunk 1711 of 3197
Performing linear regression for Chunk 1721 of 3197
Performing linear regression for Chunk 1731 of 3197
Performing linear regression for Chunk 1741 of 3197
Performing linear regression for Chunk 1751 of 3197
Performing linear regression for Chunk 1761 of 3197
Performing linear regression for Chunk 1771 of 3197
Performing linear regression for Chunk 1781 of 3197
Performing linear regression for Chunk 1791 of 3197
Performing linear regression for Chunk 1801 of 3197
Performing linear regression for Chunk 1811 of 3197
Performing linear regression for Chunk 1821 of 3197
Performing linear regression for Chunk 1831 of 3197
Performing linear regression for Chunk 1841 of 3197
Performing linear regression for Chunk 1851 of 3197
Performing linear regression for Chunk 1861 of 3197
Performing linear regression for Chunk 1871 of 3197
Performing linear regression for Chunk 1881 of 3197
Performing linear regression for Chunk 1891 of 3197
Performing linear regression for Chunk 1901 of 3197
Performing linear regression for Chunk 1911 of 3197
Performing linear regression for Chunk 1921 of 3197
Performing linear regression for Chunk 1931 of 3197
Performing linear regression for Chunk 1941 of 3197
Performing linear regression for Chunk 1951 of 3197
Performing linear regression for Chunk 1961 of 3197
Performing linear regression for Chunk 1971 of 3197
Performing linear regression for Chunk 1981 of 3197
Performing linear regression for Chunk 1991 of 3197
Performing linear regression for Chunk 2001 of 3197
Performing linear regression for Chunk 2011 of 3197
Performing linear regression for Chunk 2021 of 3197
Performing linear regression for Chunk 2031 of 3197
Performing linear regression for Chunk 2041 of 3197
Performing linear regression for Chunk 2051 of 3197
Performing linear regression for Chunk 2061 of 3197
Performing linear regression for Chunk 2071 of 3197
Performing linear regression for Chunk 2081 of 3197
Performing linear regression for Chunk 2091 of 3197
Performing linear regression for Chunk 2101 of 3197
Performing linear regression for Chunk 2111 of 3197
Performing linear regression for Chunk 2121 of 3197
Performing linear regression for Chunk 2131 of 3197
Performing linear regression for Chunk 2141 of 3197
Performing linear regression for Chunk 2151 of 3197
Performing linear regression for Chunk 2161 of 3197
Performing linear regression for Chunk 2171 of 3197
Performing linear regression for Chunk 2181 of 3197
Performing linear regression for Chunk 2191 of 3197
Performing linear regression for Chunk 2201 of 3197
Performing linear regression for Chunk 2211 of 3197
Performing linear regression for Chunk 2221 of 3197
Performing linear regression for Chunk 2231 of 3197
Performing linear regression for Chunk 2241 of 3197
Performing linear regression for Chunk 2251 of 3197
Performing linear regression for Chunk 2261 of 3197
Performing linear regression for Chunk 2271 of 3197
Performing linear regression for Chunk 2281 of 3197
Performing linear regression for Chunk 2291 of 3197
Performing linear regression for Chunk 2301 of 3197
Performing linear regression for Chunk 2311 of 3197
Performing linear regression for Chunk 2321 of 3197
Performing linear regression for Chunk 2331 of 3197
Performing linear regression for Chunk 2341 of 3197
Performing linear regression for Chunk 2351 of 3197
Performing linear regression for Chunk 2361 of 3197
Performing linear regression for Chunk 2371 of 3197
Performing linear regression for Chunk 2381 of 3197
Performing linear regression for Chunk 2391 of 3197
Performing linear regression for Chunk 2401 of 3197
Performing linear regression for Chunk 2411 of 3197
Performing linear regression for Chunk 2421 of 3197
Performing linear regression for Chunk 2431 of 3197
Performing linear regression for Chunk 2441 of 3197
Performing linear regression for Chunk 2451 of 3197
Performing linear regression for Chunk 2461 of 3197
Performing linear regression for Chunk 2471 of 3197
Performing linear regression for Chunk 2481 of 3197
Performing linear regression for Chunk 2491 of 3197
Performing linear regression for Chunk 2501 of 3197
Performing linear regression for Chunk 2511 of 3197
Performing linear regression for Chunk 2521 of 3197
Performing linear regression for Chunk 2531 of 3197
Performing linear regression for Chunk 2541 of 3197
Performing linear regression for Chunk 2551 of 3197
Performing linear regression for Chunk 2561 of 3197
Performing linear regression for Chunk 2571 of 3197
Performing linear regression for Chunk 2581 of 3197
Performing linear regression for Chunk 2591 of 3197
Performing linear regression for Chunk 2601 of 3197
Performing linear regression for Chunk 2611 of 3197
Performing linear regression for Chunk 2621 of 3197
Performing linear regression for Chunk 2631 of 3197
Performing linear regression for Chunk 2641 of 3197
Performing linear regression for Chunk 2651 of 3197
Performing linear regression for Chunk 2661 of 3197
Performing linear regression for Chunk 2671 of 3197
Performing linear regression for Chunk 2681 of 3197
Performing linear regression for Chunk 2691 of 3197
Performing linear regression for Chunk 2701 of 3197
Performing linear regression for Chunk 2711 of 3197
Performing linear regression for Chunk 2721 of 3197
Performing linear regression for Chunk 2731 of 3197
Performing linear regression for Chunk 2741 of 3197
Performing linear regression for Chunk 2751 of 3197
Performing linear regression for Chunk 2761 of 3197
Performing linear regression for Chunk 2771 of 3197
Performing linear regression for Chunk 2781 of 3197
Performing linear regression for Chunk 2791 of 3197
Performing linear regression for Chunk 2801 of 3197
Performing linear regression for Chunk 2811 of 3197
Performing linear regression for Chunk 2821 of 3197
Performing linear regression for Chunk 2831 of 3197
Performing linear regression for Chunk 2841 of 3197
Performing linear regression for Chunk 2851 of 3197
Performing linear regression for Chunk 2861 of 3197
Performing linear regression for Chunk 2871 of 3197
Performing linear regression for Chunk 2881 of 3197
Performing linear regression for Chunk 2891 of 3197
Performing linear regression for Chunk 2901 of 3197
Performing linear regression for Chunk 2911 of 3197
Performing linear regression for Chunk 2921 of 3197
Performing linear regression for Chunk 2931 of 3197
Performing linear regression for Chunk 2941 of 3197
Performing linear regression for Chunk 2951 of 3197
Performing linear regression for Chunk 2961 of 3197
Performing linear regression for Chunk 2971 of 3197
Performing linear regression for Chunk 2981 of 3197
Performing linear regression for Chunk 2991 of 3197
Performing linear regression for Chunk 3001 of 3197
Performing linear regression for Chunk 3011 of 3197
Performing linear regression for Chunk 3021 of 3197
Performing linear regression for Chunk 3031 of 3197
Performing linear regression for Chunk 3041 of 3197
Performing linear regression for Chunk 3051 of 3197
Performing linear regression for Chunk 3061 of 3197
Performing linear regression for Chunk 3071 of 3197
Performing linear regression for Chunk 3081 of 3197
Performing linear regression for Chunk 3091 of 3197
Performing linear regression for Chunk 3101 of 3197
Performing linear regression for Chunk 3111 of 3197
Performing linear regression for Chunk 3121 of 3197
Performing linear regression for Chunk 3131 of 3197
Performing linear regression for Chunk 3141 of 3197
Performing linear regression for Chunk 3151 of 3197
Performing linear regression for Chunk 3161 of 3197
Performing linear regression for Chunk 3171 of 3197
Performing linear regression for Chunk 3181 of 3197
Performing linear regression for Chunk 3191 of 3197

Main results saved in ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_linear_raw.assoc, with a summary version in ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_linear_raw.summaries, p-values in ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_linear_raw.pvalues and score file in ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_linear_raw.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:05:50 2025 and ended at Sun May 25 09:06:22 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## define p-values cut-offs for thresholding ##

# load the p-values #

# get the minimum p-value #

# make a list of thresholds that are above the minimum p-value #

## obtain scores in a reduced set of SNPs after thresholding and cumpling ##

# perform thresholding considering the threshold 1 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1/weight_change_small_set_predictors_set_linear_clump_thresholding_1_predictors
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 1

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e+00, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

3187483 of the 3187483 predictors in ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e+00
All of these are in the data

Data contain 959 samples and 3187483 predictors (will be using 959 and 3187483)

Reading p-values from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr1_858952_G_A 1.4215e-01 | chr1_905373_T_C 1.8967e-01 | chr1_911428_C_T 4.6303e-01

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 25 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 127500; stay tuned for updates ;)
Pass 1: Thinning for Chunk 2001 of 127500; kept 5244 out of 50000 predictors (10.49%)
Pass 1: Thinning for Chunk 4001 of 127500; kept 10022 out of 100000 predictors (10.02%)
Pass 1: Thinning for Chunk 6001 of 127500; kept 14822 out of 150000 predictors (9.88%)
Pass 1: Thinning for Chunk 8001 of 127500; kept 19571 out of 200000 predictors (9.79%)
Pass 1: Thinning for Chunk 10001 of 127500; kept 24124 out of 250000 predictors (9.65%)
Pass 1: Thinning for Chunk 12001 of 127500; kept 28662 out of 300000 predictors (9.55%)
Pass 1: Thinning for Chunk 14001 of 127500; kept 33221 out of 350000 predictors (9.49%)
Pass 1: Thinning for Chunk 16001 of 127500; kept 37721 out of 400000 predictors (9.43%)
Pass 1: Thinning for Chunk 18001 of 127500; kept 42567 out of 450000 predictors (9.46%)
Pass 1: Thinning for Chunk 20001 of 127500; kept 48019 out of 500000 predictors (9.60%)
Pass 1: Thinning for Chunk 22001 of 127500; kept 52487 out of 550000 predictors (9.54%)
Pass 1: Thinning for Chunk 24001 of 127500; kept 57204 out of 600000 predictors (9.53%)
Pass 1: Thinning for Chunk 26001 of 127500; kept 61753 out of 650000 predictors (9.50%)
Pass 1: Thinning for Chunk 28001 of 127500; kept 66386 out of 700000 predictors (9.48%)
Pass 1: Thinning for Chunk 30001 of 127500; kept 71110 out of 750000 predictors (9.48%)
Pass 1: Thinning for Chunk 32001 of 127500; kept 75764 out of 800000 predictors (9.47%)
Pass 1: Thinning for Chunk 34001 of 127500; kept 79902 out of 850000 predictors (9.40%)
Pass 1: Thinning for Chunk 36001 of 127500; kept 84225 out of 900000 predictors (9.36%)
Pass 1: Thinning for Chunk 38001 of 127500; kept 88669 out of 950000 predictors (9.33%)
Pass 1: Thinning for Chunk 40001 of 127500; kept 92954 out of 1000000 predictors (9.30%)
Pass 1: Thinning for Chunk 42001 of 127500; kept 97706 out of 1050000 predictors (9.31%)
Pass 1: Thinning for Chunk 44001 of 127500; kept 102480 out of 1100000 predictors (9.32%)
Pass 1: Thinning for Chunk 46001 of 127500; kept 106814 out of 1150000 predictors (9.29%)
Pass 1: Thinning for Chunk 48001 of 127500; kept 111789 out of 1200000 predictors (9.32%)
Pass 1: Thinning for Chunk 50001 of 127500; kept 116371 out of 1250000 predictors (9.31%)
Pass 1: Thinning for Chunk 52001 of 127500; kept 120228 out of 1300000 predictors (9.25%)
Pass 1: Thinning for Chunk 54001 of 127500; kept 124523 out of 1350000 predictors (9.22%)
Pass 1: Thinning for Chunk 56001 of 127500; kept 129361 out of 1400000 predictors (9.24%)
Pass 1: Thinning for Chunk 58001 of 127500; kept 133564 out of 1450000 predictors (9.21%)
Pass 1: Thinning for Chunk 60001 of 127500; kept 137714 out of 1500000 predictors (9.18%)
Pass 1: Thinning for Chunk 62001 of 127500; kept 141725 out of 1550000 predictors (9.14%)
Pass 1: Thinning for Chunk 64001 of 127500; kept 146685 out of 1600000 predictors (9.17%)
Pass 1: Thinning for Chunk 66001 of 127500; kept 151092 out of 1650000 predictors (9.16%)
Pass 1: Thinning for Chunk 68001 of 127500; kept 154877 out of 1700000 predictors (9.11%)
Pass 1: Thinning for Chunk 70001 of 127500; kept 159343 out of 1750000 predictors (9.11%)
Pass 1: Thinning for Chunk 72001 of 127500; kept 164012 out of 1800000 predictors (9.11%)
Pass 1: Thinning for Chunk 74001 of 127500; kept 168374 out of 1850000 predictors (9.10%)
Pass 1: Thinning for Chunk 76001 of 127500; kept 173167 out of 1900000 predictors (9.11%)
Pass 1: Thinning for Chunk 78001 of 127500; kept 177993 out of 1950000 predictors (9.13%)
Pass 1: Thinning for Chunk 80001 of 127500; kept 182490 out of 2000000 predictors (9.12%)
Pass 1: Thinning for Chunk 82001 of 127500; kept 186554 out of 2050000 predictors (9.10%)
Pass 1: Thinning for Chunk 84001 of 127500; kept 191263 out of 2100000 predictors (9.11%)
Pass 1: Thinning for Chunk 86001 of 127500; kept 195581 out of 2150000 predictors (9.10%)
Pass 1: Thinning for Chunk 88001 of 127500; kept 200097 out of 2200000 predictors (9.10%)
Pass 1: Thinning for Chunk 90001 of 127500; kept 204689 out of 2250000 predictors (9.10%)
Pass 1: Thinning for Chunk 92001 of 127500; kept 209384 out of 2300000 predictors (9.10%)
Pass 1: Thinning for Chunk 94001 of 127500; kept 213794 out of 2350000 predictors (9.10%)
Pass 1: Thinning for Chunk 96001 of 127500; kept 218417 out of 2400000 predictors (9.10%)
Pass 1: Thinning for Chunk 98001 of 127500; kept 223523 out of 2450000 predictors (9.12%)
Pass 1: Thinning for Chunk 100001 of 127500; kept 228364 out of 2500000 predictors (9.13%)
Pass 1: Thinning for Chunk 102001 of 127500; kept 232663 out of 2550000 predictors (9.12%)
Pass 1: Thinning for Chunk 104001 of 127500; kept 237463 out of 2600000 predictors (9.13%)
Pass 1: Thinning for Chunk 106001 of 127500; kept 241825 out of 2650000 predictors (9.13%)
Pass 1: Thinning for Chunk 108001 of 127500; kept 246965 out of 2700000 predictors (9.15%)
Pass 1: Thinning for Chunk 110001 of 127500; kept 252251 out of 2750000 predictors (9.17%)
Pass 1: Thinning for Chunk 112001 of 127500; kept 256976 out of 2800000 predictors (9.18%)
Pass 1: Thinning for Chunk 114001 of 127500; kept 261680 out of 2850000 predictors (9.18%)
Pass 1: Thinning for Chunk 116001 of 127500; kept 267160 out of 2900000 predictors (9.21%)
Pass 1: Thinning for Chunk 118001 of 127500; kept 272771 out of 2950000 predictors (9.25%)
Pass 1: Thinning for Chunk 120001 of 127500; kept 277445 out of 3000000 predictors (9.25%)
Pass 1: Thinning for Chunk 122001 of 127500; kept 282971 out of 3050000 predictors (9.28%)
Pass 1: Thinning for Chunk 124001 of 127500; kept 288065 out of 3100000 predictors (9.29%)
Pass 1: Thinning for Chunk 126001 of 127500; kept 293052 out of 3150000 predictors (9.30%)

The bit-size has now been set to 123

Pass 2: Thinning for Chunk 1 of 2416; stay tuned for updates ;)
Pass 2: Thinning for Chunk 401 of 2416; kept 15481 out of 49200 predictors (31.47%)
Pass 2: Thinning for Chunk 801 of 2416; kept 31559 out of 98400 predictors (32.07%)
Pass 2: Thinning for Chunk 1201 of 2416; kept 46367 out of 147600 predictors (31.41%)
Pass 2: Thinning for Chunk 1601 of 2416; kept 64023 out of 196800 predictors (32.53%)
Pass 2: Thinning for Chunk 2001 of 2416; kept 80209 out of 246000 predictors (32.61%)
Pass 2: Thinning for Chunk 2401 of 2416; kept 101370 out of 295200 predictors (34.34%)

Thinning complete: 102282 predictors kept (saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1/weight_change_small_set_predictors_set_linear_clump_thresholding_1_predictors.in), 3085201 lost (./results/final_results/analysis_full_data/weight_change/clump_thresholding_1/weight_change_small_set_predictors_set_linear_clump_thresholding_1_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/weight_change/clump_thresholding_1/weight_change_small_set_predictors_set_linear_clump_thresholding_1_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:06:24 2025 and ended at Sun May 25 09:07:59 2025
The elapsed time was 0.03 hours
Given the command used one thread, this means the CPU time was also 0.03 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1/weight_change_linear_clump_thresholding_1
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1/weight_change_small_set_predictors_set_linear_clump_thresholding_1_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 102282 predictors to extract from ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1/weight_change_small_set_predictors_set_linear_clump_thresholding_1_predictors.in

Data contain 959 samples and 3187483 predictors (will be using 959 and 102282)

Reading phenotypes for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 6 covariates for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1/weight_change_linear_clump_thresholding_1.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1/weight_change_linear_clump_thresholding_1.combined")

Performing linear regression for Chunk 1 of 115
Performing linear regression for Chunk 11 of 115
Performing linear regression for Chunk 21 of 115
Performing linear regression for Chunk 31 of 115
Performing linear regression for Chunk 41 of 115
Performing linear regression for Chunk 51 of 115
Performing linear regression for Chunk 61 of 115
Performing linear regression for Chunk 71 of 115
Performing linear regression for Chunk 81 of 115
Performing linear regression for Chunk 91 of 115
Performing linear regression for Chunk 101 of 115
Performing linear regression for Chunk 111 of 115

Main results saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1/weight_change_linear_clump_thresholding_1.assoc, with a summary version in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1/weight_change_linear_clump_thresholding_1.summaries, p-values in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1/weight_change_linear_clump_thresholding_1.pvalues and score file in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1/weight_change_linear_clump_thresholding_1.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:07:59 2025 and ended at Sun May 25 09:08:07 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1/weight_change_linear_clump_thresholding_1_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1/weight_change_linear_clump_thresholding_1.score
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1/weight_change_linear_clump_thresholding_1.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 959 samples and 3187483 predictors (will be using 959 and 3187483)

Reading details for 102282 predictors from ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1/weight_change_linear_clump_thresholding_1.score

Reading phenotypes for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 115
Calculating scores for Chunk 51 of 115
Calculating scores for Chunk 101 of 115

Profiles saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1/weight_change_linear_clump_thresholding_1_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.2275 to 0.8466, saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1/weight_change_linear_clump_thresholding_1_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:08:07 2025 and ended at Sun May 25 09:08:15 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1/weight_change_linear_clump_thresholding_1_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1/weight_change_linear_clump_thresholding_1.score
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1/weight_change_linear_clump_thresholding_1.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 959 samples and 3187483 predictors (will be using 959 and 3187483)

Reading details for 102282 predictors from ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1/weight_change_linear_clump_thresholding_1.score

Calculating scores for Chunk 1 of 115
Calculating scores for Chunk 51 of 115
Calculating scores for Chunk 101 of 115

Profiles saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1/weight_change_linear_clump_thresholding_1_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:08:15 2025 and ended at Sun May 25 09:08:23 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


# perform thresholding considering the threshold 0.1 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.1/weight_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 0.1

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e-01, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

322294 of the 3187483 predictors in ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e-01
All of these are in the data

Data contain 959 samples and 3187483 predictors (will be using 959 and 322294)

Reading p-values from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr1_982260_G_A 8.1584e-02 | chr1_982858_C_G 8.1584e-02 | chr1_984039_T_C 8.2972e-02

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 20 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 16115; stay tuned for updates ;)
Pass 1: Thinning for Chunk 2501 of 16115; kept 8391 out of 50000 predictors (16.78%)
Pass 1: Thinning for Chunk 5001 of 16115; kept 16816 out of 100000 predictors (16.82%)
Pass 1: Thinning for Chunk 7501 of 16115; kept 24774 out of 150000 predictors (16.52%)
Pass 1: Thinning for Chunk 10001 of 16115; kept 32611 out of 200000 predictors (16.31%)
Pass 1: Thinning for Chunk 12501 of 16115; kept 40907 out of 250000 predictors (16.36%)
Pass 1: Thinning for Chunk 15001 of 16115; kept 49491 out of 300000 predictors (16.50%)

The bit-size has now been set to 25

Pass 2: Thinning for Chunk 1 of 2138; stay tuned for updates ;)
Pass 2: Thinning for Chunk 2001 of 2138; kept 22836 out of 50000 predictors (45.67%)

Thinning complete: 24825 predictors kept (saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.1/weight_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors.in), 297469 lost (./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.1/weight_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.1/weight_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:08:23 2025 and ended at Sun May 25 09:09:07 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.1/weight_change_linear_clump_thresholding_0.1
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.1/weight_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 24825 predictors to extract from ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.1/weight_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors.in

Data contain 959 samples and 3187483 predictors (will be using 959 and 24825)

Reading phenotypes for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 6 covariates for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.1/weight_change_linear_clump_thresholding_0.1.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.1/weight_change_linear_clump_thresholding_0.1.combined")

Performing linear regression for Chunk 1 of 35
Performing linear regression for Chunk 11 of 35
Performing linear regression for Chunk 21 of 35
Performing linear regression for Chunk 31 of 35

Main results saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.1/weight_change_linear_clump_thresholding_0.1.assoc, with a summary version in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.1/weight_change_linear_clump_thresholding_0.1.summaries, p-values in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.1/weight_change_linear_clump_thresholding_0.1.pvalues and score file in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.1/weight_change_linear_clump_thresholding_0.1.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:09:07 2025 and ended at Sun May 25 09:09:14 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.1/weight_change_linear_clump_thresholding_0.1_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.1/weight_change_linear_clump_thresholding_0.1.score
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.1/weight_change_linear_clump_thresholding_0.1.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 959 samples and 3187483 predictors (will be using 959 and 3187483)

Reading details for 24825 predictors from ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.1/weight_change_linear_clump_thresholding_0.1.score

Reading phenotypes for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 35

Profiles saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.1/weight_change_linear_clump_thresholding_0.1_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.2275 to 0.8462, saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.1/weight_change_linear_clump_thresholding_0.1_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:09:14 2025 and ended at Sun May 25 09:09:21 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.1/weight_change_linear_clump_thresholding_0.1_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.1/weight_change_linear_clump_thresholding_0.1.score
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.1/weight_change_linear_clump_thresholding_0.1.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 959 samples and 3187483 predictors (will be using 959 and 3187483)

Reading details for 24825 predictors from ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.1/weight_change_linear_clump_thresholding_0.1.score

Calculating scores for Chunk 1 of 35

Profiles saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.1/weight_change_linear_clump_thresholding_0.1_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:09:21 2025 and ended at Sun May 25 09:09:28 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


# perform thresholding considering the threshold 0.01 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.01/weight_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 0.01

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e-02, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

33716 of the 3187483 predictors in ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e-02
All of these are in the data

Data contain 959 samples and 3187483 predictors (will be using 959 and 33716)

Reading p-values from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr1_2333660_C_T 4.5468e-03 | chr1_2334442_C_T 4.4387e-03 | chr1_3591297_G_A 7.3025e-03

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 20 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 1686; stay tuned for updates ;)

The bit-size has now been set to 20

Pass 2: Thinning for Chunk 1 of 339; stay tuned for updates ;)

Thinning complete: 3751 predictors kept (saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.01/weight_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors.in), 29965 lost (./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.01/weight_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.01/weight_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:09:28 2025 and ended at Sun May 25 09:10:08 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.01/weight_change_linear_clump_thresholding_0.01
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.01/weight_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 3751 predictors to extract from ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.01/weight_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors.in

Data contain 959 samples and 3187483 predictors (will be using 959 and 3751)

Reading phenotypes for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 6 covariates for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.01/weight_change_linear_clump_thresholding_0.01.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.01/weight_change_linear_clump_thresholding_0.01.combined")

Performing linear regression for Chunk 1 of 22
Performing linear regression for Chunk 11 of 22
Performing linear regression for Chunk 21 of 22

Main results saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.01/weight_change_linear_clump_thresholding_0.01.assoc, with a summary version in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.01/weight_change_linear_clump_thresholding_0.01.summaries, p-values in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.01/weight_change_linear_clump_thresholding_0.01.pvalues and score file in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.01/weight_change_linear_clump_thresholding_0.01.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:10:08 2025 and ended at Sun May 25 09:10:15 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.01/weight_change_linear_clump_thresholding_0.01_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.01/weight_change_linear_clump_thresholding_0.01.score
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.01/weight_change_linear_clump_thresholding_0.01.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 959 samples and 3187483 predictors (will be using 959 and 3187483)

Reading details for 3751 predictors from ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.01/weight_change_linear_clump_thresholding_0.01.score

Reading phenotypes for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 22

Profiles saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.01/weight_change_linear_clump_thresholding_0.01_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.2275 to 0.8408, saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.01/weight_change_linear_clump_thresholding_0.01_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:10:15 2025 and ended at Sun May 25 09:10:21 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.01/weight_change_linear_clump_thresholding_0.01_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.01/weight_change_linear_clump_thresholding_0.01.score
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.01/weight_change_linear_clump_thresholding_0.01.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 959 samples and 3187483 predictors (will be using 959 and 3187483)

Reading details for 3751 predictors from ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.01/weight_change_linear_clump_thresholding_0.01.score

Calculating scores for Chunk 1 of 22

Profiles saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.01/weight_change_linear_clump_thresholding_0.01_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:10:21 2025 and ended at Sun May 25 09:10:28 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


# perform thresholding considering the threshold 0.001 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.001/weight_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 0.001

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e-03, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

3821 of the 3187483 predictors in ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e-03
All of these are in the data

Data contain 959 samples and 3187483 predictors (will be using 959 and 3821)

Reading p-values from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr1_4630858_T_C 9.3702e-04 | chr1_4631800_C_T 2.7480e-04 | chr1_9056840_A_C 6.6467e-04

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 20 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 192; stay tuned for updates ;)

The bit-size has now been set to 20

Pass 2: Thinning for Chunk 1 of 42; stay tuned for updates ;)

Thinning complete: 518 predictors kept (saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.001/weight_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors.in), 3303 lost (./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.001/weight_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.001/weight_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:10:28 2025 and ended at Sun May 25 09:11:07 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.001/weight_change_linear_clump_thresholding_0.001
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.001/weight_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 518 predictors to extract from ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.001/weight_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors.in

Data contain 959 samples and 3187483 predictors (will be using 959 and 518)

Reading phenotypes for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 6 covariates for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.001/weight_change_linear_clump_thresholding_0.001.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.001/weight_change_linear_clump_thresholding_0.001.combined")

Performing linear regression for Chunk 1 of 22
Performing linear regression for Chunk 11 of 22
Performing linear regression for Chunk 21 of 22

Main results saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.001/weight_change_linear_clump_thresholding_0.001.assoc, with a summary version in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.001/weight_change_linear_clump_thresholding_0.001.summaries, p-values in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.001/weight_change_linear_clump_thresholding_0.001.pvalues and score file in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.001/weight_change_linear_clump_thresholding_0.001.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:11:07 2025 and ended at Sun May 25 09:11:14 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.001/weight_change_linear_clump_thresholding_0.001_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.001/weight_change_linear_clump_thresholding_0.001.score
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.001/weight_change_linear_clump_thresholding_0.001.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 959 samples and 3187483 predictors (will be using 959 and 3187483)

Reading details for 518 predictors from ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.001/weight_change_linear_clump_thresholding_0.001.score

Reading phenotypes for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 22

Profiles saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.001/weight_change_linear_clump_thresholding_0.001_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.2275 to 0.7930, saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.001/weight_change_linear_clump_thresholding_0.001_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:11:14 2025 and ended at Sun May 25 09:11:21 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.001/weight_change_linear_clump_thresholding_0.001_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.001/weight_change_linear_clump_thresholding_0.001.score
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.001/weight_change_linear_clump_thresholding_0.001.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 959 samples and 3187483 predictors (will be using 959 and 3187483)

Reading details for 518 predictors from ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.001/weight_change_linear_clump_thresholding_0.001.score

Calculating scores for Chunk 1 of 22

Profiles saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.001/weight_change_linear_clump_thresholding_0.001_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:11:21 2025 and ended at Sun May 25 09:11:28 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


# perform thresholding considering the threshold 0.0001 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.0001/weight_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 0.0001

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e-04, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

324 of the 3187483 predictors in ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e-04
All of these are in the data

Data contain 959 samples and 3187483 predictors (will be using 959 and 324)

Reading p-values from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr1_9061620_T_A 8.7466e-05 | chr1_9061628_T_C 8.7466e-05 | chr1_22692911_C_G 5.8062e-05

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 20 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 17; stay tuned for updates ;)

The bit-size has now been set to 20

Pass 2: Thinning for Chunk 1 of 5; stay tuned for updates ;)

Thinning complete: 62 predictors kept (saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.0001/weight_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors.in), 262 lost (./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.0001/weight_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.0001/weight_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:11:28 2025 and ended at Sun May 25 09:12:06 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.0001/weight_change_linear_clump_thresholding_0.0001
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.0001/weight_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 62 predictors to extract from ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.0001/weight_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors.in

Data contain 959 samples and 3187483 predictors (will be using 959 and 62)

Reading phenotypes for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 6 covariates for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.0001/weight_change_linear_clump_thresholding_0.0001.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.0001/weight_change_linear_clump_thresholding_0.0001.combined")

Performing linear regression for Chunk 1 of 21
Performing linear regression for Chunk 11 of 21
Performing linear regression for Chunk 21 of 21

Main results saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.0001/weight_change_linear_clump_thresholding_0.0001.assoc, with a summary version in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.0001/weight_change_linear_clump_thresholding_0.0001.summaries, p-values in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.0001/weight_change_linear_clump_thresholding_0.0001.pvalues and score file in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.0001/weight_change_linear_clump_thresholding_0.0001.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:12:06 2025 and ended at Sun May 25 09:12:13 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.0001/weight_change_linear_clump_thresholding_0.0001_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.0001/weight_change_linear_clump_thresholding_0.0001.score
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.0001/weight_change_linear_clump_thresholding_0.0001.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 959 samples and 3187483 predictors (will be using 959 and 3187483)

Reading details for 62 predictors from ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.0001/weight_change_linear_clump_thresholding_0.0001.score

Reading phenotypes for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 21

Profiles saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.0001/weight_change_linear_clump_thresholding_0.0001_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.2275 to 0.6014, saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.0001/weight_change_linear_clump_thresholding_0.0001_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:12:13 2025 and ended at Sun May 25 09:12:20 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.0001/weight_change_linear_clump_thresholding_0.0001_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.0001/weight_change_linear_clump_thresholding_0.0001.score
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.0001/weight_change_linear_clump_thresholding_0.0001.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 959 samples and 3187483 predictors (will be using 959 and 3187483)

Reading details for 62 predictors from ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.0001/weight_change_linear_clump_thresholding_0.0001.score

Calculating scores for Chunk 1 of 21

Profiles saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_0.0001/weight_change_linear_clump_thresholding_0.0001_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:12:20 2025 and ended at Sun May 25 09:12:26 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


# perform thresholding considering the threshold 1e-05 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1e-05/weight_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 1e-05

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e-05, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

61 of the 3187483 predictors in ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e-05
All of these are in the data

Data contain 959 samples and 3187483 predictors (will be using 959 and 61)

Reading p-values from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr5_172476765_T_C 9.1333e-06 | chr8_15296615_T_C 9.5251e-06 | chr8_15301248_A_G 8.1996e-06

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 20 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 4; stay tuned for updates ;)

The bit-size has now been set to 20

Pass 2: Thinning for Chunk 1 of 1; stay tuned for updates ;)

Thinning complete: 4 predictors kept (saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1e-05/weight_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors.in), 57 lost (./results/final_results/analysis_full_data/weight_change/clump_thresholding_1e-05/weight_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/weight_change/clump_thresholding_1e-05/weight_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:12:26 2025 and ended at Sun May 25 09:13:04 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1e-05/weight_change_linear_clump_thresholding_1e-05
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1e-05/weight_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 4 predictors to extract from ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1e-05/weight_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors.in

Data contain 959 samples and 3187483 predictors (will be using 959 and 4)

Reading phenotypes for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 6 covariates for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1e-05/weight_change_linear_clump_thresholding_1e-05.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1e-05/weight_change_linear_clump_thresholding_1e-05.combined")

Performing linear regression for Chunk 1 of 4

Main results saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1e-05/weight_change_linear_clump_thresholding_1e-05.assoc, with a summary version in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1e-05/weight_change_linear_clump_thresholding_1e-05.summaries, p-values in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1e-05/weight_change_linear_clump_thresholding_1e-05.pvalues and score file in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1e-05/weight_change_linear_clump_thresholding_1e-05.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:13:04 2025 and ended at Sun May 25 09:13:11 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1e-05/weight_change_linear_clump_thresholding_1e-05_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1e-05/weight_change_linear_clump_thresholding_1e-05.score
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1e-05/weight_change_linear_clump_thresholding_1e-05.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 959 samples and 3187483 predictors (will be using 959 and 3187483)

Reading details for 4 predictors from ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1e-05/weight_change_linear_clump_thresholding_1e-05.score

Reading phenotypes for 959 samples from ./results/final_results/analysis_full_data/weight_change/weight_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 4

Profiles saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1e-05/weight_change_linear_clump_thresholding_1e-05_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.2275 to 0.2275, saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1e-05/weight_change_linear_clump_thresholding_1e-05_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:13:11 2025 and ended at Sun May 25 09:13:18 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1e-05/weight_change_linear_clump_thresholding_1e-05_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1e-05/weight_change_linear_clump_thresholding_1e-05.score
--bfile ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1e-05/weight_change_linear_clump_thresholding_1e-05.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 959 samples from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3187483 predictors from ./data/plink_filesets/small_set_predictors/weight_change_filesets/weight_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 959 samples and 3187483 predictors (will be using 959 and 3187483)

Reading details for 4 predictors from ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1e-05/weight_change_linear_clump_thresholding_1e-05.score

Calculating scores for Chunk 1 of 4

Profiles saved in ./results/final_results/analysis_full_data/weight_change/clump_thresholding_1e-05/weight_change_linear_clump_thresholding_1e-05_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:13:18 2025 and ended at Sun May 25 09:13:24 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


###### check we have used the correct samples in both analyses ######

## load the FAM files used for training and test ##

## samples in files generated by elastic net ##

## samples in files generated by linear ##

###### plot quantiles of PRS against phenotype ######

## prepare folders ##


## load the phenotype data before transformation ##

## process it ##

# get only the samples finally included in modelling #

# split the ID into FID and IID #

# check that the new variables has been correctly create #

## open the plot ##

# Create a figure with 8 subplots arranged in a grid (e.g., 2 rows x 4 columns) #

## Flatten the axes array for easier iteration ##

## create a list of models ##

## iterate across models ##

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          4            -5.4825            -2.70              0.6300
1         14            -1.9000             1.90              4.1300
2         15            -0.6825             1.45              5.7425
3         13            -2.1000             1.40              5.6500
4          9            -4.3475            -0.45              2.8825
5         17            -1.8425             2.95              5.4950
6          2            -9.2475            -4.40             -0.0925
7         11            -2.9000             0.30              2.9000
8         10            -3.7475            -0.50              2.6125
9          8            -5.0300            -0.90              1.9475
10        19             1.4000             3.95              9.6250
11         7            -5.4425            -1.90              2.5125
12        16            -3.1325             1.85              4.8825
13        18            -0.8825             3.60              8.5600
14        20             1.8700             6.40             24.5350
15        12            -2.7825             1.05              3.5000
16         3            -7.3475            -3.45             -0.2000
17         6            -5.5025            -1.60              2.1650
18         1           -23.1900            -5.85              0.7125
19         5            -5.9300            -1.40              1.3000

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          6            -5.3775            -1.60              2.1650
1         10            -3.7475             0.10              3.0650
2         16            -2.0825             2.10              6.3425
3         13            -2.5600             1.60              4.6900
4          9            -3.4650            -0.75              2.0950
5         17            -1.0475             2.90              6.8150
6          3            -6.7775            -3.30              0.7900
7         11            -2.8850             0.50              3.3400
8         12            -1.5475             0.85              3.0125
9         14            -2.3475             0.95              3.6775
10         2            -9.2475            -4.40              0.2650
11        18            -0.3475             3.65              7.2650
12         8            -4.9950            -1.35              1.9475
13        19             1.4000             4.30             10.4600
14        15            -1.0475             2.05              5.4975
15        20             3.1350             6.40             24.5350
16         7            -5.3600            -0.75              2.0825
17         4            -5.7725            -2.40              0.2650
18         1           -23.1900            -6.10             -0.0100
19         5            -6.2475            -2.10              0.4825

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          5            -6.2475            -2.45              0.2475
1         10            -3.7475            -0.20              2.8825
2         16            -1.9775             2.65              6.3950
3         13            -2.4375             1.35              3.9475
4          9            -4.2075            -0.60              1.9475
5          3            -6.1000            -3.30              0.7900
6         12            -2.0825             1.00              3.0475
7         11            -2.8850             0.90              3.5000
8         14            -2.0650             1.10              3.7650
9          6            -5.4425            -1.60              1.9650
10         2            -9.2475            -4.40             -0.0050
11        18            -0.3475             3.80              7.9000
12         7            -5.2725            -0.95              2.3475
13        17            -1.0650             2.80              6.0650
14         8            -4.8725            -1.15              1.8775
15        19             1.4000             4.10             10.1975
16        15            -3.1675             1.95              3.2825
17        20             3.1350             6.80             24.5350
18         4            -6.1650            -1.70              0.6475
19         1           -23.1900            -5.95              0.2825

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          5            -6.8200            -2.40              1.6425
1          9            -3.7125            -0.15              2.6125
2         16            -0.9425             2.05              6.3425
3         13            -2.1650             1.15              3.5825
4          8            -4.9950            -0.75              1.9775
5          4            -6.0000            -2.10              0.4825
6         11            -3.4100             0.30              2.9000
7         12            -2.8825             1.05              3.6650
8         14            -1.7950             1.25              5.4800
9          6            -4.6475            -1.60              1.9650
10         2            -7.6825            -4.20              0.5550
11        10            -2.9650             0.40              3.3950
12        18            -0.3475             3.75              8.5600
13         3            -7.3125            -3.65              0.0475
14        17            -1.8075             2.60              5.7425
15        20             2.6050             5.90             24.5350
16        15            -2.3475             1.60              4.8825
17        19             1.4000             4.30              9.8000
18         7            -5.7475            -1.40              1.2825
19         1           -23.1900            -6.25              0.2825

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          3            -8.7550            -2.95              0.6125
1          9            -5.6775            -0.20              2.6825
2         10            -2.6650             0.50              3.3775
3         12            -2.7600             1.30              3.8300
4         15            -2.9825             1.50              4.0000
5          4            -7.4650            -2.25              1.2650
6          5            -7.4300            -2.45              2.0000
7         11            -3.8000             0.00              3.7550
8          2            -8.3600            -3.70              0.8000
9          8            -4.3650            -0.95              2.2000
10        20             2.2525             6.40             24.5350
11         6            -5.9125            -0.70              2.9000
12        13            -3.6250             0.15              3.4300
13        14            -2.1825             1.10              5.4600
14        19             0.2450             3.85              9.4150
15         7            -5.7475            -0.85              2.2250
16        18            -0.9250             3.20              7.6550
17        17            -1.5550             2.80              8.5600
18        16            -1.0650             1.75              6.1200
19         1           -23.1900            -5.60             -0.0275

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0         10            -4.7300            -0.05              3.2475
1         13            -4.2950             0.25              5.1300
2          6            -4.7300            -0.70              2.8475
3          8            -6.6525            -0.30              5.7375
4         15            -3.4125             1.65              4.8600
5          2           -10.5850            -2.30              2.6600
6          9            -5.7725            -0.45              2.9475
7         16            -3.8825             1.30              7.4325
8         19            -3.0675             3.20             11.5125
9          1           -22.6650            -2.80              1.2000
10         7            -5.4125            -1.75              5.2850
11        14            -4.2025             1.60              6.1475
12         3           -11.1900            -3.25              1.9475
13         5            -8.3875            -1.00              2.0000
14        11            -5.2950             0.50              6.9000
15        20            -1.9775             4.60             14.5900
16        17            -1.8825             1.70              5.7250
17        12            -6.8250             0.30              6.2200
18        18            -1.5475             2.35              9.5025
19         4            -7.3250            -1.00              2.4950

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          8            -5.5325             0.60              7.4325
1          7            -7.3200             0.10              5.5850
2         13            -5.6375             1.10              8.9650
3         12            -7.3125             0.25              7.4125
4          6            -8.8775            -0.05              8.2975
5         10            -5.3700             0.70              6.4800
6          5            -6.9425            -0.25              3.6675
7          1           -10.4975            -2.30              7.3750
8         11            -6.6500             0.00              7.9000
9         14            -3.6750             1.35             13.4875
10         2            -6.3400            -0.60              3.0200
11         3            -9.0225            -0.10              6.0225
12         4            -8.3250            -2.50              4.1800
13         9             1.7225             2.15              2.5775

# plot the results #

# finish the plot #

###### manhattan plots ######

## load assoc results to pandas ##
         Chromosome           Predictor  Basepair  ... CallRate MachR2  SPA_Status
0                 1     chr1_858952_G_A    858952  ...      1.0    NaN    NOT_USED
1                 1     chr1_905373_T_C    905373  ...      1.0    NaN    NOT_USED
2                 1     chr1_911428_C_T    911428  ...      1.0    NaN    NOT_USED
3                 1     chr1_918870_A_G    918870  ...      1.0    NaN    NOT_USED
4                 1     chr1_931513_T_C    931513  ...      1.0    NaN    NOT_USED
...             ...                 ...       ...  ...      ...    ...         ...
3187478          22  chr22_50749145_C_T  50749145  ...      1.0    NaN    NOT_USED
3187479          22  chr22_50749470_T_C  50749470  ...      1.0    NaN    NOT_USED
3187480          22  chr22_50749890_T_G  50749890  ...      1.0    NaN    NOT_USED
3187481          22  chr22_50751123_G_T  50751123  ...      1.0    NaN    NOT_USED
3187482          22  chr22_50752652_C_G  50752652  ...      1.0    NaN    NOT_USED

[3187483 rows x 13 columns]

## check we have the correct columns ##

## check we have the correct dtypes ##

## calculate -log_10(pvalue) ##

## convert chromosome to category and then sort by it ##

## sort rows by chromosome and basepair position ##

## Creates a new column called ind in the assoc_results DataFrame ##

## Groups the assoc_results DataFrame by the Chromosome column ##

## make manhattan plot ##

# open the plot #

# Add a title to the plot #

# Define a colorblind-friendly palette #

# iterate across chromosomes #

# set the x-axis ticks and labels of these ticks #

# add p-value thresholds as horizontal lines #

# set axis limits #

# set the axis label #

# Increase font size for tick labels #

# add a legend to the subplot #

# save the plot as a static image #

###### compress the results ######

## remove bed and bim plink files initialles used as input (compressed files already created) ##


## elastic outputs ##


## linear outputs ##

# first raw outputs before clumping #


# then outputs after clumping #







#######################################
#######################################
FINISH
#######################################
#######################################
