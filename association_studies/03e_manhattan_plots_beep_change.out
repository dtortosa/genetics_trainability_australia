
#######################################
#######################################
checking function to print nicely: header 1
#######################################
#######################################

###### checking function to print nicely: header 2 ######

## checking function to print nicely: header 3 ##

# checking function to print nicely: header 4 #

#######################################
#######################################
check behaviour run_bash
#######################################
#######################################

###### see working directory ######
/home/UGR002/dsalazar/climahealth/combat_genes


###### list files/folders there ######
03a_association_analyses.sif
03b_prs_calculation_100_iter_large_set_predictors_beep_change.out
03b_prs_calculation_100_iter_large_set_predictors_distance_change.out
03b_prs_calculation_100_iter_large_set_predictors_vo2_change.out
03b_prs_calculation_100_iter_large_set_predictors_weight_change.out
03b_prs_calculation_100_iter_small_set_predictors_beep_change.out
03b_prs_calculation_100_iter_small_set_predictors_distance_change.out
03b_prs_calculation_100_iter_small_set_predictors_vo2_change.out
03b_prs_calculation_100_iter_small_set_predictors_weight_change.out
03d_processing_results.out
03e_manhattan_plots_beep_change.out
03e_manhattan_plots_distance_change.out
data
results
scripts


#######################################
#######################################
For phenotype beep_change, and the small dataset of covariates
#######################################
#######################################

###### initial preparations ######

## create folders for results ##


## load the phenotype data ##
                 family_id AGRF code  ...  Week 1 Beep test  beep_change
0     combat_ILGSA24-17303  0200ADMM  ...          0.269686    -0.515363
1     combat_ILGSA24-17303  0200ASJM  ...          1.433991    -2.463212
2     combat_ILGSA24-17303  0200BHNM  ...         -1.713535    -0.903724
3     combat_ILGSA24-17303  0200CBOM  ...         -0.922440    -0.787307
4     combat_ILGSA24-17303  0200CDFM  ...         -0.153565    -0.903724
...                    ...       ...  ...               ...          ...
1013  combat_ILGSA24-17873  8098ATDN  ...          0.394904     0.342090
1014  combat_ILGSA24-17873  8098RSJN  ...         -0.234031    -1.025520
1015  combat_ILGSA24-17873  8098TSAN  ...          0.807651    -0.656020
1016  combat_ILGSA24-17873  8099AJNN  ...          0.269686     0.342090
1017  combat_ILGSA24-17873  8099COSN  ...          0.394904     0.074082

[1018 rows x 8 columns]

## specify the covariates ##
Index(['PCA5', 'PCA8', 'sex_code', 'Week 1 Body Mass', 'Week 1 Beep test'], dtype='object')

## decompress bim and bed files with all sample generated after NA cleaning ##


###### prepare LDAK inputs ######

###### calculate elastic PRS ######

## run the PRS with --elastic ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--elastic ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--LOCO NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Constructing elastic net PRS

Will consider five values for the predictor scaling (alpha = -1, -0.75, -0.5, -0.25 and 0); to instead specify the value, use "--power" (or use "--powerfile" to provide a range of values)

Will use the default prior parameter choices (saved in the file ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic.parameters); to instead specify your own, use "--parameters"

Will select the best prior parameters via cross-validation, using 0.10 randomly-picked test samples (use "--cv-proportion" to change this proportion, "--cv-samples" to explicitly specify the test samples, or "--cv-skip" to turn off cross-validation)

Will always include the LOCO polygenic contribution, regardless of their estimated accuracy

When constructing PRS, will scan the data at most 10 times (change this using "--num-scans")

All heritability estimates must be between 0.01 and 0.8000 (change the upper bound using "--max-her")

Will use either three or ten random vectors for Monte Carlo operations (decided based on the number of samples); change this number using "--num-random-vectors"

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 4 covariates for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic.combined")

When performing cross-validation, will use 917 samples to train models and 101 to test their accuracy

Warning, to process the data requires 6.1 Gb; sorry this can not be reduced

Warning, to perform the analysis requires approximately 2.1 Gb; sorry, this can not be reduced

Estimating per-predictor heritabilities using Randomized Haseman-Elston Regression with 10 random vectors

Will divide the predictors into 20 partitions (change this using "--num-divides")
Will exclude chunks containing a predictor with estimated variance explained greater than 0.0982 (change this using "--max-cor")

Calculating traces for Chunk 1 of 12458
Calculating traces for Chunk 201 of 12458
Calculating traces for Chunk 401 of 12458
Calculating traces for Chunk 601 of 12458
Calculating traces for Chunk 801 of 12458
Calculating traces for Chunk 1001 of 12458
Calculating traces for Chunk 1201 of 12458
Calculating traces for Chunk 1401 of 12458
Calculating traces for Chunk 1601 of 12458
Calculating traces for Chunk 1801 of 12458
Calculating traces for Chunk 2001 of 12458
Calculating traces for Chunk 2201 of 12458
Calculating traces for Chunk 2401 of 12458
Calculating traces for Chunk 2601 of 12458
Calculating traces for Chunk 2801 of 12458
Calculating traces for Chunk 3001 of 12458
Calculating traces for Chunk 3201 of 12458
Calculating traces for Chunk 3401 of 12458
Calculating traces for Chunk 3601 of 12458
Calculating traces for Chunk 3801 of 12458
Calculating traces for Chunk 4001 of 12458
Calculating traces for Chunk 4201 of 12458
Calculating traces for Chunk 4401 of 12458
Calculating traces for Chunk 4601 of 12458
Calculating traces for Chunk 4801 of 12458
Calculating traces for Chunk 5001 of 12458
Calculating traces for Chunk 5201 of 12458
Calculating traces for Chunk 5401 of 12458
Calculating traces for Chunk 5601 of 12458
Calculating traces for Chunk 5801 of 12458
Calculating traces for Chunk 6001 of 12458
Calculating traces for Chunk 6201 of 12458
Calculating traces for Chunk 6401 of 12458
Calculating traces for Chunk 6601 of 12458
Calculating traces for Chunk 6801 of 12458
Calculating traces for Chunk 7001 of 12458
Calculating traces for Chunk 7201 of 12458
Calculating traces for Chunk 7401 of 12458
Calculating traces for Chunk 7601 of 12458
Calculating traces for Chunk 7801 of 12458
Calculating traces for Chunk 8001 of 12458
Calculating traces for Chunk 8201 of 12458
Calculating traces for Chunk 8401 of 12458
Calculating traces for Chunk 8601 of 12458
Calculating traces for Chunk 8801 of 12458
Calculating traces for Chunk 9001 of 12458
Calculating traces for Chunk 9201 of 12458
Calculating traces for Chunk 9401 of 12458
Calculating traces for Chunk 9601 of 12458
Calculating traces for Chunk 9801 of 12458
Calculating traces for Chunk 10001 of 12458
Calculating traces for Chunk 10201 of 12458
Calculating traces for Chunk 10401 of 12458
Calculating traces for Chunk 10601 of 12458
Calculating traces for Chunk 10801 of 12458
Calculating traces for Chunk 11001 of 12458
Calculating traces for Chunk 11201 of 12458
Calculating traces for Chunk 11401 of 12458
Calculating traces for Chunk 11601 of 12458
Calculating traces for Chunk 11801 of 12458
Calculating traces for Chunk 12001 of 12458
Calculating traces for Chunk 12201 of 12458
Calculating traces for Chunk 12401 of 12458

Best power is -1.0000 and estimated heritability is 2.7208
Warning, the heritability is very high, so has been reduced to 0.8000

Time check: have so far spent 0.02 hours

Constructing 10 PRS using training samples
Will also make 33 MCMC REML models (using all samples)

Scan 1: estimating training effect sizes for Chunk 1 of 12458
Scan 1: estimating training effect sizes for Chunk 201 of 12458
Scan 1: estimating training effect sizes for Chunk 401 of 12458
Scan 1: estimating training effect sizes for Chunk 601 of 12458
Scan 1: estimating training effect sizes for Chunk 801 of 12458
Scan 1: estimating training effect sizes for Chunk 1001 of 12458
Scan 1: estimating training effect sizes for Chunk 1201 of 12458
Scan 1: estimating training effect sizes for Chunk 1401 of 12458
Scan 1: estimating training effect sizes for Chunk 1601 of 12458
Scan 1: estimating training effect sizes for Chunk 1801 of 12458
Scan 1: estimating training effect sizes for Chunk 2001 of 12458
Scan 1: estimating training effect sizes for Chunk 2201 of 12458
Scan 1: estimating training effect sizes for Chunk 2401 of 12458
Scan 1: estimating training effect sizes for Chunk 2601 of 12458
Scan 1: estimating training effect sizes for Chunk 2801 of 12458
Scan 1: estimating training effect sizes for Chunk 3001 of 12458
Scan 1: estimating training effect sizes for Chunk 3201 of 12458
Scan 1: estimating training effect sizes for Chunk 3401 of 12458
Scan 1: estimating training effect sizes for Chunk 3601 of 12458
Scan 1: estimating training effect sizes for Chunk 3801 of 12458
Scan 1: estimating training effect sizes for Chunk 4001 of 12458
Scan 1: estimating training effect sizes for Chunk 4201 of 12458
Scan 1: estimating training effect sizes for Chunk 4401 of 12458
Scan 1: estimating training effect sizes for Chunk 4601 of 12458
Scan 1: estimating training effect sizes for Chunk 4801 of 12458
Scan 1: estimating training effect sizes for Chunk 5001 of 12458
Scan 1: estimating training effect sizes for Chunk 5201 of 12458
Scan 1: estimating training effect sizes for Chunk 5401 of 12458
Scan 1: estimating training effect sizes for Chunk 5601 of 12458
Scan 1: estimating training effect sizes for Chunk 5801 of 12458
Scan 1: estimating training effect sizes for Chunk 6001 of 12458
Scan 1: estimating training effect sizes for Chunk 6201 of 12458
Scan 1: estimating training effect sizes for Chunk 6401 of 12458
Scan 1: estimating training effect sizes for Chunk 6601 of 12458
Scan 1: estimating training effect sizes for Chunk 6801 of 12458
Scan 1: estimating training effect sizes for Chunk 7001 of 12458
Scan 1: estimating training effect sizes for Chunk 7201 of 12458
Scan 1: estimating training effect sizes for Chunk 7401 of 12458
Scan 1: estimating training effect sizes for Chunk 7601 of 12458
Scan 1: estimating training effect sizes for Chunk 7801 of 12458
Scan 1: estimating training effect sizes for Chunk 8001 of 12458
Scan 1: estimating training effect sizes for Chunk 8201 of 12458
Scan 1: estimating training effect sizes for Chunk 8401 of 12458
Scan 1: estimating training effect sizes for Chunk 8601 of 12458
Scan 1: estimating training effect sizes for Chunk 8801 of 12458
Scan 1: estimating training effect sizes for Chunk 9001 of 12458
Scan 1: estimating training effect sizes for Chunk 9201 of 12458
Scan 1: estimating training effect sizes for Chunk 9401 of 12458
Scan 1: estimating training effect sizes for Chunk 9601 of 12458
Scan 1: estimating training effect sizes for Chunk 9801 of 12458
Scan 1: estimating training effect sizes for Chunk 10001 of 12458
Scan 1: estimating training effect sizes for Chunk 10201 of 12458
Scan 1: estimating training effect sizes for Chunk 10401 of 12458
Scan 1: estimating training effect sizes for Chunk 10601 of 12458
Scan 1: estimating training effect sizes for Chunk 10801 of 12458
Scan 1: estimating training effect sizes for Chunk 11001 of 12458
Scan 1: estimating training effect sizes for Chunk 11201 of 12458
Scan 1: estimating training effect sizes for Chunk 11401 of 12458
Scan 1: estimating training effect sizes for Chunk 11601 of 12458
Scan 1: estimating training effect sizes for Chunk 11801 of 12458
Scan 1: estimating training effect sizes for Chunk 12001 of 12458
Scan 1: estimating training effect sizes for Chunk 12201 of 12458
Scan 1: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 3.54

Scan 2: estimating training effect sizes for Chunk 1 of 12458
Scan 2: estimating training effect sizes for Chunk 201 of 12458
Scan 2: estimating training effect sizes for Chunk 401 of 12458
Scan 2: estimating training effect sizes for Chunk 601 of 12458
Scan 2: estimating training effect sizes for Chunk 801 of 12458
Scan 2: estimating training effect sizes for Chunk 1001 of 12458
Scan 2: estimating training effect sizes for Chunk 1201 of 12458
Scan 2: estimating training effect sizes for Chunk 1401 of 12458
Scan 2: estimating training effect sizes for Chunk 1601 of 12458
Scan 2: estimating training effect sizes for Chunk 1801 of 12458
Scan 2: estimating training effect sizes for Chunk 2001 of 12458
Scan 2: estimating training effect sizes for Chunk 2201 of 12458
Scan 2: estimating training effect sizes for Chunk 2401 of 12458
Scan 2: estimating training effect sizes for Chunk 2601 of 12458
Scan 2: estimating training effect sizes for Chunk 2801 of 12458
Scan 2: estimating training effect sizes for Chunk 3001 of 12458
Scan 2: estimating training effect sizes for Chunk 3201 of 12458
Scan 2: estimating training effect sizes for Chunk 3401 of 12458
Scan 2: estimating training effect sizes for Chunk 3601 of 12458
Scan 2: estimating training effect sizes for Chunk 3801 of 12458
Scan 2: estimating training effect sizes for Chunk 4001 of 12458
Scan 2: estimating training effect sizes for Chunk 4201 of 12458
Scan 2: estimating training effect sizes for Chunk 4401 of 12458
Scan 2: estimating training effect sizes for Chunk 4601 of 12458
Scan 2: estimating training effect sizes for Chunk 4801 of 12458
Scan 2: estimating training effect sizes for Chunk 5001 of 12458
Scan 2: estimating training effect sizes for Chunk 5201 of 12458
Scan 2: estimating training effect sizes for Chunk 5401 of 12458
Scan 2: estimating training effect sizes for Chunk 5601 of 12458
Scan 2: estimating training effect sizes for Chunk 5801 of 12458
Scan 2: estimating training effect sizes for Chunk 6001 of 12458
Scan 2: estimating training effect sizes for Chunk 6201 of 12458
Scan 2: estimating training effect sizes for Chunk 6401 of 12458
Scan 2: estimating training effect sizes for Chunk 6601 of 12458
Scan 2: estimating training effect sizes for Chunk 6801 of 12458
Scan 2: estimating training effect sizes for Chunk 7001 of 12458
Scan 2: estimating training effect sizes for Chunk 7201 of 12458
Scan 2: estimating training effect sizes for Chunk 7401 of 12458
Scan 2: estimating training effect sizes for Chunk 7601 of 12458
Scan 2: estimating training effect sizes for Chunk 7801 of 12458
Scan 2: estimating training effect sizes for Chunk 8001 of 12458
Scan 2: estimating training effect sizes for Chunk 8201 of 12458
Scan 2: estimating training effect sizes for Chunk 8401 of 12458
Scan 2: estimating training effect sizes for Chunk 8601 of 12458
Scan 2: estimating training effect sizes for Chunk 8801 of 12458
Scan 2: estimating training effect sizes for Chunk 9001 of 12458
Scan 2: estimating training effect sizes for Chunk 9201 of 12458
Scan 2: estimating training effect sizes for Chunk 9401 of 12458
Scan 2: estimating training effect sizes for Chunk 9601 of 12458
Scan 2: estimating training effect sizes for Chunk 9801 of 12458
Scan 2: estimating training effect sizes for Chunk 10001 of 12458
Scan 2: estimating training effect sizes for Chunk 10201 of 12458
Scan 2: estimating training effect sizes for Chunk 10401 of 12458
Scan 2: estimating training effect sizes for Chunk 10601 of 12458
Scan 2: estimating training effect sizes for Chunk 10801 of 12458
Scan 2: estimating training effect sizes for Chunk 11001 of 12458
Scan 2: estimating training effect sizes for Chunk 11201 of 12458
Scan 2: estimating training effect sizes for Chunk 11401 of 12458
Scan 2: estimating training effect sizes for Chunk 11601 of 12458
Scan 2: estimating training effect sizes for Chunk 11801 of 12458
Scan 2: estimating training effect sizes for Chunk 12001 of 12458
Scan 2: estimating training effect sizes for Chunk 12201 of 12458
Scan 2: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 3.36

Scan 3: estimating training effect sizes for Chunk 1 of 12458
Scan 3: estimating training effect sizes for Chunk 201 of 12458
Scan 3: estimating training effect sizes for Chunk 401 of 12458
Scan 3: estimating training effect sizes for Chunk 601 of 12458
Scan 3: estimating training effect sizes for Chunk 801 of 12458
Scan 3: estimating training effect sizes for Chunk 1001 of 12458
Scan 3: estimating training effect sizes for Chunk 1201 of 12458
Scan 3: estimating training effect sizes for Chunk 1401 of 12458
Scan 3: estimating training effect sizes for Chunk 1601 of 12458
Scan 3: estimating training effect sizes for Chunk 1801 of 12458
Scan 3: estimating training effect sizes for Chunk 2001 of 12458
Scan 3: estimating training effect sizes for Chunk 2201 of 12458
Scan 3: estimating training effect sizes for Chunk 2401 of 12458
Scan 3: estimating training effect sizes for Chunk 2601 of 12458
Scan 3: estimating training effect sizes for Chunk 2801 of 12458
Scan 3: estimating training effect sizes for Chunk 3001 of 12458
Scan 3: estimating training effect sizes for Chunk 3201 of 12458
Scan 3: estimating training effect sizes for Chunk 3401 of 12458
Scan 3: estimating training effect sizes for Chunk 3601 of 12458
Scan 3: estimating training effect sizes for Chunk 3801 of 12458
Scan 3: estimating training effect sizes for Chunk 4001 of 12458
Scan 3: estimating training effect sizes for Chunk 4201 of 12458
Scan 3: estimating training effect sizes for Chunk 4401 of 12458
Scan 3: estimating training effect sizes for Chunk 4601 of 12458
Scan 3: estimating training effect sizes for Chunk 4801 of 12458
Scan 3: estimating training effect sizes for Chunk 5001 of 12458
Scan 3: estimating training effect sizes for Chunk 5201 of 12458
Scan 3: estimating training effect sizes for Chunk 5401 of 12458
Scan 3: estimating training effect sizes for Chunk 5601 of 12458
Scan 3: estimating training effect sizes for Chunk 5801 of 12458
Scan 3: estimating training effect sizes for Chunk 6001 of 12458
Scan 3: estimating training effect sizes for Chunk 6201 of 12458
Scan 3: estimating training effect sizes for Chunk 6401 of 12458
Scan 3: estimating training effect sizes for Chunk 6601 of 12458
Scan 3: estimating training effect sizes for Chunk 6801 of 12458
Scan 3: estimating training effect sizes for Chunk 7001 of 12458
Scan 3: estimating training effect sizes for Chunk 7201 of 12458
Scan 3: estimating training effect sizes for Chunk 7401 of 12458
Scan 3: estimating training effect sizes for Chunk 7601 of 12458
Scan 3: estimating training effect sizes for Chunk 7801 of 12458
Scan 3: estimating training effect sizes for Chunk 8001 of 12458
Scan 3: estimating training effect sizes for Chunk 8201 of 12458
Scan 3: estimating training effect sizes for Chunk 8401 of 12458
Scan 3: estimating training effect sizes for Chunk 8601 of 12458
Scan 3: estimating training effect sizes for Chunk 8801 of 12458
Scan 3: estimating training effect sizes for Chunk 9001 of 12458
Scan 3: estimating training effect sizes for Chunk 9201 of 12458
Scan 3: estimating training effect sizes for Chunk 9401 of 12458
Scan 3: estimating training effect sizes for Chunk 9601 of 12458
Scan 3: estimating training effect sizes for Chunk 9801 of 12458
Scan 3: estimating training effect sizes for Chunk 10001 of 12458
Scan 3: estimating training effect sizes for Chunk 10201 of 12458
Scan 3: estimating training effect sizes for Chunk 10401 of 12458
Scan 3: estimating training effect sizes for Chunk 10601 of 12458
Scan 3: estimating training effect sizes for Chunk 10801 of 12458
Scan 3: estimating training effect sizes for Chunk 11001 of 12458
Scan 3: estimating training effect sizes for Chunk 11201 of 12458
Scan 3: estimating training effect sizes for Chunk 11401 of 12458
Scan 3: estimating training effect sizes for Chunk 11601 of 12458
Scan 3: estimating training effect sizes for Chunk 11801 of 12458
Scan 3: estimating training effect sizes for Chunk 12001 of 12458
Scan 3: estimating training effect sizes for Chunk 12201 of 12458
Scan 3: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 3.03

Scan 4: estimating training effect sizes for Chunk 1 of 12458
Scan 4: estimating training effect sizes for Chunk 201 of 12458
Scan 4: estimating training effect sizes for Chunk 401 of 12458
Scan 4: estimating training effect sizes for Chunk 601 of 12458
Scan 4: estimating training effect sizes for Chunk 801 of 12458
Scan 4: estimating training effect sizes for Chunk 1001 of 12458
Scan 4: estimating training effect sizes for Chunk 1201 of 12458
Scan 4: estimating training effect sizes for Chunk 1401 of 12458
Scan 4: estimating training effect sizes for Chunk 1601 of 12458
Scan 4: estimating training effect sizes for Chunk 1801 of 12458
Scan 4: estimating training effect sizes for Chunk 2001 of 12458
Scan 4: estimating training effect sizes for Chunk 2201 of 12458
Scan 4: estimating training effect sizes for Chunk 2401 of 12458
Scan 4: estimating training effect sizes for Chunk 2601 of 12458
Scan 4: estimating training effect sizes for Chunk 2801 of 12458
Scan 4: estimating training effect sizes for Chunk 3001 of 12458
Scan 4: estimating training effect sizes for Chunk 3201 of 12458
Scan 4: estimating training effect sizes for Chunk 3401 of 12458
Scan 4: estimating training effect sizes for Chunk 3601 of 12458
Scan 4: estimating training effect sizes for Chunk 3801 of 12458
Scan 4: estimating training effect sizes for Chunk 4001 of 12458
Scan 4: estimating training effect sizes for Chunk 4201 of 12458
Scan 4: estimating training effect sizes for Chunk 4401 of 12458
Scan 4: estimating training effect sizes for Chunk 4601 of 12458
Scan 4: estimating training effect sizes for Chunk 4801 of 12458
Scan 4: estimating training effect sizes for Chunk 5001 of 12458
Scan 4: estimating training effect sizes for Chunk 5201 of 12458
Scan 4: estimating training effect sizes for Chunk 5401 of 12458
Scan 4: estimating training effect sizes for Chunk 5601 of 12458
Scan 4: estimating training effect sizes for Chunk 5801 of 12458
Scan 4: estimating training effect sizes for Chunk 6001 of 12458
Scan 4: estimating training effect sizes for Chunk 6201 of 12458
Scan 4: estimating training effect sizes for Chunk 6401 of 12458
Scan 4: estimating training effect sizes for Chunk 6601 of 12458
Scan 4: estimating training effect sizes for Chunk 6801 of 12458
Scan 4: estimating training effect sizes for Chunk 7001 of 12458
Scan 4: estimating training effect sizes for Chunk 7201 of 12458
Scan 4: estimating training effect sizes for Chunk 7401 of 12458
Scan 4: estimating training effect sizes for Chunk 7601 of 12458
Scan 4: estimating training effect sizes for Chunk 7801 of 12458
Scan 4: estimating training effect sizes for Chunk 8001 of 12458
Scan 4: estimating training effect sizes for Chunk 8201 of 12458
Scan 4: estimating training effect sizes for Chunk 8401 of 12458
Scan 4: estimating training effect sizes for Chunk 8601 of 12458
Scan 4: estimating training effect sizes for Chunk 8801 of 12458
Scan 4: estimating training effect sizes for Chunk 9001 of 12458
Scan 4: estimating training effect sizes for Chunk 9201 of 12458
Scan 4: estimating training effect sizes for Chunk 9401 of 12458
Scan 4: estimating training effect sizes for Chunk 9601 of 12458
Scan 4: estimating training effect sizes for Chunk 9801 of 12458
Scan 4: estimating training effect sizes for Chunk 10001 of 12458
Scan 4: estimating training effect sizes for Chunk 10201 of 12458
Scan 4: estimating training effect sizes for Chunk 10401 of 12458
Scan 4: estimating training effect sizes for Chunk 10601 of 12458
Scan 4: estimating training effect sizes for Chunk 10801 of 12458
Scan 4: estimating training effect sizes for Chunk 11001 of 12458
Scan 4: estimating training effect sizes for Chunk 11201 of 12458
Scan 4: estimating training effect sizes for Chunk 11401 of 12458
Scan 4: estimating training effect sizes for Chunk 11601 of 12458
Scan 4: estimating training effect sizes for Chunk 11801 of 12458
Scan 4: estimating training effect sizes for Chunk 12001 of 12458
Scan 4: estimating training effect sizes for Chunk 12201 of 12458
Scan 4: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.65

Scan 5: estimating training effect sizes for Chunk 1 of 12458
Scan 5: estimating training effect sizes for Chunk 201 of 12458
Scan 5: estimating training effect sizes for Chunk 401 of 12458
Scan 5: estimating training effect sizes for Chunk 601 of 12458
Scan 5: estimating training effect sizes for Chunk 801 of 12458
Scan 5: estimating training effect sizes for Chunk 1001 of 12458
Scan 5: estimating training effect sizes for Chunk 1201 of 12458
Scan 5: estimating training effect sizes for Chunk 1401 of 12458
Scan 5: estimating training effect sizes for Chunk 1601 of 12458
Scan 5: estimating training effect sizes for Chunk 1801 of 12458
Scan 5: estimating training effect sizes for Chunk 2001 of 12458
Scan 5: estimating training effect sizes for Chunk 2201 of 12458
Scan 5: estimating training effect sizes for Chunk 2401 of 12458
Scan 5: estimating training effect sizes for Chunk 2601 of 12458
Scan 5: estimating training effect sizes for Chunk 2801 of 12458
Scan 5: estimating training effect sizes for Chunk 3001 of 12458
Scan 5: estimating training effect sizes for Chunk 3201 of 12458
Scan 5: estimating training effect sizes for Chunk 3401 of 12458
Scan 5: estimating training effect sizes for Chunk 3601 of 12458
Scan 5: estimating training effect sizes for Chunk 3801 of 12458
Scan 5: estimating training effect sizes for Chunk 4001 of 12458
Scan 5: estimating training effect sizes for Chunk 4201 of 12458
Scan 5: estimating training effect sizes for Chunk 4401 of 12458
Scan 5: estimating training effect sizes for Chunk 4601 of 12458
Scan 5: estimating training effect sizes for Chunk 4801 of 12458
Scan 5: estimating training effect sizes for Chunk 5001 of 12458
Scan 5: estimating training effect sizes for Chunk 5201 of 12458
Scan 5: estimating training effect sizes for Chunk 5401 of 12458
Scan 5: estimating training effect sizes for Chunk 5601 of 12458
Scan 5: estimating training effect sizes for Chunk 5801 of 12458
Scan 5: estimating training effect sizes for Chunk 6001 of 12458
Scan 5: estimating training effect sizes for Chunk 6201 of 12458
Scan 5: estimating training effect sizes for Chunk 6401 of 12458
Scan 5: estimating training effect sizes for Chunk 6601 of 12458
Scan 5: estimating training effect sizes for Chunk 6801 of 12458
Scan 5: estimating training effect sizes for Chunk 7001 of 12458
Scan 5: estimating training effect sizes for Chunk 7201 of 12458
Scan 5: estimating training effect sizes for Chunk 7401 of 12458
Scan 5: estimating training effect sizes for Chunk 7601 of 12458
Scan 5: estimating training effect sizes for Chunk 7801 of 12458
Scan 5: estimating training effect sizes for Chunk 8001 of 12458
Scan 5: estimating training effect sizes for Chunk 8201 of 12458
Scan 5: estimating training effect sizes for Chunk 8401 of 12458
Scan 5: estimating training effect sizes for Chunk 8601 of 12458
Scan 5: estimating training effect sizes for Chunk 8801 of 12458
Scan 5: estimating training effect sizes for Chunk 9001 of 12458
Scan 5: estimating training effect sizes for Chunk 9201 of 12458
Scan 5: estimating training effect sizes for Chunk 9401 of 12458
Scan 5: estimating training effect sizes for Chunk 9601 of 12458
Scan 5: estimating training effect sizes for Chunk 9801 of 12458
Scan 5: estimating training effect sizes for Chunk 10001 of 12458
Scan 5: estimating training effect sizes for Chunk 10201 of 12458
Scan 5: estimating training effect sizes for Chunk 10401 of 12458
Scan 5: estimating training effect sizes for Chunk 10601 of 12458
Scan 5: estimating training effect sizes for Chunk 10801 of 12458
Scan 5: estimating training effect sizes for Chunk 11001 of 12458
Scan 5: estimating training effect sizes for Chunk 11201 of 12458
Scan 5: estimating training effect sizes for Chunk 11401 of 12458
Scan 5: estimating training effect sizes for Chunk 11601 of 12458
Scan 5: estimating training effect sizes for Chunk 11801 of 12458
Scan 5: estimating training effect sizes for Chunk 12001 of 12458
Scan 5: estimating training effect sizes for Chunk 12201 of 12458
Scan 5: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.43

Scan 6: estimating training effect sizes for Chunk 1 of 12458
Scan 6: estimating training effect sizes for Chunk 201 of 12458
Scan 6: estimating training effect sizes for Chunk 401 of 12458
Scan 6: estimating training effect sizes for Chunk 601 of 12458
Scan 6: estimating training effect sizes for Chunk 801 of 12458
Scan 6: estimating training effect sizes for Chunk 1001 of 12458
Scan 6: estimating training effect sizes for Chunk 1201 of 12458
Scan 6: estimating training effect sizes for Chunk 1401 of 12458
Scan 6: estimating training effect sizes for Chunk 1601 of 12458
Scan 6: estimating training effect sizes for Chunk 1801 of 12458
Scan 6: estimating training effect sizes for Chunk 2001 of 12458
Scan 6: estimating training effect sizes for Chunk 2201 of 12458
Scan 6: estimating training effect sizes for Chunk 2401 of 12458
Scan 6: estimating training effect sizes for Chunk 2601 of 12458
Scan 6: estimating training effect sizes for Chunk 2801 of 12458
Scan 6: estimating training effect sizes for Chunk 3001 of 12458
Scan 6: estimating training effect sizes for Chunk 3201 of 12458
Scan 6: estimating training effect sizes for Chunk 3401 of 12458
Scan 6: estimating training effect sizes for Chunk 3601 of 12458
Scan 6: estimating training effect sizes for Chunk 3801 of 12458
Scan 6: estimating training effect sizes for Chunk 4001 of 12458
Scan 6: estimating training effect sizes for Chunk 4201 of 12458
Scan 6: estimating training effect sizes for Chunk 4401 of 12458
Scan 6: estimating training effect sizes for Chunk 4601 of 12458
Scan 6: estimating training effect sizes for Chunk 4801 of 12458
Scan 6: estimating training effect sizes for Chunk 5001 of 12458
Scan 6: estimating training effect sizes for Chunk 5201 of 12458
Scan 6: estimating training effect sizes for Chunk 5401 of 12458
Scan 6: estimating training effect sizes for Chunk 5601 of 12458
Scan 6: estimating training effect sizes for Chunk 5801 of 12458
Scan 6: estimating training effect sizes for Chunk 6001 of 12458
Scan 6: estimating training effect sizes for Chunk 6201 of 12458
Scan 6: estimating training effect sizes for Chunk 6401 of 12458
Scan 6: estimating training effect sizes for Chunk 6601 of 12458
Scan 6: estimating training effect sizes for Chunk 6801 of 12458
Scan 6: estimating training effect sizes for Chunk 7001 of 12458
Scan 6: estimating training effect sizes for Chunk 7201 of 12458
Scan 6: estimating training effect sizes for Chunk 7401 of 12458
Scan 6: estimating training effect sizes for Chunk 7601 of 12458
Scan 6: estimating training effect sizes for Chunk 7801 of 12458
Scan 6: estimating training effect sizes for Chunk 8001 of 12458
Scan 6: estimating training effect sizes for Chunk 8201 of 12458
Scan 6: estimating training effect sizes for Chunk 8401 of 12458
Scan 6: estimating training effect sizes for Chunk 8601 of 12458
Scan 6: estimating training effect sizes for Chunk 8801 of 12458
Scan 6: estimating training effect sizes for Chunk 9001 of 12458
Scan 6: estimating training effect sizes for Chunk 9201 of 12458
Scan 6: estimating training effect sizes for Chunk 9401 of 12458
Scan 6: estimating training effect sizes for Chunk 9601 of 12458
Scan 6: estimating training effect sizes for Chunk 9801 of 12458
Scan 6: estimating training effect sizes for Chunk 10001 of 12458
Scan 6: estimating training effect sizes for Chunk 10201 of 12458
Scan 6: estimating training effect sizes for Chunk 10401 of 12458
Scan 6: estimating training effect sizes for Chunk 10601 of 12458
Scan 6: estimating training effect sizes for Chunk 10801 of 12458
Scan 6: estimating training effect sizes for Chunk 11001 of 12458
Scan 6: estimating training effect sizes for Chunk 11201 of 12458
Scan 6: estimating training effect sizes for Chunk 11401 of 12458
Scan 6: estimating training effect sizes for Chunk 11601 of 12458
Scan 6: estimating training effect sizes for Chunk 11801 of 12458
Scan 6: estimating training effect sizes for Chunk 12001 of 12458
Scan 6: estimating training effect sizes for Chunk 12201 of 12458
Scan 6: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.33

Scan 7: estimating training effect sizes for Chunk 1 of 12458
Scan 7: estimating training effect sizes for Chunk 201 of 12458
Scan 7: estimating training effect sizes for Chunk 401 of 12458
Scan 7: estimating training effect sizes for Chunk 601 of 12458
Scan 7: estimating training effect sizes for Chunk 801 of 12458
Scan 7: estimating training effect sizes for Chunk 1001 of 12458
Scan 7: estimating training effect sizes for Chunk 1201 of 12458
Scan 7: estimating training effect sizes for Chunk 1401 of 12458
Scan 7: estimating training effect sizes for Chunk 1601 of 12458
Scan 7: estimating training effect sizes for Chunk 1801 of 12458
Scan 7: estimating training effect sizes for Chunk 2001 of 12458
Scan 7: estimating training effect sizes for Chunk 2201 of 12458
Scan 7: estimating training effect sizes for Chunk 2401 of 12458
Scan 7: estimating training effect sizes for Chunk 2601 of 12458
Scan 7: estimating training effect sizes for Chunk 2801 of 12458
Scan 7: estimating training effect sizes for Chunk 3001 of 12458
Scan 7: estimating training effect sizes for Chunk 3201 of 12458
Scan 7: estimating training effect sizes for Chunk 3401 of 12458
Scan 7: estimating training effect sizes for Chunk 3601 of 12458
Scan 7: estimating training effect sizes for Chunk 3801 of 12458
Scan 7: estimating training effect sizes for Chunk 4001 of 12458
Scan 7: estimating training effect sizes for Chunk 4201 of 12458
Scan 7: estimating training effect sizes for Chunk 4401 of 12458
Scan 7: estimating training effect sizes for Chunk 4601 of 12458
Scan 7: estimating training effect sizes for Chunk 4801 of 12458
Scan 7: estimating training effect sizes for Chunk 5001 of 12458
Scan 7: estimating training effect sizes for Chunk 5201 of 12458
Scan 7: estimating training effect sizes for Chunk 5401 of 12458
Scan 7: estimating training effect sizes for Chunk 5601 of 12458
Scan 7: estimating training effect sizes for Chunk 5801 of 12458
Scan 7: estimating training effect sizes for Chunk 6001 of 12458
Scan 7: estimating training effect sizes for Chunk 6201 of 12458
Scan 7: estimating training effect sizes for Chunk 6401 of 12458
Scan 7: estimating training effect sizes for Chunk 6601 of 12458
Scan 7: estimating training effect sizes for Chunk 6801 of 12458
Scan 7: estimating training effect sizes for Chunk 7001 of 12458
Scan 7: estimating training effect sizes for Chunk 7201 of 12458
Scan 7: estimating training effect sizes for Chunk 7401 of 12458
Scan 7: estimating training effect sizes for Chunk 7601 of 12458
Scan 7: estimating training effect sizes for Chunk 7801 of 12458
Scan 7: estimating training effect sizes for Chunk 8001 of 12458
Scan 7: estimating training effect sizes for Chunk 8201 of 12458
Scan 7: estimating training effect sizes for Chunk 8401 of 12458
Scan 7: estimating training effect sizes for Chunk 8601 of 12458
Scan 7: estimating training effect sizes for Chunk 8801 of 12458
Scan 7: estimating training effect sizes for Chunk 9001 of 12458
Scan 7: estimating training effect sizes for Chunk 9201 of 12458
Scan 7: estimating training effect sizes for Chunk 9401 of 12458
Scan 7: estimating training effect sizes for Chunk 9601 of 12458
Scan 7: estimating training effect sizes for Chunk 9801 of 12458
Scan 7: estimating training effect sizes for Chunk 10001 of 12458
Scan 7: estimating training effect sizes for Chunk 10201 of 12458
Scan 7: estimating training effect sizes for Chunk 10401 of 12458
Scan 7: estimating training effect sizes for Chunk 10601 of 12458
Scan 7: estimating training effect sizes for Chunk 10801 of 12458
Scan 7: estimating training effect sizes for Chunk 11001 of 12458
Scan 7: estimating training effect sizes for Chunk 11201 of 12458
Scan 7: estimating training effect sizes for Chunk 11401 of 12458
Scan 7: estimating training effect sizes for Chunk 11601 of 12458
Scan 7: estimating training effect sizes for Chunk 11801 of 12458
Scan 7: estimating training effect sizes for Chunk 12001 of 12458
Scan 7: estimating training effect sizes for Chunk 12201 of 12458
Scan 7: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.25

Scan 8: estimating training effect sizes for Chunk 1 of 12458
Scan 8: estimating training effect sizes for Chunk 201 of 12458
Scan 8: estimating training effect sizes for Chunk 401 of 12458
Scan 8: estimating training effect sizes for Chunk 601 of 12458
Scan 8: estimating training effect sizes for Chunk 801 of 12458
Scan 8: estimating training effect sizes for Chunk 1001 of 12458
Scan 8: estimating training effect sizes for Chunk 1201 of 12458
Scan 8: estimating training effect sizes for Chunk 1401 of 12458
Scan 8: estimating training effect sizes for Chunk 1601 of 12458
Scan 8: estimating training effect sizes for Chunk 1801 of 12458
Scan 8: estimating training effect sizes for Chunk 2001 of 12458
Scan 8: estimating training effect sizes for Chunk 2201 of 12458
Scan 8: estimating training effect sizes for Chunk 2401 of 12458
Scan 8: estimating training effect sizes for Chunk 2601 of 12458
Scan 8: estimating training effect sizes for Chunk 2801 of 12458
Scan 8: estimating training effect sizes for Chunk 3001 of 12458
Scan 8: estimating training effect sizes for Chunk 3201 of 12458
Scan 8: estimating training effect sizes for Chunk 3401 of 12458
Scan 8: estimating training effect sizes for Chunk 3601 of 12458
Scan 8: estimating training effect sizes for Chunk 3801 of 12458
Scan 8: estimating training effect sizes for Chunk 4001 of 12458
Scan 8: estimating training effect sizes for Chunk 4201 of 12458
Scan 8: estimating training effect sizes for Chunk 4401 of 12458
Scan 8: estimating training effect sizes for Chunk 4601 of 12458
Scan 8: estimating training effect sizes for Chunk 4801 of 12458
Scan 8: estimating training effect sizes for Chunk 5001 of 12458
Scan 8: estimating training effect sizes for Chunk 5201 of 12458
Scan 8: estimating training effect sizes for Chunk 5401 of 12458
Scan 8: estimating training effect sizes for Chunk 5601 of 12458
Scan 8: estimating training effect sizes for Chunk 5801 of 12458
Scan 8: estimating training effect sizes for Chunk 6001 of 12458
Scan 8: estimating training effect sizes for Chunk 6201 of 12458
Scan 8: estimating training effect sizes for Chunk 6401 of 12458
Scan 8: estimating training effect sizes for Chunk 6601 of 12458
Scan 8: estimating training effect sizes for Chunk 6801 of 12458
Scan 8: estimating training effect sizes for Chunk 7001 of 12458
Scan 8: estimating training effect sizes for Chunk 7201 of 12458
Scan 8: estimating training effect sizes for Chunk 7401 of 12458
Scan 8: estimating training effect sizes for Chunk 7601 of 12458
Scan 8: estimating training effect sizes for Chunk 7801 of 12458
Scan 8: estimating training effect sizes for Chunk 8001 of 12458
Scan 8: estimating training effect sizes for Chunk 8201 of 12458
Scan 8: estimating training effect sizes for Chunk 8401 of 12458
Scan 8: estimating training effect sizes for Chunk 8601 of 12458
Scan 8: estimating training effect sizes for Chunk 8801 of 12458
Scan 8: estimating training effect sizes for Chunk 9001 of 12458
Scan 8: estimating training effect sizes for Chunk 9201 of 12458
Scan 8: estimating training effect sizes for Chunk 9401 of 12458
Scan 8: estimating training effect sizes for Chunk 9601 of 12458
Scan 8: estimating training effect sizes for Chunk 9801 of 12458
Scan 8: estimating training effect sizes for Chunk 10001 of 12458
Scan 8: estimating training effect sizes for Chunk 10201 of 12458
Scan 8: estimating training effect sizes for Chunk 10401 of 12458
Scan 8: estimating training effect sizes for Chunk 10601 of 12458
Scan 8: estimating training effect sizes for Chunk 10801 of 12458
Scan 8: estimating training effect sizes for Chunk 11001 of 12458
Scan 8: estimating training effect sizes for Chunk 11201 of 12458
Scan 8: estimating training effect sizes for Chunk 11401 of 12458
Scan 8: estimating training effect sizes for Chunk 11601 of 12458
Scan 8: estimating training effect sizes for Chunk 11801 of 12458
Scan 8: estimating training effect sizes for Chunk 12001 of 12458
Scan 8: estimating training effect sizes for Chunk 12201 of 12458
Scan 8: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.20

Scan 9: estimating training effect sizes for Chunk 1 of 12458
Scan 9: estimating training effect sizes for Chunk 201 of 12458
Scan 9: estimating training effect sizes for Chunk 401 of 12458
Scan 9: estimating training effect sizes for Chunk 601 of 12458
Scan 9: estimating training effect sizes for Chunk 801 of 12458
Scan 9: estimating training effect sizes for Chunk 1001 of 12458
Scan 9: estimating training effect sizes for Chunk 1201 of 12458
Scan 9: estimating training effect sizes for Chunk 1401 of 12458
Scan 9: estimating training effect sizes for Chunk 1601 of 12458
Scan 9: estimating training effect sizes for Chunk 1801 of 12458
Scan 9: estimating training effect sizes for Chunk 2001 of 12458
Scan 9: estimating training effect sizes for Chunk 2201 of 12458
Scan 9: estimating training effect sizes for Chunk 2401 of 12458
Scan 9: estimating training effect sizes for Chunk 2601 of 12458
Scan 9: estimating training effect sizes for Chunk 2801 of 12458
Scan 9: estimating training effect sizes for Chunk 3001 of 12458
Scan 9: estimating training effect sizes for Chunk 3201 of 12458
Scan 9: estimating training effect sizes for Chunk 3401 of 12458
Scan 9: estimating training effect sizes for Chunk 3601 of 12458
Scan 9: estimating training effect sizes for Chunk 3801 of 12458
Scan 9: estimating training effect sizes for Chunk 4001 of 12458
Scan 9: estimating training effect sizes for Chunk 4201 of 12458
Scan 9: estimating training effect sizes for Chunk 4401 of 12458
Scan 9: estimating training effect sizes for Chunk 4601 of 12458
Scan 9: estimating training effect sizes for Chunk 4801 of 12458
Scan 9: estimating training effect sizes for Chunk 5001 of 12458
Scan 9: estimating training effect sizes for Chunk 5201 of 12458
Scan 9: estimating training effect sizes for Chunk 5401 of 12458
Scan 9: estimating training effect sizes for Chunk 5601 of 12458
Scan 9: estimating training effect sizes for Chunk 5801 of 12458
Scan 9: estimating training effect sizes for Chunk 6001 of 12458
Scan 9: estimating training effect sizes for Chunk 6201 of 12458
Scan 9: estimating training effect sizes for Chunk 6401 of 12458
Scan 9: estimating training effect sizes for Chunk 6601 of 12458
Scan 9: estimating training effect sizes for Chunk 6801 of 12458
Scan 9: estimating training effect sizes for Chunk 7001 of 12458
Scan 9: estimating training effect sizes for Chunk 7201 of 12458
Scan 9: estimating training effect sizes for Chunk 7401 of 12458
Scan 9: estimating training effect sizes for Chunk 7601 of 12458
Scan 9: estimating training effect sizes for Chunk 7801 of 12458
Scan 9: estimating training effect sizes for Chunk 8001 of 12458
Scan 9: estimating training effect sizes for Chunk 8201 of 12458
Scan 9: estimating training effect sizes for Chunk 8401 of 12458
Scan 9: estimating training effect sizes for Chunk 8601 of 12458
Scan 9: estimating training effect sizes for Chunk 8801 of 12458
Scan 9: estimating training effect sizes for Chunk 9001 of 12458
Scan 9: estimating training effect sizes for Chunk 9201 of 12458
Scan 9: estimating training effect sizes for Chunk 9401 of 12458
Scan 9: estimating training effect sizes for Chunk 9601 of 12458
Scan 9: estimating training effect sizes for Chunk 9801 of 12458
Scan 9: estimating training effect sizes for Chunk 10001 of 12458
Scan 9: estimating training effect sizes for Chunk 10201 of 12458
Scan 9: estimating training effect sizes for Chunk 10401 of 12458
Scan 9: estimating training effect sizes for Chunk 10601 of 12458
Scan 9: estimating training effect sizes for Chunk 10801 of 12458
Scan 9: estimating training effect sizes for Chunk 11001 of 12458
Scan 9: estimating training effect sizes for Chunk 11201 of 12458
Scan 9: estimating training effect sizes for Chunk 11401 of 12458
Scan 9: estimating training effect sizes for Chunk 11601 of 12458
Scan 9: estimating training effect sizes for Chunk 11801 of 12458
Scan 9: estimating training effect sizes for Chunk 12001 of 12458
Scan 9: estimating training effect sizes for Chunk 12201 of 12458
Scan 9: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.18

The revised estimate of heritability is 0.0100

Measuring accuracy of each model
Model 1: heritability 0.0100, p 0.0000, f2 1.0000, mean squared error 1.0047
Model 2: heritability 0.0100, p 0.5000, f2 0.5000, mean squared error 1.0047
Model 3: heritability 0.0100, p 0.5000, f2 0.3000, mean squared error 1.0084
Model 4: heritability 0.0100, p 0.5000, f2 0.1000, mean squared error 1.0088
Model 5: heritability 0.0100, p 0.1000, f2 0.5000, mean squared error 1.0067
Model 6: heritability 0.0100, p 0.1000, f2 0.3000, mean squared error 1.0084
Model 7: heritability 0.0100, p 0.1000, f2 0.1000, mean squared error 1.0116
Model 8: heritability 0.0100, p 0.0100, f2 0.5000, mean squared error 1.0050
Model 9: heritability 0.0100, p 0.0100, f2 0.3000, mean squared error 1.0053
Model 10: heritability 0.0100, p 0.0100, f2 0.1000, mean squared error 1.0056

Time check: have so far spent 0.27 hours

Constructing final PRS (heritability 0.0100, p 0.5000, f2 0.5000) using all samples

Scan 1: estimating final effect sizes for Chunk 1 of 12458
Scan 1: estimating final effect sizes for Chunk 201 of 12458
Scan 1: estimating final effect sizes for Chunk 401 of 12458
Scan 1: estimating final effect sizes for Chunk 601 of 12458
Scan 1: estimating final effect sizes for Chunk 801 of 12458
Scan 1: estimating final effect sizes for Chunk 1001 of 12458
Scan 1: estimating final effect sizes for Chunk 1201 of 12458
Scan 1: estimating final effect sizes for Chunk 1401 of 12458
Scan 1: estimating final effect sizes for Chunk 1601 of 12458
Scan 1: estimating final effect sizes for Chunk 1801 of 12458
Scan 1: estimating final effect sizes for Chunk 2001 of 12458
Scan 1: estimating final effect sizes for Chunk 2201 of 12458
Scan 1: estimating final effect sizes for Chunk 2401 of 12458
Scan 1: estimating final effect sizes for Chunk 2601 of 12458
Scan 1: estimating final effect sizes for Chunk 2801 of 12458
Scan 1: estimating final effect sizes for Chunk 3001 of 12458
Scan 1: estimating final effect sizes for Chunk 3201 of 12458
Scan 1: estimating final effect sizes for Chunk 3401 of 12458
Scan 1: estimating final effect sizes for Chunk 3601 of 12458
Scan 1: estimating final effect sizes for Chunk 3801 of 12458
Scan 1: estimating final effect sizes for Chunk 4001 of 12458
Scan 1: estimating final effect sizes for Chunk 4201 of 12458
Scan 1: estimating final effect sizes for Chunk 4401 of 12458
Scan 1: estimating final effect sizes for Chunk 4601 of 12458
Scan 1: estimating final effect sizes for Chunk 4801 of 12458
Scan 1: estimating final effect sizes for Chunk 5001 of 12458
Scan 1: estimating final effect sizes for Chunk 5201 of 12458
Scan 1: estimating final effect sizes for Chunk 5401 of 12458
Scan 1: estimating final effect sizes for Chunk 5601 of 12458
Scan 1: estimating final effect sizes for Chunk 5801 of 12458
Scan 1: estimating final effect sizes for Chunk 6001 of 12458
Scan 1: estimating final effect sizes for Chunk 6201 of 12458
Scan 1: estimating final effect sizes for Chunk 6401 of 12458
Scan 1: estimating final effect sizes for Chunk 6601 of 12458
Scan 1: estimating final effect sizes for Chunk 6801 of 12458
Scan 1: estimating final effect sizes for Chunk 7001 of 12458
Scan 1: estimating final effect sizes for Chunk 7201 of 12458
Scan 1: estimating final effect sizes for Chunk 7401 of 12458
Scan 1: estimating final effect sizes for Chunk 7601 of 12458
Scan 1: estimating final effect sizes for Chunk 7801 of 12458
Scan 1: estimating final effect sizes for Chunk 8001 of 12458
Scan 1: estimating final effect sizes for Chunk 8201 of 12458
Scan 1: estimating final effect sizes for Chunk 8401 of 12458
Scan 1: estimating final effect sizes for Chunk 8601 of 12458
Scan 1: estimating final effect sizes for Chunk 8801 of 12458
Scan 1: estimating final effect sizes for Chunk 9001 of 12458
Scan 1: estimating final effect sizes for Chunk 9201 of 12458
Scan 1: estimating final effect sizes for Chunk 9401 of 12458
Scan 1: estimating final effect sizes for Chunk 9601 of 12458
Scan 1: estimating final effect sizes for Chunk 9801 of 12458
Scan 1: estimating final effect sizes for Chunk 10001 of 12458
Scan 1: estimating final effect sizes for Chunk 10201 of 12458
Scan 1: estimating final effect sizes for Chunk 10401 of 12458
Scan 1: estimating final effect sizes for Chunk 10601 of 12458
Scan 1: estimating final effect sizes for Chunk 10801 of 12458
Scan 1: estimating final effect sizes for Chunk 11001 of 12458
Scan 1: estimating final effect sizes for Chunk 11201 of 12458
Scan 1: estimating final effect sizes for Chunk 11401 of 12458
Scan 1: estimating final effect sizes for Chunk 11601 of 12458
Scan 1: estimating final effect sizes for Chunk 11801 of 12458
Scan 1: estimating final effect sizes for Chunk 12001 of 12458
Scan 1: estimating final effect sizes for Chunk 12201 of 12458
Scan 1: estimating final effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.00

Scan 2: estimating final effect sizes for Chunk 1 of 12458
Scan 2: estimating final effect sizes for Chunk 201 of 12458
Scan 2: estimating final effect sizes for Chunk 401 of 12458
Scan 2: estimating final effect sizes for Chunk 601 of 12458
Scan 2: estimating final effect sizes for Chunk 801 of 12458
Scan 2: estimating final effect sizes for Chunk 1001 of 12458
Scan 2: estimating final effect sizes for Chunk 1201 of 12458
Scan 2: estimating final effect sizes for Chunk 1401 of 12458
Scan 2: estimating final effect sizes for Chunk 1601 of 12458
Scan 2: estimating final effect sizes for Chunk 1801 of 12458
Scan 2: estimating final effect sizes for Chunk 2001 of 12458
Scan 2: estimating final effect sizes for Chunk 2201 of 12458
Scan 2: estimating final effect sizes for Chunk 2401 of 12458
Scan 2: estimating final effect sizes for Chunk 2601 of 12458
Scan 2: estimating final effect sizes for Chunk 2801 of 12458
Scan 2: estimating final effect sizes for Chunk 3001 of 12458
Scan 2: estimating final effect sizes for Chunk 3201 of 12458
Scan 2: estimating final effect sizes for Chunk 3401 of 12458
Scan 2: estimating final effect sizes for Chunk 3601 of 12458
Scan 2: estimating final effect sizes for Chunk 3801 of 12458
Scan 2: estimating final effect sizes for Chunk 4001 of 12458
Scan 2: estimating final effect sizes for Chunk 4201 of 12458
Scan 2: estimating final effect sizes for Chunk 4401 of 12458
Scan 2: estimating final effect sizes for Chunk 4601 of 12458
Scan 2: estimating final effect sizes for Chunk 4801 of 12458
Scan 2: estimating final effect sizes for Chunk 5001 of 12458
Scan 2: estimating final effect sizes for Chunk 5201 of 12458
Scan 2: estimating final effect sizes for Chunk 5401 of 12458
Scan 2: estimating final effect sizes for Chunk 5601 of 12458
Scan 2: estimating final effect sizes for Chunk 5801 of 12458
Scan 2: estimating final effect sizes for Chunk 6001 of 12458
Scan 2: estimating final effect sizes for Chunk 6201 of 12458
Scan 2: estimating final effect sizes for Chunk 6401 of 12458
Scan 2: estimating final effect sizes for Chunk 6601 of 12458
Scan 2: estimating final effect sizes for Chunk 6801 of 12458
Scan 2: estimating final effect sizes for Chunk 7001 of 12458
Scan 2: estimating final effect sizes for Chunk 7201 of 12458
Scan 2: estimating final effect sizes for Chunk 7401 of 12458
Scan 2: estimating final effect sizes for Chunk 7601 of 12458
Scan 2: estimating final effect sizes for Chunk 7801 of 12458
Scan 2: estimating final effect sizes for Chunk 8001 of 12458
Scan 2: estimating final effect sizes for Chunk 8201 of 12458
Scan 2: estimating final effect sizes for Chunk 8401 of 12458
Scan 2: estimating final effect sizes for Chunk 8601 of 12458
Scan 2: estimating final effect sizes for Chunk 8801 of 12458
Scan 2: estimating final effect sizes for Chunk 9001 of 12458
Scan 2: estimating final effect sizes for Chunk 9201 of 12458
Scan 2: estimating final effect sizes for Chunk 9401 of 12458
Scan 2: estimating final effect sizes for Chunk 9601 of 12458
Scan 2: estimating final effect sizes for Chunk 9801 of 12458
Scan 2: estimating final effect sizes for Chunk 10001 of 12458
Scan 2: estimating final effect sizes for Chunk 10201 of 12458
Scan 2: estimating final effect sizes for Chunk 10401 of 12458
Scan 2: estimating final effect sizes for Chunk 10601 of 12458
Scan 2: estimating final effect sizes for Chunk 10801 of 12458
Scan 2: estimating final effect sizes for Chunk 11001 of 12458
Scan 2: estimating final effect sizes for Chunk 11201 of 12458
Scan 2: estimating final effect sizes for Chunk 11401 of 12458
Scan 2: estimating final effect sizes for Chunk 11601 of 12458
Scan 2: estimating final effect sizes for Chunk 11801 of 12458
Scan 2: estimating final effect sizes for Chunk 12001 of 12458
Scan 2: estimating final effect sizes for Chunk 12201 of 12458
Scan 2: estimating final effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 1.00

Scan 3: estimating final effect sizes for Chunk 1 of 4
Average number of iterations per chunk: 1.00

Best-fitting model saved in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic.effects, with posterior probabilities in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic.probs

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 08:45:05 2025 and ended at Sun May 25 09:01:49 2025
The elapsed time was 0.28 hours
Given the command used one thread, this means the CPU time was also 0.28 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic.effects
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for one profile

Please note that ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic.effects is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 3189219 predictors from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic.effects

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 3200
Calculating scores for Chunk 51 of 3200
Calculating scores for Chunk 101 of 3200
Calculating scores for Chunk 151 of 3200
Calculating scores for Chunk 201 of 3200
Calculating scores for Chunk 251 of 3200
Calculating scores for Chunk 301 of 3200
Calculating scores for Chunk 351 of 3200
Calculating scores for Chunk 401 of 3200
Calculating scores for Chunk 451 of 3200
Calculating scores for Chunk 501 of 3200
Calculating scores for Chunk 551 of 3200
Calculating scores for Chunk 601 of 3200
Calculating scores for Chunk 651 of 3200
Calculating scores for Chunk 701 of 3200
Calculating scores for Chunk 751 of 3200
Calculating scores for Chunk 801 of 3200
Calculating scores for Chunk 851 of 3200
Calculating scores for Chunk 901 of 3200
Calculating scores for Chunk 951 of 3200
Calculating scores for Chunk 1001 of 3200
Calculating scores for Chunk 1051 of 3200
Calculating scores for Chunk 1101 of 3200
Calculating scores for Chunk 1151 of 3200
Calculating scores for Chunk 1201 of 3200
Calculating scores for Chunk 1251 of 3200
Calculating scores for Chunk 1301 of 3200
Calculating scores for Chunk 1351 of 3200
Calculating scores for Chunk 1401 of 3200
Calculating scores for Chunk 1451 of 3200
Calculating scores for Chunk 1501 of 3200
Calculating scores for Chunk 1551 of 3200
Calculating scores for Chunk 1601 of 3200
Calculating scores for Chunk 1651 of 3200
Calculating scores for Chunk 1701 of 3200
Calculating scores for Chunk 1751 of 3200
Calculating scores for Chunk 1801 of 3200
Calculating scores for Chunk 1851 of 3200
Calculating scores for Chunk 1901 of 3200
Calculating scores for Chunk 1951 of 3200
Calculating scores for Chunk 2001 of 3200
Calculating scores for Chunk 2051 of 3200
Calculating scores for Chunk 2101 of 3200
Calculating scores for Chunk 2151 of 3200
Calculating scores for Chunk 2201 of 3200
Calculating scores for Chunk 2251 of 3200
Calculating scores for Chunk 2301 of 3200
Calculating scores for Chunk 2351 of 3200
Calculating scores for Chunk 2401 of 3200
Calculating scores for Chunk 2451 of 3200
Calculating scores for Chunk 2501 of 3200
Calculating scores for Chunk 2551 of 3200
Calculating scores for Chunk 2601 of 3200
Calculating scores for Chunk 2651 of 3200
Calculating scores for Chunk 2701 of 3200
Calculating scores for Chunk 2751 of 3200
Calculating scores for Chunk 2801 of 3200
Calculating scores for Chunk 2851 of 3200
Calculating scores for Chunk 2901 of 3200
Calculating scores for Chunk 2951 of 3200
Calculating scores for Chunk 3001 of 3200
Calculating scores for Chunk 3051 of 3200
Calculating scores for Chunk 3101 of 3200
Calculating scores for Chunk 3151 of 3200

Profile saved in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic_prs_calc_with_pheno.profile

Correlation between score and phenotype is 0.8586, saved in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:01:49 2025 and ended at Sun May 25 09:02:21 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic.effects
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for one profile

Please note that ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic.effects is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 3189219 predictors from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic.effects

Calculating scores for Chunk 1 of 3200
Calculating scores for Chunk 51 of 3200
Calculating scores for Chunk 101 of 3200
Calculating scores for Chunk 151 of 3200
Calculating scores for Chunk 201 of 3200
Calculating scores for Chunk 251 of 3200
Calculating scores for Chunk 301 of 3200
Calculating scores for Chunk 351 of 3200
Calculating scores for Chunk 401 of 3200
Calculating scores for Chunk 451 of 3200
Calculating scores for Chunk 501 of 3200
Calculating scores for Chunk 551 of 3200
Calculating scores for Chunk 601 of 3200
Calculating scores for Chunk 651 of 3200
Calculating scores for Chunk 701 of 3200
Calculating scores for Chunk 751 of 3200
Calculating scores for Chunk 801 of 3200
Calculating scores for Chunk 851 of 3200
Calculating scores for Chunk 901 of 3200
Calculating scores for Chunk 951 of 3200
Calculating scores for Chunk 1001 of 3200
Calculating scores for Chunk 1051 of 3200
Calculating scores for Chunk 1101 of 3200
Calculating scores for Chunk 1151 of 3200
Calculating scores for Chunk 1201 of 3200
Calculating scores for Chunk 1251 of 3200
Calculating scores for Chunk 1301 of 3200
Calculating scores for Chunk 1351 of 3200
Calculating scores for Chunk 1401 of 3200
Calculating scores for Chunk 1451 of 3200
Calculating scores for Chunk 1501 of 3200
Calculating scores for Chunk 1551 of 3200
Calculating scores for Chunk 1601 of 3200
Calculating scores for Chunk 1651 of 3200
Calculating scores for Chunk 1701 of 3200
Calculating scores for Chunk 1751 of 3200
Calculating scores for Chunk 1801 of 3200
Calculating scores for Chunk 1851 of 3200
Calculating scores for Chunk 1901 of 3200
Calculating scores for Chunk 1951 of 3200
Calculating scores for Chunk 2001 of 3200
Calculating scores for Chunk 2051 of 3200
Calculating scores for Chunk 2101 of 3200
Calculating scores for Chunk 2151 of 3200
Calculating scores for Chunk 2201 of 3200
Calculating scores for Chunk 2251 of 3200
Calculating scores for Chunk 2301 of 3200
Calculating scores for Chunk 2351 of 3200
Calculating scores for Chunk 2401 of 3200
Calculating scores for Chunk 2451 of 3200
Calculating scores for Chunk 2501 of 3200
Calculating scores for Chunk 2551 of 3200
Calculating scores for Chunk 2601 of 3200
Calculating scores for Chunk 2651 of 3200
Calculating scores for Chunk 2701 of 3200
Calculating scores for Chunk 2751 of 3200
Calculating scores for Chunk 2801 of 3200
Calculating scores for Chunk 2851 of 3200
Calculating scores for Chunk 2901 of 3200
Calculating scores for Chunk 2951 of 3200
Calculating scores for Chunk 3001 of 3200
Calculating scores for Chunk 3051 of 3200
Calculating scores for Chunk 3101 of 3200
Calculating scores for Chunk 3151 of 3200

Profile saved in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:02:21 2025 and ended at Sun May 25 09:02:53 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## check the PRS is the same ##


###### calculate a PRS using the classical approach ######

## calculate linear association between SNPs and the phenotype ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--linear ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 4 covariates for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.combined")

Performing linear regression for Chunk 1 of 3200
Performing linear regression for Chunk 11 of 3200
Performing linear regression for Chunk 21 of 3200
Performing linear regression for Chunk 31 of 3200
Performing linear regression for Chunk 41 of 3200
Performing linear regression for Chunk 51 of 3200
Performing linear regression for Chunk 61 of 3200
Performing linear regression for Chunk 71 of 3200
Performing linear regression for Chunk 81 of 3200
Performing linear regression for Chunk 91 of 3200
Performing linear regression for Chunk 101 of 3200
Performing linear regression for Chunk 111 of 3200
Performing linear regression for Chunk 121 of 3200
Performing linear regression for Chunk 131 of 3200
Performing linear regression for Chunk 141 of 3200
Performing linear regression for Chunk 151 of 3200
Performing linear regression for Chunk 161 of 3200
Performing linear regression for Chunk 171 of 3200
Performing linear regression for Chunk 181 of 3200
Performing linear regression for Chunk 191 of 3200
Performing linear regression for Chunk 201 of 3200
Performing linear regression for Chunk 211 of 3200
Performing linear regression for Chunk 221 of 3200
Performing linear regression for Chunk 231 of 3200
Performing linear regression for Chunk 241 of 3200
Performing linear regression for Chunk 251 of 3200
Performing linear regression for Chunk 261 of 3200
Performing linear regression for Chunk 271 of 3200
Performing linear regression for Chunk 281 of 3200
Performing linear regression for Chunk 291 of 3200
Performing linear regression for Chunk 301 of 3200
Performing linear regression for Chunk 311 of 3200
Performing linear regression for Chunk 321 of 3200
Performing linear regression for Chunk 331 of 3200
Performing linear regression for Chunk 341 of 3200
Performing linear regression for Chunk 351 of 3200
Performing linear regression for Chunk 361 of 3200
Performing linear regression for Chunk 371 of 3200
Performing linear regression for Chunk 381 of 3200
Performing linear regression for Chunk 391 of 3200
Performing linear regression for Chunk 401 of 3200
Performing linear regression for Chunk 411 of 3200
Performing linear regression for Chunk 421 of 3200
Performing linear regression for Chunk 431 of 3200
Performing linear regression for Chunk 441 of 3200
Performing linear regression for Chunk 451 of 3200
Performing linear regression for Chunk 461 of 3200
Performing linear regression for Chunk 471 of 3200
Performing linear regression for Chunk 481 of 3200
Performing linear regression for Chunk 491 of 3200
Performing linear regression for Chunk 501 of 3200
Performing linear regression for Chunk 511 of 3200
Performing linear regression for Chunk 521 of 3200
Performing linear regression for Chunk 531 of 3200
Performing linear regression for Chunk 541 of 3200
Performing linear regression for Chunk 551 of 3200
Performing linear regression for Chunk 561 of 3200
Performing linear regression for Chunk 571 of 3200
Performing linear regression for Chunk 581 of 3200
Performing linear regression for Chunk 591 of 3200
Performing linear regression for Chunk 601 of 3200
Performing linear regression for Chunk 611 of 3200
Performing linear regression for Chunk 621 of 3200
Performing linear regression for Chunk 631 of 3200
Performing linear regression for Chunk 641 of 3200
Performing linear regression for Chunk 651 of 3200
Performing linear regression for Chunk 661 of 3200
Performing linear regression for Chunk 671 of 3200
Performing linear regression for Chunk 681 of 3200
Performing linear regression for Chunk 691 of 3200
Performing linear regression for Chunk 701 of 3200
Performing linear regression for Chunk 711 of 3200
Performing linear regression for Chunk 721 of 3200
Performing linear regression for Chunk 731 of 3200
Performing linear regression for Chunk 741 of 3200
Performing linear regression for Chunk 751 of 3200
Performing linear regression for Chunk 761 of 3200
Performing linear regression for Chunk 771 of 3200
Performing linear regression for Chunk 781 of 3200
Performing linear regression for Chunk 791 of 3200
Performing linear regression for Chunk 801 of 3200
Performing linear regression for Chunk 811 of 3200
Performing linear regression for Chunk 821 of 3200
Performing linear regression for Chunk 831 of 3200
Performing linear regression for Chunk 841 of 3200
Performing linear regression for Chunk 851 of 3200
Performing linear regression for Chunk 861 of 3200
Performing linear regression for Chunk 871 of 3200
Performing linear regression for Chunk 881 of 3200
Performing linear regression for Chunk 891 of 3200
Performing linear regression for Chunk 901 of 3200
Performing linear regression for Chunk 911 of 3200
Performing linear regression for Chunk 921 of 3200
Performing linear regression for Chunk 931 of 3200
Performing linear regression for Chunk 941 of 3200
Performing linear regression for Chunk 951 of 3200
Performing linear regression for Chunk 961 of 3200
Performing linear regression for Chunk 971 of 3200
Performing linear regression for Chunk 981 of 3200
Performing linear regression for Chunk 991 of 3200
Performing linear regression for Chunk 1001 of 3200
Performing linear regression for Chunk 1011 of 3200
Performing linear regression for Chunk 1021 of 3200
Performing linear regression for Chunk 1031 of 3200
Performing linear regression for Chunk 1041 of 3200
Performing linear regression for Chunk 1051 of 3200
Performing linear regression for Chunk 1061 of 3200
Performing linear regression for Chunk 1071 of 3200
Performing linear regression for Chunk 1081 of 3200
Performing linear regression for Chunk 1091 of 3200
Performing linear regression for Chunk 1101 of 3200
Performing linear regression for Chunk 1111 of 3200
Performing linear regression for Chunk 1121 of 3200
Performing linear regression for Chunk 1131 of 3200
Performing linear regression for Chunk 1141 of 3200
Performing linear regression for Chunk 1151 of 3200
Performing linear regression for Chunk 1161 of 3200
Performing linear regression for Chunk 1171 of 3200
Performing linear regression for Chunk 1181 of 3200
Performing linear regression for Chunk 1191 of 3200
Performing linear regression for Chunk 1201 of 3200
Performing linear regression for Chunk 1211 of 3200
Performing linear regression for Chunk 1221 of 3200
Performing linear regression for Chunk 1231 of 3200
Performing linear regression for Chunk 1241 of 3200
Performing linear regression for Chunk 1251 of 3200
Performing linear regression for Chunk 1261 of 3200
Performing linear regression for Chunk 1271 of 3200
Performing linear regression for Chunk 1281 of 3200
Performing linear regression for Chunk 1291 of 3200
Performing linear regression for Chunk 1301 of 3200
Performing linear regression for Chunk 1311 of 3200
Performing linear regression for Chunk 1321 of 3200
Performing linear regression for Chunk 1331 of 3200
Performing linear regression for Chunk 1341 of 3200
Performing linear regression for Chunk 1351 of 3200
Performing linear regression for Chunk 1361 of 3200
Performing linear regression for Chunk 1371 of 3200
Performing linear regression for Chunk 1381 of 3200
Performing linear regression for Chunk 1391 of 3200
Performing linear regression for Chunk 1401 of 3200
Performing linear regression for Chunk 1411 of 3200
Performing linear regression for Chunk 1421 of 3200
Performing linear regression for Chunk 1431 of 3200
Performing linear regression for Chunk 1441 of 3200
Performing linear regression for Chunk 1451 of 3200
Performing linear regression for Chunk 1461 of 3200
Performing linear regression for Chunk 1471 of 3200
Performing linear regression for Chunk 1481 of 3200
Performing linear regression for Chunk 1491 of 3200
Performing linear regression for Chunk 1501 of 3200
Performing linear regression for Chunk 1511 of 3200
Performing linear regression for Chunk 1521 of 3200
Performing linear regression for Chunk 1531 of 3200
Performing linear regression for Chunk 1541 of 3200
Performing linear regression for Chunk 1551 of 3200
Performing linear regression for Chunk 1561 of 3200
Performing linear regression for Chunk 1571 of 3200
Performing linear regression for Chunk 1581 of 3200
Performing linear regression for Chunk 1591 of 3200
Performing linear regression for Chunk 1601 of 3200
Performing linear regression for Chunk 1611 of 3200
Performing linear regression for Chunk 1621 of 3200
Performing linear regression for Chunk 1631 of 3200
Performing linear regression for Chunk 1641 of 3200
Performing linear regression for Chunk 1651 of 3200
Performing linear regression for Chunk 1661 of 3200
Performing linear regression for Chunk 1671 of 3200
Performing linear regression for Chunk 1681 of 3200
Performing linear regression for Chunk 1691 of 3200
Performing linear regression for Chunk 1701 of 3200
Performing linear regression for Chunk 1711 of 3200
Performing linear regression for Chunk 1721 of 3200
Performing linear regression for Chunk 1731 of 3200
Performing linear regression for Chunk 1741 of 3200
Performing linear regression for Chunk 1751 of 3200
Performing linear regression for Chunk 1761 of 3200
Performing linear regression for Chunk 1771 of 3200
Performing linear regression for Chunk 1781 of 3200
Performing linear regression for Chunk 1791 of 3200
Performing linear regression for Chunk 1801 of 3200
Performing linear regression for Chunk 1811 of 3200
Performing linear regression for Chunk 1821 of 3200
Performing linear regression for Chunk 1831 of 3200
Performing linear regression for Chunk 1841 of 3200
Performing linear regression for Chunk 1851 of 3200
Performing linear regression for Chunk 1861 of 3200
Performing linear regression for Chunk 1871 of 3200
Performing linear regression for Chunk 1881 of 3200
Performing linear regression for Chunk 1891 of 3200
Performing linear regression for Chunk 1901 of 3200
Performing linear regression for Chunk 1911 of 3200
Performing linear regression for Chunk 1921 of 3200
Performing linear regression for Chunk 1931 of 3200
Performing linear regression for Chunk 1941 of 3200
Performing linear regression for Chunk 1951 of 3200
Performing linear regression for Chunk 1961 of 3200
Performing linear regression for Chunk 1971 of 3200
Performing linear regression for Chunk 1981 of 3200
Performing linear regression for Chunk 1991 of 3200
Performing linear regression for Chunk 2001 of 3200
Performing linear regression for Chunk 2011 of 3200
Performing linear regression for Chunk 2021 of 3200
Performing linear regression for Chunk 2031 of 3200
Performing linear regression for Chunk 2041 of 3200
Performing linear regression for Chunk 2051 of 3200
Performing linear regression for Chunk 2061 of 3200
Performing linear regression for Chunk 2071 of 3200
Performing linear regression for Chunk 2081 of 3200
Performing linear regression for Chunk 2091 of 3200
Performing linear regression for Chunk 2101 of 3200
Performing linear regression for Chunk 2111 of 3200
Performing linear regression for Chunk 2121 of 3200
Performing linear regression for Chunk 2131 of 3200
Performing linear regression for Chunk 2141 of 3200
Performing linear regression for Chunk 2151 of 3200
Performing linear regression for Chunk 2161 of 3200
Performing linear regression for Chunk 2171 of 3200
Performing linear regression for Chunk 2181 of 3200
Performing linear regression for Chunk 2191 of 3200
Performing linear regression for Chunk 2201 of 3200
Performing linear regression for Chunk 2211 of 3200
Performing linear regression for Chunk 2221 of 3200
Performing linear regression for Chunk 2231 of 3200
Performing linear regression for Chunk 2241 of 3200
Performing linear regression for Chunk 2251 of 3200
Performing linear regression for Chunk 2261 of 3200
Performing linear regression for Chunk 2271 of 3200
Performing linear regression for Chunk 2281 of 3200
Performing linear regression for Chunk 2291 of 3200
Performing linear regression for Chunk 2301 of 3200
Performing linear regression for Chunk 2311 of 3200
Performing linear regression for Chunk 2321 of 3200
Performing linear regression for Chunk 2331 of 3200
Performing linear regression for Chunk 2341 of 3200
Performing linear regression for Chunk 2351 of 3200
Performing linear regression for Chunk 2361 of 3200
Performing linear regression for Chunk 2371 of 3200
Performing linear regression for Chunk 2381 of 3200
Performing linear regression for Chunk 2391 of 3200
Performing linear regression for Chunk 2401 of 3200
Performing linear regression for Chunk 2411 of 3200
Performing linear regression for Chunk 2421 of 3200
Performing linear regression for Chunk 2431 of 3200
Performing linear regression for Chunk 2441 of 3200
Performing linear regression for Chunk 2451 of 3200
Performing linear regression for Chunk 2461 of 3200
Performing linear regression for Chunk 2471 of 3200
Performing linear regression for Chunk 2481 of 3200
Performing linear regression for Chunk 2491 of 3200
Performing linear regression for Chunk 2501 of 3200
Performing linear regression for Chunk 2511 of 3200
Performing linear regression for Chunk 2521 of 3200
Performing linear regression for Chunk 2531 of 3200
Performing linear regression for Chunk 2541 of 3200
Performing linear regression for Chunk 2551 of 3200
Performing linear regression for Chunk 2561 of 3200
Performing linear regression for Chunk 2571 of 3200
Performing linear regression for Chunk 2581 of 3200
Performing linear regression for Chunk 2591 of 3200
Performing linear regression for Chunk 2601 of 3200
Performing linear regression for Chunk 2611 of 3200
Performing linear regression for Chunk 2621 of 3200
Performing linear regression for Chunk 2631 of 3200
Performing linear regression for Chunk 2641 of 3200
Performing linear regression for Chunk 2651 of 3200
Performing linear regression for Chunk 2661 of 3200
Performing linear regression for Chunk 2671 of 3200
Performing linear regression for Chunk 2681 of 3200
Performing linear regression for Chunk 2691 of 3200
Performing linear regression for Chunk 2701 of 3200
Performing linear regression for Chunk 2711 of 3200
Performing linear regression for Chunk 2721 of 3200
Performing linear regression for Chunk 2731 of 3200
Performing linear regression for Chunk 2741 of 3200
Performing linear regression for Chunk 2751 of 3200
Performing linear regression for Chunk 2761 of 3200
Performing linear regression for Chunk 2771 of 3200
Performing linear regression for Chunk 2781 of 3200
Performing linear regression for Chunk 2791 of 3200
Performing linear regression for Chunk 2801 of 3200
Performing linear regression for Chunk 2811 of 3200
Performing linear regression for Chunk 2821 of 3200
Performing linear regression for Chunk 2831 of 3200
Performing linear regression for Chunk 2841 of 3200
Performing linear regression for Chunk 2851 of 3200
Performing linear regression for Chunk 2861 of 3200
Performing linear regression for Chunk 2871 of 3200
Performing linear regression for Chunk 2881 of 3200
Performing linear regression for Chunk 2891 of 3200
Performing linear regression for Chunk 2901 of 3200
Performing linear regression for Chunk 2911 of 3200
Performing linear regression for Chunk 2921 of 3200
Performing linear regression for Chunk 2931 of 3200
Performing linear regression for Chunk 2941 of 3200
Performing linear regression for Chunk 2951 of 3200
Performing linear regression for Chunk 2961 of 3200
Performing linear regression for Chunk 2971 of 3200
Performing linear regression for Chunk 2981 of 3200
Performing linear regression for Chunk 2991 of 3200
Performing linear regression for Chunk 3001 of 3200
Performing linear regression for Chunk 3011 of 3200
Performing linear regression for Chunk 3021 of 3200
Performing linear regression for Chunk 3031 of 3200
Performing linear regression for Chunk 3041 of 3200
Performing linear regression for Chunk 3051 of 3200
Performing linear regression for Chunk 3061 of 3200
Performing linear regression for Chunk 3071 of 3200
Performing linear regression for Chunk 3081 of 3200
Performing linear regression for Chunk 3091 of 3200
Performing linear regression for Chunk 3101 of 3200
Performing linear regression for Chunk 3111 of 3200
Performing linear regression for Chunk 3121 of 3200
Performing linear regression for Chunk 3131 of 3200
Performing linear regression for Chunk 3141 of 3200
Performing linear regression for Chunk 3151 of 3200
Performing linear regression for Chunk 3161 of 3200
Performing linear regression for Chunk 3171 of 3200
Performing linear regression for Chunk 3181 of 3200
Performing linear regression for Chunk 3191 of 3200

Main results saved in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.assoc, with a summary version in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.summaries, p-values in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues and score file in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:02:53 2025 and ended at Sun May 25 09:03:26 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## define p-values cut-offs for thresholding ##

# load the p-values #

# get the minimum p-value #

# make a list of thresholds that are above the minimum p-value #

## obtain scores in a reduced set of SNPs after thresholding and cumpling ##

# perform thresholding considering the threshold 1 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_small_set_predictors_set_linear_clump_thresholding_1_predictors
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 1

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e+00, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

3189219 of the 3189219 predictors in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e+00
All of these are in the data

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading p-values from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr1_858952_G_A 8.0753e-01 | chr1_905373_T_C 7.1965e-01 | chr1_911428_C_T 2.4518e-01

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 25 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 127569; stay tuned for updates ;)
Pass 1: Thinning for Chunk 2001 of 127569; kept 5359 out of 50000 predictors (10.72%)
Pass 1: Thinning for Chunk 4001 of 127569; kept 10173 out of 100000 predictors (10.17%)
Pass 1: Thinning for Chunk 6001 of 127569; kept 15077 out of 150000 predictors (10.05%)
Pass 1: Thinning for Chunk 8001 of 127569; kept 19860 out of 200000 predictors (9.93%)
Pass 1: Thinning for Chunk 10001 of 127569; kept 24427 out of 250000 predictors (9.77%)
Pass 1: Thinning for Chunk 12001 of 127569; kept 28937 out of 300000 predictors (9.65%)
Pass 1: Thinning for Chunk 14001 of 127569; kept 33527 out of 350000 predictors (9.58%)
Pass 1: Thinning for Chunk 16001 of 127569; kept 38143 out of 400000 predictors (9.54%)
Pass 1: Thinning for Chunk 18001 of 127569; kept 42966 out of 450000 predictors (9.55%)
Pass 1: Thinning for Chunk 20001 of 127569; kept 48384 out of 500000 predictors (9.68%)
Pass 1: Thinning for Chunk 22001 of 127569; kept 52886 out of 550000 predictors (9.62%)
Pass 1: Thinning for Chunk 24001 of 127569; kept 57645 out of 600000 predictors (9.61%)
Pass 1: Thinning for Chunk 26001 of 127569; kept 62134 out of 650000 predictors (9.56%)
Pass 1: Thinning for Chunk 28001 of 127569; kept 66899 out of 700000 predictors (9.56%)
Pass 1: Thinning for Chunk 30001 of 127569; kept 71637 out of 750000 predictors (9.55%)
Pass 1: Thinning for Chunk 32001 of 127569; kept 76255 out of 800000 predictors (9.53%)
Pass 1: Thinning for Chunk 34001 of 127569; kept 80439 out of 850000 predictors (9.46%)
Pass 1: Thinning for Chunk 36001 of 127569; kept 84774 out of 900000 predictors (9.42%)
Pass 1: Thinning for Chunk 38001 of 127569; kept 89254 out of 950000 predictors (9.40%)
Pass 1: Thinning for Chunk 40001 of 127569; kept 93592 out of 1000000 predictors (9.36%)
Pass 1: Thinning for Chunk 42001 of 127569; kept 98240 out of 1050000 predictors (9.36%)
Pass 1: Thinning for Chunk 44001 of 127569; kept 103203 out of 1100000 predictors (9.38%)
Pass 1: Thinning for Chunk 46001 of 127569; kept 107568 out of 1150000 predictors (9.35%)
Pass 1: Thinning for Chunk 48001 of 127569; kept 112613 out of 1200000 predictors (9.38%)
Pass 1: Thinning for Chunk 50001 of 127569; kept 117105 out of 1250000 predictors (9.37%)
Pass 1: Thinning for Chunk 52001 of 127569; kept 120907 out of 1300000 predictors (9.30%)
Pass 1: Thinning for Chunk 54001 of 127569; kept 125222 out of 1350000 predictors (9.28%)
Pass 1: Thinning for Chunk 56001 of 127569; kept 130162 out of 1400000 predictors (9.30%)
Pass 1: Thinning for Chunk 58001 of 127569; kept 134420 out of 1450000 predictors (9.27%)
Pass 1: Thinning for Chunk 60001 of 127569; kept 138616 out of 1500000 predictors (9.24%)
Pass 1: Thinning for Chunk 62001 of 127569; kept 142658 out of 1550000 predictors (9.20%)
Pass 1: Thinning for Chunk 64001 of 127569; kept 147596 out of 1600000 predictors (9.22%)
Pass 1: Thinning for Chunk 66001 of 127569; kept 152089 out of 1650000 predictors (9.22%)
Pass 1: Thinning for Chunk 68001 of 127569; kept 155878 out of 1700000 predictors (9.17%)
Pass 1: Thinning for Chunk 70001 of 127569; kept 160409 out of 1750000 predictors (9.17%)
Pass 1: Thinning for Chunk 72001 of 127569; kept 165106 out of 1800000 predictors (9.17%)
Pass 1: Thinning for Chunk 74001 of 127569; kept 169516 out of 1850000 predictors (9.16%)
Pass 1: Thinning for Chunk 76001 of 127569; kept 174326 out of 1900000 predictors (9.18%)
Pass 1: Thinning for Chunk 78001 of 127569; kept 179230 out of 1950000 predictors (9.19%)
Pass 1: Thinning for Chunk 80001 of 127569; kept 183893 out of 2000000 predictors (9.19%)
Pass 1: Thinning for Chunk 82001 of 127569; kept 187992 out of 2050000 predictors (9.17%)
Pass 1: Thinning for Chunk 84001 of 127569; kept 192783 out of 2100000 predictors (9.18%)
Pass 1: Thinning for Chunk 86001 of 127569; kept 197175 out of 2150000 predictors (9.17%)
Pass 1: Thinning for Chunk 88001 of 127569; kept 201648 out of 2200000 predictors (9.17%)
Pass 1: Thinning for Chunk 90001 of 127569; kept 206392 out of 2250000 predictors (9.17%)
Pass 1: Thinning for Chunk 92001 of 127569; kept 211061 out of 2300000 predictors (9.18%)
Pass 1: Thinning for Chunk 94001 of 127569; kept 215449 out of 2350000 predictors (9.17%)
Pass 1: Thinning for Chunk 96001 of 127569; kept 220081 out of 2400000 predictors (9.17%)
Pass 1: Thinning for Chunk 98001 of 127569; kept 225180 out of 2450000 predictors (9.19%)
Pass 1: Thinning for Chunk 100001 of 127569; kept 230041 out of 2500000 predictors (9.20%)
Pass 1: Thinning for Chunk 102001 of 127569; kept 234415 out of 2550000 predictors (9.19%)
Pass 1: Thinning for Chunk 104001 of 127569; kept 239186 out of 2600000 predictors (9.20%)
Pass 1: Thinning for Chunk 106001 of 127569; kept 243557 out of 2650000 predictors (9.19%)
Pass 1: Thinning for Chunk 108001 of 127569; kept 248616 out of 2700000 predictors (9.21%)
Pass 1: Thinning for Chunk 110001 of 127569; kept 253882 out of 2750000 predictors (9.23%)
Pass 1: Thinning for Chunk 112001 of 127569; kept 258680 out of 2800000 predictors (9.24%)
Pass 1: Thinning for Chunk 114001 of 127569; kept 263493 out of 2850000 predictors (9.25%)
Pass 1: Thinning for Chunk 116001 of 127569; kept 268943 out of 2900000 predictors (9.27%)
Pass 1: Thinning for Chunk 118001 of 127569; kept 274559 out of 2950000 predictors (9.31%)
Pass 1: Thinning for Chunk 120001 of 127569; kept 279290 out of 3000000 predictors (9.31%)
Pass 1: Thinning for Chunk 122001 of 127569; kept 284746 out of 3050000 predictors (9.34%)
Pass 1: Thinning for Chunk 124001 of 127569; kept 289886 out of 3100000 predictors (9.35%)
Pass 1: Thinning for Chunk 126001 of 127569; kept 294931 out of 3150000 predictors (9.36%)

The bit-size has now been set to 124

Pass 2: Thinning for Chunk 1 of 2413; stay tuned for updates ;)
Pass 2: Thinning for Chunk 401 of 2413; kept 15490 out of 49600 predictors (31.23%)
Pass 2: Thinning for Chunk 801 of 2413; kept 31602 out of 99200 predictors (31.86%)
Pass 2: Thinning for Chunk 1201 of 2413; kept 46595 out of 148800 predictors (31.31%)
Pass 2: Thinning for Chunk 1601 of 2413; kept 64255 out of 198400 predictors (32.39%)
Pass 2: Thinning for Chunk 2001 of 2413; kept 80550 out of 248000 predictors (32.48%)
Pass 2: Thinning for Chunk 2401 of 2413; kept 101788 out of 297600 predictors (34.20%)

Thinning complete: 102483 predictors kept (saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_small_set_predictors_set_linear_clump_thresholding_1_predictors.in), 3086736 lost (./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_small_set_predictors_set_linear_clump_thresholding_1_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_small_set_predictors_set_linear_clump_thresholding_1_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:03:28 2025 and ended at Sun May 25 09:05:06 2025
The elapsed time was 0.03 hours
Given the command used one thread, this means the CPU time was also 0.03 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_small_set_predictors_set_linear_clump_thresholding_1_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 102483 predictors to extract from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_small_set_predictors_set_linear_clump_thresholding_1_predictors.in

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 102483)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 4 covariates for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1.combined")

Performing linear regression for Chunk 1 of 115
Performing linear regression for Chunk 11 of 115
Performing linear regression for Chunk 21 of 115
Performing linear regression for Chunk 31 of 115
Performing linear regression for Chunk 41 of 115
Performing linear regression for Chunk 51 of 115
Performing linear regression for Chunk 61 of 115
Performing linear regression for Chunk 71 of 115
Performing linear regression for Chunk 81 of 115
Performing linear regression for Chunk 91 of 115
Performing linear regression for Chunk 101 of 115
Performing linear regression for Chunk 111 of 115

Main results saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1.assoc, with a summary version in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1.summaries, p-values in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1.pvalues and score file in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:05:07 2025 and ended at Sun May 25 09:05:14 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1.score
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 102483 predictors from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1.score

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 115
Calculating scores for Chunk 51 of 115
Calculating scores for Chunk 101 of 115

Profiles saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.2437 to 0.8703, saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:05:14 2025 and ended at Sun May 25 09:05:23 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1.score
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 102483 predictors from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1.score

Calculating scores for Chunk 1 of 115
Calculating scores for Chunk 51 of 115
Calculating scores for Chunk 101 of 115

Profiles saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:05:23 2025 and ended at Sun May 25 09:05:31 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


# perform thresholding considering the threshold 0.1 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 0.1

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e-01, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

328514 of the 3189219 predictors in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e-01
All of these are in the data

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 328514)

Reading p-values from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr1_962184_T_C 3.5768e-02 | chr1_984039_T_C 3.0091e-02 | chr1_1094994_G_A 3.9481e-02

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 20 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 16426; stay tuned for updates ;)
Pass 1: Thinning for Chunk 2501 of 16426; kept 8229 out of 50000 predictors (16.46%)
Pass 1: Thinning for Chunk 5001 of 16426; kept 15951 out of 100000 predictors (15.95%)
Pass 1: Thinning for Chunk 7501 of 16426; kept 24234 out of 150000 predictors (16.16%)
Pass 1: Thinning for Chunk 10001 of 16426; kept 32303 out of 200000 predictors (16.15%)
Pass 1: Thinning for Chunk 12501 of 16426; kept 40049 out of 250000 predictors (16.02%)
Pass 1: Thinning for Chunk 15001 of 16426; kept 48605 out of 300000 predictors (16.20%)

The bit-size has now been set to 25

Pass 2: Thinning for Chunk 1 of 2144; stay tuned for updates ;)
Pass 2: Thinning for Chunk 2001 of 2144; kept 22790 out of 50000 predictors (45.58%)

Thinning complete: 24835 predictors kept (saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors.in), 303679 lost (./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:05:31 2025 and ended at Sun May 25 09:06:16 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 24835 predictors to extract from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors.in

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 24835)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 4 covariates for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1.combined")

Performing linear regression for Chunk 1 of 35
Performing linear regression for Chunk 11 of 35
Performing linear regression for Chunk 21 of 35
Performing linear regression for Chunk 31 of 35

Main results saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1.assoc, with a summary version in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1.summaries, p-values in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1.pvalues and score file in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:06:16 2025 and ended at Sun May 25 09:06:23 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1.score
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 24835 predictors from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1.score

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 35

Profiles saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.2437 to 0.8694, saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:06:23 2025 and ended at Sun May 25 09:06:30 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1.score
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 24835 predictors from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1.score

Calculating scores for Chunk 1 of 35

Profiles saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:06:30 2025 and ended at Sun May 25 09:06:37 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


# perform thresholding considering the threshold 0.01 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 0.01

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e-02, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

32836 of the 3189219 predictors in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e-02
All of these are in the data

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 32836)

Reading p-values from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr1_3972702_C_T 5.6554e-03 | chr1_4061687_C_A 7.3383e-05 | chr1_4163015_G_A 7.5650e-03

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 20 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 1642; stay tuned for updates ;)

The bit-size has now been set to 20

Pass 2: Thinning for Chunk 1 of 333; stay tuned for updates ;)

Thinning complete: 3742 predictors kept (saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors.in), 29094 lost (./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:06:37 2025 and ended at Sun May 25 09:07:16 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 3742 predictors to extract from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors.in

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3742)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 4 covariates for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01.combined")

Performing linear regression for Chunk 1 of 22
Performing linear regression for Chunk 11 of 22
Performing linear regression for Chunk 21 of 22

Main results saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01.assoc, with a summary version in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01.summaries, p-values in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01.pvalues and score file in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:07:16 2025 and ended at Sun May 25 09:07:23 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01.score
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 3742 predictors from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01.score

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 22

Profiles saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.2437 to 0.8618, saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:07:23 2025 and ended at Sun May 25 09:07:30 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01.score
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 3742 predictors from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01.score

Calculating scores for Chunk 1 of 22

Profiles saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:07:30 2025 and ended at Sun May 25 09:07:37 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


# perform thresholding considering the threshold 0.001 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 0.001

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e-03, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

3195 of the 3189219 predictors in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e-03
All of these are in the data

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3195)

Reading p-values from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr1_4061687_C_A 7.3383e-05 | chr1_7770202_T_C 3.9856e-04 | chr1_7776738_A_G 4.9591e-04

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 20 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 160; stay tuned for updates ;)

The bit-size has now been set to 20

Pass 2: Thinning for Chunk 1 of 41; stay tuned for updates ;)

Thinning complete: 499 predictors kept (saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors.in), 2696 lost (./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:07:37 2025 and ended at Sun May 25 09:08:15 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 499 predictors to extract from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors.in

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 499)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 4 covariates for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001.combined")

Performing linear regression for Chunk 1 of 22
Performing linear regression for Chunk 11 of 22
Performing linear regression for Chunk 21 of 22

Main results saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001.assoc, with a summary version in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001.summaries, p-values in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001.pvalues and score file in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:08:15 2025 and ended at Sun May 25 09:08:22 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001.score
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 499 predictors from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001.score

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 22

Profiles saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.2437 to 0.8139, saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:08:22 2025 and ended at Sun May 25 09:08:29 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001.score
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 499 predictors from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001.score

Calculating scores for Chunk 1 of 22

Profiles saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:08:29 2025 and ended at Sun May 25 09:08:36 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


# perform thresholding considering the threshold 0.0001 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 0.0001

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e-04, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

221 of the 3189219 predictors in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e-04
All of these are in the data

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 221)

Reading p-values from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr1_4061687_C_A 7.3383e-05 | chr1_68384728_T_A 2.7984e-05 | chr1_68384919_G_C 2.7984e-05

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 20 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 12; stay tuned for updates ;)

The bit-size has now been set to 20

Pass 2: Thinning for Chunk 1 of 4; stay tuned for updates ;)

Thinning complete: 44 predictors kept (saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors.in), 177 lost (./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:08:36 2025 and ended at Sun May 25 09:09:14 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 44 predictors to extract from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors.in

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 44)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 4 covariates for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001.combined")

Performing linear regression for Chunk 1 of 17
Performing linear regression for Chunk 11 of 17

Main results saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001.assoc, with a summary version in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001.summaries, p-values in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001.pvalues and score file in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:09:14 2025 and ended at Sun May 25 09:09:21 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001.score
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 44 predictors from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001.score

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 17

Profiles saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.2437 to 0.5632, saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:09:21 2025 and ended at Sun May 25 09:09:28 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001.score
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 44 predictors from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001.score

Calculating scores for Chunk 1 of 17

Profiles saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:09:28 2025 and ended at Sun May 25 09:09:34 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


# perform thresholding considering the threshold 1e-05 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 1e-05

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e-05, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

28 of the 3189219 predictors in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e-05
All of these are in the data

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 28)

Reading p-values from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr3_187336176_T_C 5.1538e-06 | chr3_187336418_T_C 5.0764e-06 | chr3_187336496_T_C 7.7543e-06

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 20 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 2; stay tuned for updates ;)

The bit-size has now been set to 20

Pass 2: Thinning for Chunk 1 of 1; stay tuned for updates ;)

Thinning complete: 4 predictors kept (saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors.in), 24 lost (./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:09:34 2025 and ended at Sun May 25 09:10:13 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 4 predictors to extract from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors.in

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 4)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 4 covariates for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05.combined")

Performing linear regression for Chunk 1 of 4

Main results saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05.assoc, with a summary version in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05.summaries, p-values in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05.pvalues and score file in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:10:13 2025 and ended at Sun May 25 09:10:19 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05.score
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 4 predictors from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05.score

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 4

Profiles saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.2437 to 0.2437, saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:10:19 2025 and ended at Sun May 25 09:10:26 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05.score
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 4 predictors from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05.score

Calculating scores for Chunk 1 of 4

Profiles saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:10:26 2025 and ended at Sun May 25 09:10:33 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


###### check we have used the correct samples in both analyses ######

## load the FAM files used for training and test ##

## samples in files generated by elastic net ##

## samples in files generated by linear ##

###### plot quantiles of PRS against phenotype ######

## prepare folders ##


## load the phenotype data before transformation ##

## process it ##

# get only the samples finally included in modelling #

# split the ID into FID and IID #

# check that the new variables has been correctly create #

## open the plot ##

# Create a figure with 8 subplots arranged in a grid (e.g., 2 rows x 4 columns) #

## Flatten the axes array for easier iteration ##

## create a list of models ##

## iterate across models ##

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          5            -0.9450            0.020             0.99500
1          3            -1.0075            0.020             0.99500
2          2            -1.7000           -0.010             0.97750
3          1            -2.9900           -1.000             0.03750
4          9            -0.0450            0.960             1.02000
5          4            -1.0075            0.010             1.00500
6         15             0.9400            1.020             1.97750
7         16             0.2725            1.040             2.01500
8         11             0.0225            1.000             1.96750
9         12             0.0325            1.000             1.98500
10        19             1.0050            2.000             2.95750
11        13             0.2250            1.010             1.98500
12        17             0.9600            1.060             2.01500
13         7            -0.7330            0.085             1.03775
14         6            -0.7425            0.030             0.98000
15        20             1.0150            2.920             4.76750
16        10             0.0200            0.990             1.96000
17         8            -0.9375            0.920             1.75000
18        14             0.0300            1.010             1.98775
19        18             0.9950            1.990             2.96750

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          6           -0.96750            0.040             1.00750
1          3           -1.00000            0.010             0.98000
2          1           -2.99000           -0.990             0.03750
3          9            0.00000            0.970             1.98750
4          4           -0.95000            0.020             0.98000
5         13            0.90750            1.010             1.96750
6         14            0.30675            1.010             1.99000
7         17            0.98250            1.060             2.00000
8         11            0.02250            1.000             1.96750
9          5           -0.94750            0.020             0.99500
10         8           -0.72000            0.100             1.03000
11        20            1.95250            2.920             4.76750
12        10            0.00250            0.980             1.04250
13        16            0.96000            1.040             1.99750
14        19            0.98500            2.000             2.96750
15        12            0.00750            1.000             1.96750
16         2           -1.95000           -0.010             0.97000
17         7           -0.97650            0.075             1.03325
18        15            0.94500            1.030             2.05000
19        18            1.02000            1.980             2.97500

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          6           -0.93000            0.050              1.0075
1          3           -1.00000            0.020              0.9750
2          1           -2.99000           -0.990              0.0375
3         10            0.00000            0.970              1.9575
4          5           -0.97250            0.020              0.9950
5         13            0.31500            1.000              1.9675
6         14            0.95000            1.015              1.9855
7          9            0.01250            0.970              1.7525
8         12            0.02250            1.010              1.9700
9         17            0.96500            1.110              2.0200
10        18            1.02000            1.970              2.7325
11        11            0.02250            1.000              1.9600
12         8           -0.69750            0.930              1.0275
13        19            0.98500            2.000              2.9675
14         7           -0.98775            0.055              1.0255
15         4           -0.96750            0.030              0.9800
16        16            0.96250            1.040              2.0150
17        20            1.25250            2.900              4.7675
18         2           -1.95000           -0.030              0.9700
19        15            0.94750            1.020              2.0425

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          7            -0.9455            0.055             1.01775
1          2            -1.9750           -0.010             0.96000
2          4            -0.9975            0.020             0.99500
3          1            -2.9900           -1.000             0.04000
4         11             0.0025            1.000             1.96000
5          6            -0.9850            0.070             1.00750
6         14             0.9445            1.005             1.98325
7         10             0.0000            0.980             1.97750
8         12             0.0300            1.000             1.95750
9         17             0.9650            1.060             2.00000
10        18             0.9950            1.980             2.95750
11         5            -0.0300            0.020             0.98000
12        20             1.2525            2.920             4.76750
13         9            -0.0125            0.940             1.72750
14         3            -1.0175            0.000             0.98000
15        16             0.9700            1.030             2.01750
16        19             1.0025            2.000             2.97500
17        13             0.0375            1.010             1.97750
18        15             0.9525            1.020             2.03000
19         8            -0.7050            0.930             1.02750

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          9            -0.0100            0.970              1.9575
1          4            -1.0150            0.010              0.9950
2          2            -1.7250           -0.010              0.9975
3         11            -0.0150            0.990              1.9700
4          3            -1.0325            0.000              1.0025
5         12             0.0300            1.000              1.9675
6         16             0.9700            1.060              2.9400
7          6            -1.0000            0.030              1.7175
8         14             0.0445            1.010              1.9700
9         18             0.9825            1.930              2.9750
10        10             0.0100            0.970              1.9600
11        19             1.0000            2.000              3.6850
12         1            -2.9900           -0.990              0.9525
13         7            -0.7330            0.915              1.7310
14         8            -0.9275            0.960              1.9500
15        17             0.2350            1.920              2.0475
16        13             0.0325            1.020              1.9975
17        20             1.0300            2.900              4.7675
18        15             0.0150            1.020              1.9875
19         5            -0.6800            0.050              0.9975

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          4            -1.0000             0.07              2.7275
1          3            -1.7775             0.04              1.0550
2          6            -1.7125             0.05              1.9700
3         10            -0.9475             0.99              2.0200
4         13            -0.0075             0.99              2.9550
5          7            -1.0100             0.95              2.0155
6         14            -0.0200             1.00              2.0155
7         17             0.0350             1.06              2.7100
8         18             0.9325             1.06              3.7225
9         19            -0.0075             1.03              2.9225
10         1            -2.9900             0.01              1.0000
11         8            -1.6950             0.94              1.9550
12        11            -0.7225             1.00              1.9950
13        20            -0.0600             2.00              4.7675
14         2            -1.9725             0.05              1.9600
15         5            -1.7000             0.04              1.9500
16        16             0.0000             1.02              2.9475
17        12             0.0000             0.99              2.6800
18         9            -0.0475             0.97              1.9775
19        15             0.0125             1.01              2.9975

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          7           -1.01700            1.010             3.97000
1          3           -1.00000            0.975             2.03325
2          1           -1.92000            0.960             2.02000
3          8           -1.21800            1.000             2.93300
4          5           -1.00000            0.970             2.05950
5          2           -1.95000            0.970             2.04900
6          4           -0.98800            0.960             2.02800
7          6            0.01650            1.000             1.99675
8          9            0.00000            1.050             3.84700
9         11           -0.81000            1.030             2.05600
10        12           -0.79375            1.030             3.78025
11        10           -0.02775            1.010             3.73500

# plot the results #

# finish the plot #

###### manhattan plots ######

## load assoc results to pandas ##
         Chromosome           Predictor  Basepair  ... CallRate MachR2  SPA_Status
0                 1     chr1_858952_G_A    858952  ...      1.0    NaN    NOT_USED
1                 1     chr1_905373_T_C    905373  ...      1.0    NaN    NOT_USED
2                 1     chr1_911428_C_T    911428  ...      1.0    NaN    NOT_USED
3                 1     chr1_918870_A_G    918870  ...      1.0    NaN    NOT_USED
4                 1     chr1_931513_T_C    931513  ...      1.0    NaN    NOT_USED
...             ...                 ...       ...  ...      ...    ...         ...
3189214          22  chr22_50749145_C_T  50749145  ...      1.0    NaN    NOT_USED
3189215          22  chr22_50749470_T_C  50749470  ...      1.0    NaN    NOT_USED
3189216          22  chr22_50749890_T_G  50749890  ...      1.0    NaN    NOT_USED
3189217          22  chr22_50751123_G_T  50751123  ...      1.0    NaN    NOT_USED
3189218          22  chr22_50752652_C_G  50752652  ...      1.0    NaN    NOT_USED

[3189219 rows x 13 columns]

## check we have the correct columns ##

## check we have the correct dtypes ##

## calculate -log_10(pvalue) ##

## convert chromosome to category and then sort by it ##

## sort rows by chromosome and basepair position ##

## Creates a new column called ind in the assoc_results DataFrame ##

## Groups the assoc_results DataFrame by the Chromosome column ##

## make manhattan plot ##

# open the plot #

# Add a title to the plot #

# Define a colorblind-friendly palette #

# iterate across chromosomes #

# set the x-axis ticks and labels of these ticks #

# add p-value thresholds as horizontal lines #

# set axis limits #

# set the axis label #

# Increase font size for tick labels #

# add a legend to the subplot #

# save the plot as a static image #

###### compress the results ######

## remove bed and bim plink files initialles used as input (compressed files already created) ##


## elastic outputs ##


## linear outputs ##

# first raw outputs before clumping #


# then outputs after clumping #







#######################################
#######################################
FINISH
#######################################
#######################################
