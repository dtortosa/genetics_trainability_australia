
#######################################
#######################################
checking function to print nicely: header 1
#######################################
#######################################

###### checking function to print nicely: header 2 ######

## checking function to print nicely: header 3 ##

# checking function to print nicely: header 4 #

#######################################
#######################################
check behaviour run_bash
#######################################
#######################################

###### see working directory ######
/home/dftortosa/diego_docs/science/other_projects/australian_army_bishop/heavy_analyses/australian_army_bishop/association_studies


###### list files/folders there ######
03a_association_analyses.sif
03a_phenotype_prep.out
03aa_final_pca_calc.out
03b_prs_calculation_100_iter_large_set_predictors_beep_change.out
03b_prs_calculation_100_iter_large_set_predictors_distance_change.out
03b_prs_calculation_100_iter_large_set_predictors_vo2_change.out
03b_prs_calculation_100_iter_large_set_predictors_weight_change.out
03b_prs_calculation_100_iter_small_set_predictors_beep_change.out
03b_prs_calculation_100_iter_small_set_predictors_distance_change.out
03b_prs_calculation_100_iter_small_set_predictors_vo2_change.out
03b_prs_calculation_100_iter_small_set_predictors_weight_change.out
03c_prepare_slurm_files.out
03d_processing_results.out
03e_manhattan_plots_beep_change.out
03e_manhattan_plots_distance_change.out
03e_manhattan_plots_vo2_change.out
03e_manhattan_plots_weight_change.out
03f_bat_smt_analyses_bat.out
03f_bat_smt_analyses_metabolic.out
03f_bat_smt_analyses_smt.out
data
literature
results
scripts


#######################################
#######################################
For phenotype beep_change, and the small dataset of covariates
#######################################
#######################################

###### initial preparations ######

## create folders for results ##


## load the phenotype data ##
                 family_id AGRF code  ...  Week 1 Beep test  beep_change
0     combat_ILGSA24-17303  0200ADMM  ...          0.269686    -0.269686
1     combat_ILGSA24-17303  0200ASJM  ...          1.447926    -2.463212
2     combat_ILGSA24-17303  0200BHNM  ...         -1.713535    -0.885320
3     combat_ILGSA24-17303  0200CBOM  ...         -0.922440    -0.757386
4     combat_ILGSA24-17303  0200CDFM  ...         -0.143587    -0.885320
...                    ...       ...  ...               ...          ...
1013  combat_ILGSA24-17873  8098ATDN  ...          0.405596     0.416334
1014  combat_ILGSA24-17873  8098RSJN  ...         -0.223901    -1.025520
1015  combat_ILGSA24-17873  8098TSAN  ...          0.814507    -0.593047
1016  combat_ILGSA24-17873  8099AJNN  ...          0.269686     0.336863
1017  combat_ILGSA24-17873  8099COSN  ...          0.405596     0.079031

[1018 rows x 8 columns]

## specify the covariates ##
Index(['PCA5', 'PCA8', 'sex_code', 'Week 1 Body Mass', 'Week 1 Beep test'], dtype='object')

## decompress bim and bed files with all sample generated after NA cleaning ##


###### prepare LDAK inputs ######

###### calculate elastic PRS ######

## run the PRS with --elastic ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--elastic ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--LOCO NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Constructing elastic net PRS

Will consider five values for the predictor scaling (alpha = -1, -0.75, -0.5, -0.25 and 0); to instead specify the value, use "--power" (or use "--powerfile" to provide a range of values)

Will use the default prior parameter choices (saved in the file ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic.parameters); to instead specify your own, use "--parameters"

Will select the best prior parameters via cross-validation, using 0.10 randomly-picked test samples (use "--cv-proportion" to change this proportion, "--cv-samples" to explicitly specify the test samples, or "--cv-skip" to turn off cross-validation)

Will always include the LOCO polygenic contribution, regardless of their estimated accuracy

When constructing PRS, will scan the data at most 10 times (change this using "--num-scans")

All heritability estimates must be between 0.01 and 0.8000 (change the upper bound using "--max-her")

Will use either three or ten random vectors for Monte Carlo operations (decided based on the number of samples); change this number using "--num-random-vectors"

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 4 covariates for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic.combined")

When performing cross-validation, will use 917 samples to train models and 101 to test their accuracy

Warning, to process the data requires 6.1 Gb; sorry this can not be reduced

Warning, to perform the analysis requires approximately 2.1 Gb; sorry, this can not be reduced

Estimating per-predictor heritabilities using Randomized Haseman-Elston Regression with 10 random vectors

Will divide the predictors into 20 partitions (change this using "--num-divides")
Will exclude chunks containing a predictor with estimated variance explained greater than 0.0982 (change this using "--max-cor")

Calculating traces for Chunk 1 of 12458
Calculating traces for Chunk 201 of 12458
Calculating traces for Chunk 401 of 12458
Calculating traces for Chunk 601 of 12458
Calculating traces for Chunk 801 of 12458
Calculating traces for Chunk 1001 of 12458
Calculating traces for Chunk 1201 of 12458
Calculating traces for Chunk 1401 of 12458
Calculating traces for Chunk 1601 of 12458
Calculating traces for Chunk 1801 of 12458
Calculating traces for Chunk 2001 of 12458
Calculating traces for Chunk 2201 of 12458
Calculating traces for Chunk 2401 of 12458
Calculating traces for Chunk 2601 of 12458
Calculating traces for Chunk 2801 of 12458
Calculating traces for Chunk 3001 of 12458
Calculating traces for Chunk 3201 of 12458
Calculating traces for Chunk 3401 of 12458
Calculating traces for Chunk 3601 of 12458
Calculating traces for Chunk 3801 of 12458
Calculating traces for Chunk 4001 of 12458
Calculating traces for Chunk 4201 of 12458
Calculating traces for Chunk 4401 of 12458
Calculating traces for Chunk 4601 of 12458
Calculating traces for Chunk 4801 of 12458
Calculating traces for Chunk 5001 of 12458
Calculating traces for Chunk 5201 of 12458
Calculating traces for Chunk 5401 of 12458
Calculating traces for Chunk 5601 of 12458
Calculating traces for Chunk 5801 of 12458
Calculating traces for Chunk 6001 of 12458
Calculating traces for Chunk 6201 of 12458
Calculating traces for Chunk 6401 of 12458
Calculating traces for Chunk 6601 of 12458
Calculating traces for Chunk 6801 of 12458
Calculating traces for Chunk 7001 of 12458
Calculating traces for Chunk 7201 of 12458
Calculating traces for Chunk 7401 of 12458
Calculating traces for Chunk 7601 of 12458
Calculating traces for Chunk 7801 of 12458
Calculating traces for Chunk 8001 of 12458
Calculating traces for Chunk 8201 of 12458
Calculating traces for Chunk 8401 of 12458
Calculating traces for Chunk 8601 of 12458
Calculating traces for Chunk 8801 of 12458
Calculating traces for Chunk 9001 of 12458
Calculating traces for Chunk 9201 of 12458
Calculating traces for Chunk 9401 of 12458
Calculating traces for Chunk 9601 of 12458
Calculating traces for Chunk 9801 of 12458
Calculating traces for Chunk 10001 of 12458
Calculating traces for Chunk 10201 of 12458
Calculating traces for Chunk 10401 of 12458
Calculating traces for Chunk 10601 of 12458
Calculating traces for Chunk 10801 of 12458
Calculating traces for Chunk 11001 of 12458
Calculating traces for Chunk 11201 of 12458
Calculating traces for Chunk 11401 of 12458
Calculating traces for Chunk 11601 of 12458
Calculating traces for Chunk 11801 of 12458
Calculating traces for Chunk 12001 of 12458
Calculating traces for Chunk 12201 of 12458
Calculating traces for Chunk 12401 of 12458

Best power is -1.0000 and estimated heritability is 0.5314

Time check: have so far spent 0.02 hours

Constructing 10 PRS using training samples
Will also make 33 MCMC REML models (using all samples)

Scan 1: estimating training effect sizes for Chunk 1 of 12458
Scan 1: estimating training effect sizes for Chunk 201 of 12458
Scan 1: estimating training effect sizes for Chunk 401 of 12458
Scan 1: estimating training effect sizes for Chunk 601 of 12458
Scan 1: estimating training effect sizes for Chunk 801 of 12458
Scan 1: estimating training effect sizes for Chunk 1001 of 12458
Scan 1: estimating training effect sizes for Chunk 1201 of 12458
Scan 1: estimating training effect sizes for Chunk 1401 of 12458
Scan 1: estimating training effect sizes for Chunk 1601 of 12458
Scan 1: estimating training effect sizes for Chunk 1801 of 12458
Scan 1: estimating training effect sizes for Chunk 2001 of 12458
Scan 1: estimating training effect sizes for Chunk 2201 of 12458
Scan 1: estimating training effect sizes for Chunk 2401 of 12458
Scan 1: estimating training effect sizes for Chunk 2601 of 12458
Scan 1: estimating training effect sizes for Chunk 2801 of 12458
Scan 1: estimating training effect sizes for Chunk 3001 of 12458
Scan 1: estimating training effect sizes for Chunk 3201 of 12458
Scan 1: estimating training effect sizes for Chunk 3401 of 12458
Scan 1: estimating training effect sizes for Chunk 3601 of 12458
Scan 1: estimating training effect sizes for Chunk 3801 of 12458
Scan 1: estimating training effect sizes for Chunk 4001 of 12458
Scan 1: estimating training effect sizes for Chunk 4201 of 12458
Scan 1: estimating training effect sizes for Chunk 4401 of 12458
Scan 1: estimating training effect sizes for Chunk 4601 of 12458
Scan 1: estimating training effect sizes for Chunk 4801 of 12458
Scan 1: estimating training effect sizes for Chunk 5001 of 12458
Scan 1: estimating training effect sizes for Chunk 5201 of 12458
Scan 1: estimating training effect sizes for Chunk 5401 of 12458
Scan 1: estimating training effect sizes for Chunk 5601 of 12458
Scan 1: estimating training effect sizes for Chunk 5801 of 12458
Scan 1: estimating training effect sizes for Chunk 6001 of 12458
Scan 1: estimating training effect sizes for Chunk 6201 of 12458
Scan 1: estimating training effect sizes for Chunk 6401 of 12458
Scan 1: estimating training effect sizes for Chunk 6601 of 12458
Scan 1: estimating training effect sizes for Chunk 6801 of 12458
Scan 1: estimating training effect sizes for Chunk 7001 of 12458
Scan 1: estimating training effect sizes for Chunk 7201 of 12458
Scan 1: estimating training effect sizes for Chunk 7401 of 12458
Scan 1: estimating training effect sizes for Chunk 7601 of 12458
Scan 1: estimating training effect sizes for Chunk 7801 of 12458
Scan 1: estimating training effect sizes for Chunk 8001 of 12458
Scan 1: estimating training effect sizes for Chunk 8201 of 12458
Scan 1: estimating training effect sizes for Chunk 8401 of 12458
Scan 1: estimating training effect sizes for Chunk 8601 of 12458
Scan 1: estimating training effect sizes for Chunk 8801 of 12458
Scan 1: estimating training effect sizes for Chunk 9001 of 12458
Scan 1: estimating training effect sizes for Chunk 9201 of 12458
Scan 1: estimating training effect sizes for Chunk 9401 of 12458
Scan 1: estimating training effect sizes for Chunk 9601 of 12458
Scan 1: estimating training effect sizes for Chunk 9801 of 12458
Scan 1: estimating training effect sizes for Chunk 10001 of 12458
Scan 1: estimating training effect sizes for Chunk 10201 of 12458
Scan 1: estimating training effect sizes for Chunk 10401 of 12458
Scan 1: estimating training effect sizes for Chunk 10601 of 12458
Scan 1: estimating training effect sizes for Chunk 10801 of 12458
Scan 1: estimating training effect sizes for Chunk 11001 of 12458
Scan 1: estimating training effect sizes for Chunk 11201 of 12458
Scan 1: estimating training effect sizes for Chunk 11401 of 12458
Scan 1: estimating training effect sizes for Chunk 11601 of 12458
Scan 1: estimating training effect sizes for Chunk 11801 of 12458
Scan 1: estimating training effect sizes for Chunk 12001 of 12458
Scan 1: estimating training effect sizes for Chunk 12201 of 12458
Scan 1: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.62

Scan 2: estimating training effect sizes for Chunk 1 of 12458
Scan 2: estimating training effect sizes for Chunk 201 of 12458
Scan 2: estimating training effect sizes for Chunk 401 of 12458
Scan 2: estimating training effect sizes for Chunk 601 of 12458
Scan 2: estimating training effect sizes for Chunk 801 of 12458
Scan 2: estimating training effect sizes for Chunk 1001 of 12458
Scan 2: estimating training effect sizes for Chunk 1201 of 12458
Scan 2: estimating training effect sizes for Chunk 1401 of 12458
Scan 2: estimating training effect sizes for Chunk 1601 of 12458
Scan 2: estimating training effect sizes for Chunk 1801 of 12458
Scan 2: estimating training effect sizes for Chunk 2001 of 12458
Scan 2: estimating training effect sizes for Chunk 2201 of 12458
Scan 2: estimating training effect sizes for Chunk 2401 of 12458
Scan 2: estimating training effect sizes for Chunk 2601 of 12458
Scan 2: estimating training effect sizes for Chunk 2801 of 12458
Scan 2: estimating training effect sizes for Chunk 3001 of 12458
Scan 2: estimating training effect sizes for Chunk 3201 of 12458
Scan 2: estimating training effect sizes for Chunk 3401 of 12458
Scan 2: estimating training effect sizes for Chunk 3601 of 12458
Scan 2: estimating training effect sizes for Chunk 3801 of 12458
Scan 2: estimating training effect sizes for Chunk 4001 of 12458
Scan 2: estimating training effect sizes for Chunk 4201 of 12458
Scan 2: estimating training effect sizes for Chunk 4401 of 12458
Scan 2: estimating training effect sizes for Chunk 4601 of 12458
Scan 2: estimating training effect sizes for Chunk 4801 of 12458
Scan 2: estimating training effect sizes for Chunk 5001 of 12458
Scan 2: estimating training effect sizes for Chunk 5201 of 12458
Scan 2: estimating training effect sizes for Chunk 5401 of 12458
Scan 2: estimating training effect sizes for Chunk 5601 of 12458
Scan 2: estimating training effect sizes for Chunk 5801 of 12458
Scan 2: estimating training effect sizes for Chunk 6001 of 12458
Scan 2: estimating training effect sizes for Chunk 6201 of 12458
Scan 2: estimating training effect sizes for Chunk 6401 of 12458
Scan 2: estimating training effect sizes for Chunk 6601 of 12458
Scan 2: estimating training effect sizes for Chunk 6801 of 12458
Scan 2: estimating training effect sizes for Chunk 7001 of 12458
Scan 2: estimating training effect sizes for Chunk 7201 of 12458
Scan 2: estimating training effect sizes for Chunk 7401 of 12458
Scan 2: estimating training effect sizes for Chunk 7601 of 12458
Scan 2: estimating training effect sizes for Chunk 7801 of 12458
Scan 2: estimating training effect sizes for Chunk 8001 of 12458
Scan 2: estimating training effect sizes for Chunk 8201 of 12458
Scan 2: estimating training effect sizes for Chunk 8401 of 12458
Scan 2: estimating training effect sizes for Chunk 8601 of 12458
Scan 2: estimating training effect sizes for Chunk 8801 of 12458
Scan 2: estimating training effect sizes for Chunk 9001 of 12458
Scan 2: estimating training effect sizes for Chunk 9201 of 12458
Scan 2: estimating training effect sizes for Chunk 9401 of 12458
Scan 2: estimating training effect sizes for Chunk 9601 of 12458
Scan 2: estimating training effect sizes for Chunk 9801 of 12458
Scan 2: estimating training effect sizes for Chunk 10001 of 12458
Scan 2: estimating training effect sizes for Chunk 10201 of 12458
Scan 2: estimating training effect sizes for Chunk 10401 of 12458
Scan 2: estimating training effect sizes for Chunk 10601 of 12458
Scan 2: estimating training effect sizes for Chunk 10801 of 12458
Scan 2: estimating training effect sizes for Chunk 11001 of 12458
Scan 2: estimating training effect sizes for Chunk 11201 of 12458
Scan 2: estimating training effect sizes for Chunk 11401 of 12458
Scan 2: estimating training effect sizes for Chunk 11601 of 12458
Scan 2: estimating training effect sizes for Chunk 11801 of 12458
Scan 2: estimating training effect sizes for Chunk 12001 of 12458
Scan 2: estimating training effect sizes for Chunk 12201 of 12458
Scan 2: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.13

Scan 3: estimating training effect sizes for Chunk 1 of 12458
Scan 3: estimating training effect sizes for Chunk 201 of 12458
Scan 3: estimating training effect sizes for Chunk 401 of 12458
Scan 3: estimating training effect sizes for Chunk 601 of 12458
Scan 3: estimating training effect sizes for Chunk 801 of 12458
Scan 3: estimating training effect sizes for Chunk 1001 of 12458
Scan 3: estimating training effect sizes for Chunk 1201 of 12458
Scan 3: estimating training effect sizes for Chunk 1401 of 12458
Scan 3: estimating training effect sizes for Chunk 1601 of 12458
Scan 3: estimating training effect sizes for Chunk 1801 of 12458
Scan 3: estimating training effect sizes for Chunk 2001 of 12458
Scan 3: estimating training effect sizes for Chunk 2201 of 12458
Scan 3: estimating training effect sizes for Chunk 2401 of 12458
Scan 3: estimating training effect sizes for Chunk 2601 of 12458
Scan 3: estimating training effect sizes for Chunk 2801 of 12458
Scan 3: estimating training effect sizes for Chunk 3001 of 12458
Scan 3: estimating training effect sizes for Chunk 3201 of 12458
Scan 3: estimating training effect sizes for Chunk 3401 of 12458
Scan 3: estimating training effect sizes for Chunk 3601 of 12458
Scan 3: estimating training effect sizes for Chunk 3801 of 12458
Scan 3: estimating training effect sizes for Chunk 4001 of 12458
Scan 3: estimating training effect sizes for Chunk 4201 of 12458
Scan 3: estimating training effect sizes for Chunk 4401 of 12458
Scan 3: estimating training effect sizes for Chunk 4601 of 12458
Scan 3: estimating training effect sizes for Chunk 4801 of 12458
Scan 3: estimating training effect sizes for Chunk 5001 of 12458
Scan 3: estimating training effect sizes for Chunk 5201 of 12458
Scan 3: estimating training effect sizes for Chunk 5401 of 12458
Scan 3: estimating training effect sizes for Chunk 5601 of 12458
Scan 3: estimating training effect sizes for Chunk 5801 of 12458
Scan 3: estimating training effect sizes for Chunk 6001 of 12458
Scan 3: estimating training effect sizes for Chunk 6201 of 12458
Scan 3: estimating training effect sizes for Chunk 6401 of 12458
Scan 3: estimating training effect sizes for Chunk 6601 of 12458
Scan 3: estimating training effect sizes for Chunk 6801 of 12458
Scan 3: estimating training effect sizes for Chunk 7001 of 12458
Scan 3: estimating training effect sizes for Chunk 7201 of 12458
Scan 3: estimating training effect sizes for Chunk 7401 of 12458
Scan 3: estimating training effect sizes for Chunk 7601 of 12458
Scan 3: estimating training effect sizes for Chunk 7801 of 12458
Scan 3: estimating training effect sizes for Chunk 8001 of 12458
Scan 3: estimating training effect sizes for Chunk 8201 of 12458
Scan 3: estimating training effect sizes for Chunk 8401 of 12458
Scan 3: estimating training effect sizes for Chunk 8601 of 12458
Scan 3: estimating training effect sizes for Chunk 8801 of 12458
Scan 3: estimating training effect sizes for Chunk 9001 of 12458
Scan 3: estimating training effect sizes for Chunk 9201 of 12458
Scan 3: estimating training effect sizes for Chunk 9401 of 12458
Scan 3: estimating training effect sizes for Chunk 9601 of 12458
Scan 3: estimating training effect sizes for Chunk 9801 of 12458
Scan 3: estimating training effect sizes for Chunk 10001 of 12458
Scan 3: estimating training effect sizes for Chunk 10201 of 12458
Scan 3: estimating training effect sizes for Chunk 10401 of 12458
Scan 3: estimating training effect sizes for Chunk 10601 of 12458
Scan 3: estimating training effect sizes for Chunk 10801 of 12458
Scan 3: estimating training effect sizes for Chunk 11001 of 12458
Scan 3: estimating training effect sizes for Chunk 11201 of 12458
Scan 3: estimating training effect sizes for Chunk 11401 of 12458
Scan 3: estimating training effect sizes for Chunk 11601 of 12458
Scan 3: estimating training effect sizes for Chunk 11801 of 12458
Scan 3: estimating training effect sizes for Chunk 12001 of 12458
Scan 3: estimating training effect sizes for Chunk 12201 of 12458
Scan 3: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.01

Scan 4: estimating training effect sizes for Chunk 1 of 12458
Scan 4: estimating training effect sizes for Chunk 201 of 12458
Scan 4: estimating training effect sizes for Chunk 401 of 12458
Scan 4: estimating training effect sizes for Chunk 601 of 12458
Scan 4: estimating training effect sizes for Chunk 801 of 12458
Scan 4: estimating training effect sizes for Chunk 1001 of 12458
Scan 4: estimating training effect sizes for Chunk 1201 of 12458
Scan 4: estimating training effect sizes for Chunk 1401 of 12458
Scan 4: estimating training effect sizes for Chunk 1601 of 12458
Scan 4: estimating training effect sizes for Chunk 1801 of 12458
Scan 4: estimating training effect sizes for Chunk 2001 of 12458
Scan 4: estimating training effect sizes for Chunk 2201 of 12458
Scan 4: estimating training effect sizes for Chunk 2401 of 12458
Scan 4: estimating training effect sizes for Chunk 2601 of 12458
Scan 4: estimating training effect sizes for Chunk 2801 of 12458
Scan 4: estimating training effect sizes for Chunk 3001 of 12458
Scan 4: estimating training effect sizes for Chunk 3201 of 12458
Scan 4: estimating training effect sizes for Chunk 3401 of 12458
Scan 4: estimating training effect sizes for Chunk 3601 of 12458
Scan 4: estimating training effect sizes for Chunk 3801 of 12458
Scan 4: estimating training effect sizes for Chunk 4001 of 12458
Scan 4: estimating training effect sizes for Chunk 4201 of 12458
Scan 4: estimating training effect sizes for Chunk 4401 of 12458
Scan 4: estimating training effect sizes for Chunk 4601 of 12458
Scan 4: estimating training effect sizes for Chunk 4801 of 12458
Scan 4: estimating training effect sizes for Chunk 5001 of 12458
Scan 4: estimating training effect sizes for Chunk 5201 of 12458
Scan 4: estimating training effect sizes for Chunk 5401 of 12458
Scan 4: estimating training effect sizes for Chunk 5601 of 12458
Scan 4: estimating training effect sizes for Chunk 5801 of 12458
Scan 4: estimating training effect sizes for Chunk 6001 of 12458
Scan 4: estimating training effect sizes for Chunk 6201 of 12458
Scan 4: estimating training effect sizes for Chunk 6401 of 12458
Scan 4: estimating training effect sizes for Chunk 6601 of 12458
Scan 4: estimating training effect sizes for Chunk 6801 of 12458
Scan 4: estimating training effect sizes for Chunk 7001 of 12458
Scan 4: estimating training effect sizes for Chunk 7201 of 12458
Scan 4: estimating training effect sizes for Chunk 7401 of 12458
Scan 4: estimating training effect sizes for Chunk 7601 of 12458
Scan 4: estimating training effect sizes for Chunk 7801 of 12458
Scan 4: estimating training effect sizes for Chunk 8001 of 12458
Scan 4: estimating training effect sizes for Chunk 8201 of 12458
Scan 4: estimating training effect sizes for Chunk 8401 of 12458
Scan 4: estimating training effect sizes for Chunk 8601 of 12458
Scan 4: estimating training effect sizes for Chunk 8801 of 12458
Scan 4: estimating training effect sizes for Chunk 9001 of 12458
Scan 4: estimating training effect sizes for Chunk 9201 of 12458
Scan 4: estimating training effect sizes for Chunk 9401 of 12458
Scan 4: estimating training effect sizes for Chunk 9601 of 12458
Scan 4: estimating training effect sizes for Chunk 9801 of 12458
Scan 4: estimating training effect sizes for Chunk 10001 of 12458
Scan 4: estimating training effect sizes for Chunk 10201 of 12458
Scan 4: estimating training effect sizes for Chunk 10401 of 12458
Scan 4: estimating training effect sizes for Chunk 10601 of 12458
Scan 4: estimating training effect sizes for Chunk 10801 of 12458
Scan 4: estimating training effect sizes for Chunk 11001 of 12458
Scan 4: estimating training effect sizes for Chunk 11201 of 12458
Scan 4: estimating training effect sizes for Chunk 11401 of 12458
Scan 4: estimating training effect sizes for Chunk 11601 of 12458
Scan 4: estimating training effect sizes for Chunk 11801 of 12458
Scan 4: estimating training effect sizes for Chunk 12001 of 12458
Scan 4: estimating training effect sizes for Chunk 12201 of 12458
Scan 4: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.00

Scan 5: estimating training effect sizes for Chunk 1 of 12458
Scan 5: estimating training effect sizes for Chunk 201 of 12458
Scan 5: estimating training effect sizes for Chunk 401 of 12458
Scan 5: estimating training effect sizes for Chunk 601 of 12458
Scan 5: estimating training effect sizes for Chunk 801 of 12458
Scan 5: estimating training effect sizes for Chunk 1001 of 12458
Scan 5: estimating training effect sizes for Chunk 1201 of 12458
Scan 5: estimating training effect sizes for Chunk 1401 of 12458
Scan 5: estimating training effect sizes for Chunk 1601 of 12458
Scan 5: estimating training effect sizes for Chunk 1801 of 12458
Scan 5: estimating training effect sizes for Chunk 2001 of 12458
Scan 5: estimating training effect sizes for Chunk 2201 of 12458
Scan 5: estimating training effect sizes for Chunk 2401 of 12458
Scan 5: estimating training effect sizes for Chunk 2601 of 12458
Scan 5: estimating training effect sizes for Chunk 2801 of 12458
Scan 5: estimating training effect sizes for Chunk 3001 of 12458
Scan 5: estimating training effect sizes for Chunk 3201 of 12458
Scan 5: estimating training effect sizes for Chunk 3401 of 12458
Scan 5: estimating training effect sizes for Chunk 3601 of 12458
Scan 5: estimating training effect sizes for Chunk 3801 of 12458
Scan 5: estimating training effect sizes for Chunk 4001 of 12458
Scan 5: estimating training effect sizes for Chunk 4201 of 12458
Scan 5: estimating training effect sizes for Chunk 4401 of 12458
Scan 5: estimating training effect sizes for Chunk 4601 of 12458
Scan 5: estimating training effect sizes for Chunk 4801 of 12458
Scan 5: estimating training effect sizes for Chunk 5001 of 12458
Scan 5: estimating training effect sizes for Chunk 5201 of 12458
Scan 5: estimating training effect sizes for Chunk 5401 of 12458
Scan 5: estimating training effect sizes for Chunk 5601 of 12458
Scan 5: estimating training effect sizes for Chunk 5801 of 12458
Scan 5: estimating training effect sizes for Chunk 6001 of 12458
Scan 5: estimating training effect sizes for Chunk 6201 of 12458
Scan 5: estimating training effect sizes for Chunk 6401 of 12458
Scan 5: estimating training effect sizes for Chunk 6601 of 12458
Scan 5: estimating training effect sizes for Chunk 6801 of 12458
Scan 5: estimating training effect sizes for Chunk 7001 of 12458
Scan 5: estimating training effect sizes for Chunk 7201 of 12458
Scan 5: estimating training effect sizes for Chunk 7401 of 12458
Scan 5: estimating training effect sizes for Chunk 7601 of 12458
Scan 5: estimating training effect sizes for Chunk 7801 of 12458
Scan 5: estimating training effect sizes for Chunk 8001 of 12458
Scan 5: estimating training effect sizes for Chunk 8201 of 12458
Scan 5: estimating training effect sizes for Chunk 8401 of 12458
Scan 5: estimating training effect sizes for Chunk 8601 of 12458
Scan 5: estimating training effect sizes for Chunk 8801 of 12458
Scan 5: estimating training effect sizes for Chunk 9001 of 12458
Scan 5: estimating training effect sizes for Chunk 9201 of 12458
Scan 5: estimating training effect sizes for Chunk 9401 of 12458
Scan 5: estimating training effect sizes for Chunk 9601 of 12458
Scan 5: estimating training effect sizes for Chunk 9801 of 12458
Scan 5: estimating training effect sizes for Chunk 10001 of 12458
Scan 5: estimating training effect sizes for Chunk 10201 of 12458
Scan 5: estimating training effect sizes for Chunk 10401 of 12458
Scan 5: estimating training effect sizes for Chunk 10601 of 12458
Scan 5: estimating training effect sizes for Chunk 10801 of 12458
Scan 5: estimating training effect sizes for Chunk 11001 of 12458
Scan 5: estimating training effect sizes for Chunk 11201 of 12458
Scan 5: estimating training effect sizes for Chunk 11401 of 12458
Scan 5: estimating training effect sizes for Chunk 11601 of 12458
Scan 5: estimating training effect sizes for Chunk 11801 of 12458
Scan 5: estimating training effect sizes for Chunk 12001 of 12458
Scan 5: estimating training effect sizes for Chunk 12201 of 12458
Scan 5: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.00

Scan 6: estimating training effect sizes for Chunk 1 of 12458
Scan 6: estimating training effect sizes for Chunk 201 of 12458
Scan 6: estimating training effect sizes for Chunk 401 of 12458
Scan 6: estimating training effect sizes for Chunk 601 of 12458
Scan 6: estimating training effect sizes for Chunk 801 of 12458
Scan 6: estimating training effect sizes for Chunk 1001 of 12458
Scan 6: estimating training effect sizes for Chunk 1201 of 12458
Scan 6: estimating training effect sizes for Chunk 1401 of 12458
Scan 6: estimating training effect sizes for Chunk 1601 of 12458
Scan 6: estimating training effect sizes for Chunk 1801 of 12458
Scan 6: estimating training effect sizes for Chunk 2001 of 12458
Scan 6: estimating training effect sizes for Chunk 2201 of 12458
Scan 6: estimating training effect sizes for Chunk 2401 of 12458
Scan 6: estimating training effect sizes for Chunk 2601 of 12458
Scan 6: estimating training effect sizes for Chunk 2801 of 12458
Scan 6: estimating training effect sizes for Chunk 3001 of 12458
Scan 6: estimating training effect sizes for Chunk 3201 of 12458
Scan 6: estimating training effect sizes for Chunk 3401 of 12458
Scan 6: estimating training effect sizes for Chunk 3601 of 12458
Scan 6: estimating training effect sizes for Chunk 3801 of 12458
Scan 6: estimating training effect sizes for Chunk 4001 of 12458
Scan 6: estimating training effect sizes for Chunk 4201 of 12458
Scan 6: estimating training effect sizes for Chunk 4401 of 12458
Scan 6: estimating training effect sizes for Chunk 4601 of 12458
Scan 6: estimating training effect sizes for Chunk 4801 of 12458
Scan 6: estimating training effect sizes for Chunk 5001 of 12458
Scan 6: estimating training effect sizes for Chunk 5201 of 12458
Scan 6: estimating training effect sizes for Chunk 5401 of 12458
Scan 6: estimating training effect sizes for Chunk 5601 of 12458
Scan 6: estimating training effect sizes for Chunk 5801 of 12458
Scan 6: estimating training effect sizes for Chunk 6001 of 12458
Scan 6: estimating training effect sizes for Chunk 6201 of 12458
Scan 6: estimating training effect sizes for Chunk 6401 of 12458
Scan 6: estimating training effect sizes for Chunk 6601 of 12458
Scan 6: estimating training effect sizes for Chunk 6801 of 12458
Scan 6: estimating training effect sizes for Chunk 7001 of 12458
Scan 6: estimating training effect sizes for Chunk 7201 of 12458
Scan 6: estimating training effect sizes for Chunk 7401 of 12458
Scan 6: estimating training effect sizes for Chunk 7601 of 12458
Scan 6: estimating training effect sizes for Chunk 7801 of 12458
Scan 6: estimating training effect sizes for Chunk 8001 of 12458
Scan 6: estimating training effect sizes for Chunk 8201 of 12458
Scan 6: estimating training effect sizes for Chunk 8401 of 12458
Scan 6: estimating training effect sizes for Chunk 8601 of 12458
Scan 6: estimating training effect sizes for Chunk 8801 of 12458
Scan 6: estimating training effect sizes for Chunk 9001 of 12458
Scan 6: estimating training effect sizes for Chunk 9201 of 12458
Scan 6: estimating training effect sizes for Chunk 9401 of 12458
Scan 6: estimating training effect sizes for Chunk 9601 of 12458
Scan 6: estimating training effect sizes for Chunk 9801 of 12458
Scan 6: estimating training effect sizes for Chunk 10001 of 12458
Scan 6: estimating training effect sizes for Chunk 10201 of 12458
Scan 6: estimating training effect sizes for Chunk 10401 of 12458
Scan 6: estimating training effect sizes for Chunk 10601 of 12458
Scan 6: estimating training effect sizes for Chunk 10801 of 12458
Scan 6: estimating training effect sizes for Chunk 11001 of 12458
Scan 6: estimating training effect sizes for Chunk 11201 of 12458
Scan 6: estimating training effect sizes for Chunk 11401 of 12458
Scan 6: estimating training effect sizes for Chunk 11601 of 12458
Scan 6: estimating training effect sizes for Chunk 11801 of 12458
Scan 6: estimating training effect sizes for Chunk 12001 of 12458
Scan 6: estimating training effect sizes for Chunk 12201 of 12458
Scan 6: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.00

Scan 7: estimating training effect sizes for Chunk 1 of 12458
Scan 7: estimating training effect sizes for Chunk 201 of 12458
Scan 7: estimating training effect sizes for Chunk 401 of 12458
Scan 7: estimating training effect sizes for Chunk 601 of 12458
Scan 7: estimating training effect sizes for Chunk 801 of 12458
Scan 7: estimating training effect sizes for Chunk 1001 of 12458
Scan 7: estimating training effect sizes for Chunk 1201 of 12458
Scan 7: estimating training effect sizes for Chunk 1401 of 12458
Scan 7: estimating training effect sizes for Chunk 1601 of 12458
Scan 7: estimating training effect sizes for Chunk 1801 of 12458
Scan 7: estimating training effect sizes for Chunk 2001 of 12458
Scan 7: estimating training effect sizes for Chunk 2201 of 12458
Scan 7: estimating training effect sizes for Chunk 2401 of 12458
Scan 7: estimating training effect sizes for Chunk 2601 of 12458
Scan 7: estimating training effect sizes for Chunk 2801 of 12458
Scan 7: estimating training effect sizes for Chunk 3001 of 12458
Scan 7: estimating training effect sizes for Chunk 3201 of 12458
Scan 7: estimating training effect sizes for Chunk 3401 of 12458
Scan 7: estimating training effect sizes for Chunk 3601 of 12458
Scan 7: estimating training effect sizes for Chunk 3801 of 12458
Scan 7: estimating training effect sizes for Chunk 4001 of 12458
Scan 7: estimating training effect sizes for Chunk 4201 of 12458
Scan 7: estimating training effect sizes for Chunk 4401 of 12458
Scan 7: estimating training effect sizes for Chunk 4601 of 12458
Scan 7: estimating training effect sizes for Chunk 4801 of 12458
Scan 7: estimating training effect sizes for Chunk 5001 of 12458
Scan 7: estimating training effect sizes for Chunk 5201 of 12458
Scan 7: estimating training effect sizes for Chunk 5401 of 12458
Scan 7: estimating training effect sizes for Chunk 5601 of 12458
Scan 7: estimating training effect sizes for Chunk 5801 of 12458
Scan 7: estimating training effect sizes for Chunk 6001 of 12458
Scan 7: estimating training effect sizes for Chunk 6201 of 12458
Scan 7: estimating training effect sizes for Chunk 6401 of 12458
Scan 7: estimating training effect sizes for Chunk 6601 of 12458
Scan 7: estimating training effect sizes for Chunk 6801 of 12458
Scan 7: estimating training effect sizes for Chunk 7001 of 12458
Scan 7: estimating training effect sizes for Chunk 7201 of 12458
Scan 7: estimating training effect sizes for Chunk 7401 of 12458
Scan 7: estimating training effect sizes for Chunk 7601 of 12458
Scan 7: estimating training effect sizes for Chunk 7801 of 12458
Scan 7: estimating training effect sizes for Chunk 8001 of 12458
Scan 7: estimating training effect sizes for Chunk 8201 of 12458
Scan 7: estimating training effect sizes for Chunk 8401 of 12458
Scan 7: estimating training effect sizes for Chunk 8601 of 12458
Scan 7: estimating training effect sizes for Chunk 8801 of 12458
Scan 7: estimating training effect sizes for Chunk 9001 of 12458
Scan 7: estimating training effect sizes for Chunk 9201 of 12458
Scan 7: estimating training effect sizes for Chunk 9401 of 12458
Scan 7: estimating training effect sizes for Chunk 9601 of 12458
Scan 7: estimating training effect sizes for Chunk 9801 of 12458
Scan 7: estimating training effect sizes for Chunk 10001 of 12458
Scan 7: estimating training effect sizes for Chunk 10201 of 12458
Scan 7: estimating training effect sizes for Chunk 10401 of 12458
Scan 7: estimating training effect sizes for Chunk 10601 of 12458
Scan 7: estimating training effect sizes for Chunk 10801 of 12458
Scan 7: estimating training effect sizes for Chunk 11001 of 12458
Scan 7: estimating training effect sizes for Chunk 11201 of 12458
Scan 7: estimating training effect sizes for Chunk 11401 of 12458
Scan 7: estimating training effect sizes for Chunk 11601 of 12458
Scan 7: estimating training effect sizes for Chunk 11801 of 12458
Scan 7: estimating training effect sizes for Chunk 12001 of 12458
Scan 7: estimating training effect sizes for Chunk 12201 of 12458
Scan 7: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.00

Scan 8: estimating training effect sizes for Chunk 1 of 12458
Scan 8: estimating training effect sizes for Chunk 201 of 12458
Scan 8: estimating training effect sizes for Chunk 401 of 12458
Scan 8: estimating training effect sizes for Chunk 601 of 12458
Scan 8: estimating training effect sizes for Chunk 801 of 12458
Scan 8: estimating training effect sizes for Chunk 1001 of 12458
Scan 8: estimating training effect sizes for Chunk 1201 of 12458
Scan 8: estimating training effect sizes for Chunk 1401 of 12458
Scan 8: estimating training effect sizes for Chunk 1601 of 12458
Scan 8: estimating training effect sizes for Chunk 1801 of 12458
Scan 8: estimating training effect sizes for Chunk 2001 of 12458
Scan 8: estimating training effect sizes for Chunk 2201 of 12458
Scan 8: estimating training effect sizes for Chunk 2401 of 12458
Scan 8: estimating training effect sizes for Chunk 2601 of 12458
Scan 8: estimating training effect sizes for Chunk 2801 of 12458
Scan 8: estimating training effect sizes for Chunk 3001 of 12458
Scan 8: estimating training effect sizes for Chunk 3201 of 12458
Scan 8: estimating training effect sizes for Chunk 3401 of 12458
Scan 8: estimating training effect sizes for Chunk 3601 of 12458
Scan 8: estimating training effect sizes for Chunk 3801 of 12458
Scan 8: estimating training effect sizes for Chunk 4001 of 12458
Scan 8: estimating training effect sizes for Chunk 4201 of 12458
Scan 8: estimating training effect sizes for Chunk 4401 of 12458
Scan 8: estimating training effect sizes for Chunk 4601 of 12458
Scan 8: estimating training effect sizes for Chunk 4801 of 12458
Scan 8: estimating training effect sizes for Chunk 5001 of 12458
Scan 8: estimating training effect sizes for Chunk 5201 of 12458
Scan 8: estimating training effect sizes for Chunk 5401 of 12458
Scan 8: estimating training effect sizes for Chunk 5601 of 12458
Scan 8: estimating training effect sizes for Chunk 5801 of 12458
Scan 8: estimating training effect sizes for Chunk 6001 of 12458
Scan 8: estimating training effect sizes for Chunk 6201 of 12458
Scan 8: estimating training effect sizes for Chunk 6401 of 12458
Scan 8: estimating training effect sizes for Chunk 6601 of 12458
Scan 8: estimating training effect sizes for Chunk 6801 of 12458
Scan 8: estimating training effect sizes for Chunk 7001 of 12458
Scan 8: estimating training effect sizes for Chunk 7201 of 12458
Scan 8: estimating training effect sizes for Chunk 7401 of 12458
Scan 8: estimating training effect sizes for Chunk 7601 of 12458
Scan 8: estimating training effect sizes for Chunk 7801 of 12458
Scan 8: estimating training effect sizes for Chunk 8001 of 12458
Scan 8: estimating training effect sizes for Chunk 8201 of 12458
Scan 8: estimating training effect sizes for Chunk 8401 of 12458
Scan 8: estimating training effect sizes for Chunk 8601 of 12458
Scan 8: estimating training effect sizes for Chunk 8801 of 12458
Scan 8: estimating training effect sizes for Chunk 9001 of 12458
Scan 8: estimating training effect sizes for Chunk 9201 of 12458
Scan 8: estimating training effect sizes for Chunk 9401 of 12458
Scan 8: estimating training effect sizes for Chunk 9601 of 12458
Scan 8: estimating training effect sizes for Chunk 9801 of 12458
Scan 8: estimating training effect sizes for Chunk 10001 of 12458
Scan 8: estimating training effect sizes for Chunk 10201 of 12458
Scan 8: estimating training effect sizes for Chunk 10401 of 12458
Scan 8: estimating training effect sizes for Chunk 10601 of 12458
Scan 8: estimating training effect sizes for Chunk 10801 of 12458
Scan 8: estimating training effect sizes for Chunk 11001 of 12458
Scan 8: estimating training effect sizes for Chunk 11201 of 12458
Scan 8: estimating training effect sizes for Chunk 11401 of 12458
Scan 8: estimating training effect sizes for Chunk 11601 of 12458
Scan 8: estimating training effect sizes for Chunk 11801 of 12458
Scan 8: estimating training effect sizes for Chunk 12001 of 12458
Scan 8: estimating training effect sizes for Chunk 12201 of 12458
Scan 8: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.00

Scan 9: estimating training effect sizes for Chunk 1 of 12458
Scan 9: estimating training effect sizes for Chunk 201 of 12458
Scan 9: estimating training effect sizes for Chunk 401 of 12458
Scan 9: estimating training effect sizes for Chunk 601 of 12458
Scan 9: estimating training effect sizes for Chunk 801 of 12458
Scan 9: estimating training effect sizes for Chunk 1001 of 12458
Scan 9: estimating training effect sizes for Chunk 1201 of 12458
Scan 9: estimating training effect sizes for Chunk 1401 of 12458
Scan 9: estimating training effect sizes for Chunk 1601 of 12458
Scan 9: estimating training effect sizes for Chunk 1801 of 12458
Scan 9: estimating training effect sizes for Chunk 2001 of 12458
Scan 9: estimating training effect sizes for Chunk 2201 of 12458
Scan 9: estimating training effect sizes for Chunk 2401 of 12458
Scan 9: estimating training effect sizes for Chunk 2601 of 12458
Scan 9: estimating training effect sizes for Chunk 2801 of 12458
Scan 9: estimating training effect sizes for Chunk 3001 of 12458
Scan 9: estimating training effect sizes for Chunk 3201 of 12458
Scan 9: estimating training effect sizes for Chunk 3401 of 12458
Scan 9: estimating training effect sizes for Chunk 3601 of 12458
Scan 9: estimating training effect sizes for Chunk 3801 of 12458
Scan 9: estimating training effect sizes for Chunk 4001 of 12458
Scan 9: estimating training effect sizes for Chunk 4201 of 12458
Scan 9: estimating training effect sizes for Chunk 4401 of 12458
Scan 9: estimating training effect sizes for Chunk 4601 of 12458
Scan 9: estimating training effect sizes for Chunk 4801 of 12458
Scan 9: estimating training effect sizes for Chunk 5001 of 12458
Scan 9: estimating training effect sizes for Chunk 5201 of 12458
Scan 9: estimating training effect sizes for Chunk 5401 of 12458
Scan 9: estimating training effect sizes for Chunk 5601 of 12458
Scan 9: estimating training effect sizes for Chunk 5801 of 12458
Scan 9: estimating training effect sizes for Chunk 6001 of 12458
Scan 9: estimating training effect sizes for Chunk 6201 of 12458
Scan 9: estimating training effect sizes for Chunk 6401 of 12458
Scan 9: estimating training effect sizes for Chunk 6601 of 12458
Scan 9: estimating training effect sizes for Chunk 6801 of 12458
Scan 9: estimating training effect sizes for Chunk 7001 of 12458
Scan 9: estimating training effect sizes for Chunk 7201 of 12458
Scan 9: estimating training effect sizes for Chunk 7401 of 12458
Scan 9: estimating training effect sizes for Chunk 7601 of 12458
Scan 9: estimating training effect sizes for Chunk 7801 of 12458
Scan 9: estimating training effect sizes for Chunk 8001 of 12458
Scan 9: estimating training effect sizes for Chunk 8201 of 12458
Scan 9: estimating training effect sizes for Chunk 8401 of 12458
Scan 9: estimating training effect sizes for Chunk 8601 of 12458
Scan 9: estimating training effect sizes for Chunk 8801 of 12458
Scan 9: estimating training effect sizes for Chunk 9001 of 12458
Scan 9: estimating training effect sizes for Chunk 9201 of 12458
Scan 9: estimating training effect sizes for Chunk 9401 of 12458
Scan 9: estimating training effect sizes for Chunk 9601 of 12458
Scan 9: estimating training effect sizes for Chunk 9801 of 12458
Scan 9: estimating training effect sizes for Chunk 10001 of 12458
Scan 9: estimating training effect sizes for Chunk 10201 of 12458
Scan 9: estimating training effect sizes for Chunk 10401 of 12458
Scan 9: estimating training effect sizes for Chunk 10601 of 12458
Scan 9: estimating training effect sizes for Chunk 10801 of 12458
Scan 9: estimating training effect sizes for Chunk 11001 of 12458
Scan 9: estimating training effect sizes for Chunk 11201 of 12458
Scan 9: estimating training effect sizes for Chunk 11401 of 12458
Scan 9: estimating training effect sizes for Chunk 11601 of 12458
Scan 9: estimating training effect sizes for Chunk 11801 of 12458
Scan 9: estimating training effect sizes for Chunk 12001 of 12458
Scan 9: estimating training effect sizes for Chunk 12201 of 12458
Scan 9: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.00

Scan 10: estimating training effect sizes for Chunk 1 of 12458
Scan 10: estimating training effect sizes for Chunk 201 of 12458
Scan 10: estimating training effect sizes for Chunk 401 of 12458
Scan 10: estimating training effect sizes for Chunk 601 of 12458
Scan 10: estimating training effect sizes for Chunk 801 of 12458
Scan 10: estimating training effect sizes for Chunk 1001 of 12458
Scan 10: estimating training effect sizes for Chunk 1201 of 12458
Scan 10: estimating training effect sizes for Chunk 1401 of 12458
Scan 10: estimating training effect sizes for Chunk 1601 of 12458
Scan 10: estimating training effect sizes for Chunk 1801 of 12458
Scan 10: estimating training effect sizes for Chunk 2001 of 12458
Scan 10: estimating training effect sizes for Chunk 2201 of 12458
Scan 10: estimating training effect sizes for Chunk 2401 of 12458
Scan 10: estimating training effect sizes for Chunk 2601 of 12458
Scan 10: estimating training effect sizes for Chunk 2801 of 12458
Scan 10: estimating training effect sizes for Chunk 3001 of 12458
Scan 10: estimating training effect sizes for Chunk 3201 of 12458
Scan 10: estimating training effect sizes for Chunk 3401 of 12458
Scan 10: estimating training effect sizes for Chunk 3601 of 12458
Scan 10: estimating training effect sizes for Chunk 3801 of 12458
Scan 10: estimating training effect sizes for Chunk 4001 of 12458
Scan 10: estimating training effect sizes for Chunk 4201 of 12458
Scan 10: estimating training effect sizes for Chunk 4401 of 12458
Scan 10: estimating training effect sizes for Chunk 4601 of 12458
Scan 10: estimating training effect sizes for Chunk 4801 of 12458
Scan 10: estimating training effect sizes for Chunk 5001 of 12458
Scan 10: estimating training effect sizes for Chunk 5201 of 12458
Scan 10: estimating training effect sizes for Chunk 5401 of 12458
Scan 10: estimating training effect sizes for Chunk 5601 of 12458
Scan 10: estimating training effect sizes for Chunk 5801 of 12458
Scan 10: estimating training effect sizes for Chunk 6001 of 12458
Scan 10: estimating training effect sizes for Chunk 6201 of 12458
Scan 10: estimating training effect sizes for Chunk 6401 of 12458
Scan 10: estimating training effect sizes for Chunk 6601 of 12458
Scan 10: estimating training effect sizes for Chunk 6801 of 12458
Scan 10: estimating training effect sizes for Chunk 7001 of 12458
Scan 10: estimating training effect sizes for Chunk 7201 of 12458
Scan 10: estimating training effect sizes for Chunk 7401 of 12458
Scan 10: estimating training effect sizes for Chunk 7601 of 12458
Scan 10: estimating training effect sizes for Chunk 7801 of 12458
Scan 10: estimating training effect sizes for Chunk 8001 of 12458
Scan 10: estimating training effect sizes for Chunk 8201 of 12458
Scan 10: estimating training effect sizes for Chunk 8401 of 12458
Scan 10: estimating training effect sizes for Chunk 8601 of 12458
Scan 10: estimating training effect sizes for Chunk 8801 of 12458
Scan 10: estimating training effect sizes for Chunk 9001 of 12458
Scan 10: estimating training effect sizes for Chunk 9201 of 12458
Scan 10: estimating training effect sizes for Chunk 9401 of 12458
Scan 10: estimating training effect sizes for Chunk 9601 of 12458
Scan 10: estimating training effect sizes for Chunk 9801 of 12458
Scan 10: estimating training effect sizes for Chunk 10001 of 12458
Scan 10: estimating training effect sizes for Chunk 10201 of 12458
Scan 10: estimating training effect sizes for Chunk 10401 of 12458
Scan 10: estimating training effect sizes for Chunk 10601 of 12458
Scan 10: estimating training effect sizes for Chunk 10801 of 12458
Scan 10: estimating training effect sizes for Chunk 11001 of 12458
Scan 10: estimating training effect sizes for Chunk 11201 of 12458
Scan 10: estimating training effect sizes for Chunk 11401 of 12458
Scan 10: estimating training effect sizes for Chunk 11601 of 12458
Scan 10: estimating training effect sizes for Chunk 11801 of 12458
Scan 10: estimating training effect sizes for Chunk 12001 of 12458
Scan 10: estimating training effect sizes for Chunk 12201 of 12458
Scan 10: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.00

The revised estimate of heritability is 0.0100

Measuring accuracy of each model
Model 1: heritability 0.0100, p 0.0000, f2 1.0000, mean squared error 0.9929
Model 2: heritability 0.0100, p 0.5000, f2 0.5000, mean squared error 0.9929
Model 3: heritability 0.0100, p 0.5000, f2 0.3000, mean squared error 0.9950
Model 4: heritability 0.0100, p 0.5000, f2 0.1000, mean squared error 1.0019
Model 5: heritability 0.0100, p 0.1000, f2 0.5000, mean squared error 0.9960
Model 6: heritability 0.0100, p 0.1000, f2 0.3000, mean squared error 0.9992
Model 7: heritability 0.0100, p 0.1000, f2 0.1000, mean squared error 1.0059
Model 8: heritability 0.0100, p 0.0100, f2 0.5000, mean squared error 0.9955
Model 9: heritability 0.0100, p 0.0100, f2 0.3000, mean squared error 0.9902
Model 10: heritability 0.0100, p 0.0100, f2 0.1000, mean squared error 0.9925

Time check: have so far spent 0.14 hours

Constructing final PRS (heritability 0.0100, p 0.0100, f2 0.3000) using all samples

Scan 1: estimating final effect sizes for Chunk 1 of 12458
Scan 1: estimating final effect sizes for Chunk 201 of 12458
Scan 1: estimating final effect sizes for Chunk 401 of 12458
Scan 1: estimating final effect sizes for Chunk 601 of 12458
Scan 1: estimating final effect sizes for Chunk 801 of 12458
Scan 1: estimating final effect sizes for Chunk 1001 of 12458
Scan 1: estimating final effect sizes for Chunk 1201 of 12458
Scan 1: estimating final effect sizes for Chunk 1401 of 12458
Scan 1: estimating final effect sizes for Chunk 1601 of 12458
Scan 1: estimating final effect sizes for Chunk 1801 of 12458
Scan 1: estimating final effect sizes for Chunk 2001 of 12458
Scan 1: estimating final effect sizes for Chunk 2201 of 12458
Scan 1: estimating final effect sizes for Chunk 2401 of 12458
Scan 1: estimating final effect sizes for Chunk 2601 of 12458
Scan 1: estimating final effect sizes for Chunk 2801 of 12458
Scan 1: estimating final effect sizes for Chunk 3001 of 12458
Scan 1: estimating final effect sizes for Chunk 3201 of 12458
Scan 1: estimating final effect sizes for Chunk 3401 of 12458
Scan 1: estimating final effect sizes for Chunk 3601 of 12458
Scan 1: estimating final effect sizes for Chunk 3801 of 12458
Scan 1: estimating final effect sizes for Chunk 4001 of 12458
Scan 1: estimating final effect sizes for Chunk 4201 of 12458
Scan 1: estimating final effect sizes for Chunk 4401 of 12458
Scan 1: estimating final effect sizes for Chunk 4601 of 12458
Scan 1: estimating final effect sizes for Chunk 4801 of 12458
Scan 1: estimating final effect sizes for Chunk 5001 of 12458
Scan 1: estimating final effect sizes for Chunk 5201 of 12458
Scan 1: estimating final effect sizes for Chunk 5401 of 12458
Scan 1: estimating final effect sizes for Chunk 5601 of 12458
Scan 1: estimating final effect sizes for Chunk 5801 of 12458
Scan 1: estimating final effect sizes for Chunk 6001 of 12458
Scan 1: estimating final effect sizes for Chunk 6201 of 12458
Scan 1: estimating final effect sizes for Chunk 6401 of 12458
Scan 1: estimating final effect sizes for Chunk 6601 of 12458
Scan 1: estimating final effect sizes for Chunk 6801 of 12458
Scan 1: estimating final effect sizes for Chunk 7001 of 12458
Scan 1: estimating final effect sizes for Chunk 7201 of 12458
Scan 1: estimating final effect sizes for Chunk 7401 of 12458
Scan 1: estimating final effect sizes for Chunk 7601 of 12458
Scan 1: estimating final effect sizes for Chunk 7801 of 12458
Scan 1: estimating final effect sizes for Chunk 8001 of 12458
Scan 1: estimating final effect sizes for Chunk 8201 of 12458
Scan 1: estimating final effect sizes for Chunk 8401 of 12458
Scan 1: estimating final effect sizes for Chunk 8601 of 12458
Scan 1: estimating final effect sizes for Chunk 8801 of 12458
Scan 1: estimating final effect sizes for Chunk 9001 of 12458
Scan 1: estimating final effect sizes for Chunk 9201 of 12458
Scan 1: estimating final effect sizes for Chunk 9401 of 12458
Scan 1: estimating final effect sizes for Chunk 9601 of 12458
Scan 1: estimating final effect sizes for Chunk 9801 of 12458
Scan 1: estimating final effect sizes for Chunk 10001 of 12458
Scan 1: estimating final effect sizes for Chunk 10201 of 12458
Scan 1: estimating final effect sizes for Chunk 10401 of 12458
Scan 1: estimating final effect sizes for Chunk 10601 of 12458
Scan 1: estimating final effect sizes for Chunk 10801 of 12458
Scan 1: estimating final effect sizes for Chunk 11001 of 12458
Scan 1: estimating final effect sizes for Chunk 11201 of 12458
Scan 1: estimating final effect sizes for Chunk 11401 of 12458
Scan 1: estimating final effect sizes for Chunk 11601 of 12458
Scan 1: estimating final effect sizes for Chunk 11801 of 12458
Scan 1: estimating final effect sizes for Chunk 12001 of 12458
Scan 1: estimating final effect sizes for Chunk 12201 of 12458
Scan 1: estimating final effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.00

Scan 2: estimating final effect sizes for Chunk 1 of 12458
Scan 2: estimating final effect sizes for Chunk 201 of 12458
Scan 2: estimating final effect sizes for Chunk 401 of 12458
Scan 2: estimating final effect sizes for Chunk 601 of 12458
Scan 2: estimating final effect sizes for Chunk 801 of 12458
Scan 2: estimating final effect sizes for Chunk 1001 of 12458
Scan 2: estimating final effect sizes for Chunk 1201 of 12458
Scan 2: estimating final effect sizes for Chunk 1401 of 12458
Scan 2: estimating final effect sizes for Chunk 1601 of 12458
Scan 2: estimating final effect sizes for Chunk 1801 of 12458
Scan 2: estimating final effect sizes for Chunk 2001 of 12458
Scan 2: estimating final effect sizes for Chunk 2201 of 12458
Scan 2: estimating final effect sizes for Chunk 2401 of 12458
Scan 2: estimating final effect sizes for Chunk 2601 of 12458
Scan 2: estimating final effect sizes for Chunk 2801 of 12458
Scan 2: estimating final effect sizes for Chunk 3001 of 12458
Scan 2: estimating final effect sizes for Chunk 3201 of 12458
Scan 2: estimating final effect sizes for Chunk 3401 of 12458
Scan 2: estimating final effect sizes for Chunk 3601 of 12458
Scan 2: estimating final effect sizes for Chunk 3801 of 12458
Scan 2: estimating final effect sizes for Chunk 4001 of 12458
Scan 2: estimating final effect sizes for Chunk 4201 of 12458
Scan 2: estimating final effect sizes for Chunk 4401 of 12458
Scan 2: estimating final effect sizes for Chunk 4601 of 12458
Scan 2: estimating final effect sizes for Chunk 4801 of 12458
Scan 2: estimating final effect sizes for Chunk 5001 of 12458
Scan 2: estimating final effect sizes for Chunk 5201 of 12458
Scan 2: estimating final effect sizes for Chunk 5401 of 12458
Scan 2: estimating final effect sizes for Chunk 5601 of 12458
Scan 2: estimating final effect sizes for Chunk 5801 of 12458
Scan 2: estimating final effect sizes for Chunk 6001 of 12458
Scan 2: estimating final effect sizes for Chunk 6201 of 12458
Scan 2: estimating final effect sizes for Chunk 6401 of 12458
Scan 2: estimating final effect sizes for Chunk 6601 of 12458
Scan 2: estimating final effect sizes for Chunk 6801 of 12458
Scan 2: estimating final effect sizes for Chunk 7001 of 12458
Scan 2: estimating final effect sizes for Chunk 7201 of 12458
Scan 2: estimating final effect sizes for Chunk 7401 of 12458
Scan 2: estimating final effect sizes for Chunk 7601 of 12458
Scan 2: estimating final effect sizes for Chunk 7801 of 12458
Scan 2: estimating final effect sizes for Chunk 8001 of 12458
Scan 2: estimating final effect sizes for Chunk 8201 of 12458
Scan 2: estimating final effect sizes for Chunk 8401 of 12458
Scan 2: estimating final effect sizes for Chunk 8601 of 12458
Scan 2: estimating final effect sizes for Chunk 8801 of 12458
Scan 2: estimating final effect sizes for Chunk 9001 of 12458
Scan 2: estimating final effect sizes for Chunk 9201 of 12458
Scan 2: estimating final effect sizes for Chunk 9401 of 12458
Scan 2: estimating final effect sizes for Chunk 9601 of 12458
Scan 2: estimating final effect sizes for Chunk 9801 of 12458
Scan 2: estimating final effect sizes for Chunk 10001 of 12458
Scan 2: estimating final effect sizes for Chunk 10201 of 12458
Scan 2: estimating final effect sizes for Chunk 10401 of 12458
Scan 2: estimating final effect sizes for Chunk 10601 of 12458
Scan 2: estimating final effect sizes for Chunk 10801 of 12458
Scan 2: estimating final effect sizes for Chunk 11001 of 12458
Scan 2: estimating final effect sizes for Chunk 11201 of 12458
Scan 2: estimating final effect sizes for Chunk 11401 of 12458
Scan 2: estimating final effect sizes for Chunk 11601 of 12458
Scan 2: estimating final effect sizes for Chunk 11801 of 12458
Scan 2: estimating final effect sizes for Chunk 12001 of 12458
Scan 2: estimating final effect sizes for Chunk 12201 of 12458
Scan 2: estimating final effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 1.00

Best-fitting model saved in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic.effects, with posterior probabilities in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic.probs

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:46:42 2025 and ended at Mon May 12 20:55:24 2025
The elapsed time was 0.14 hours
Given the command used one thread, this means the CPU time was also 0.14 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic.effects
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for one profile

Please note that ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic.effects is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 3189219 predictors from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic.effects

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 3200
Calculating scores for Chunk 51 of 3200
Calculating scores for Chunk 101 of 3200
Calculating scores for Chunk 151 of 3200
Calculating scores for Chunk 201 of 3200
Calculating scores for Chunk 251 of 3200
Calculating scores for Chunk 301 of 3200
Calculating scores for Chunk 351 of 3200
Calculating scores for Chunk 401 of 3200
Calculating scores for Chunk 451 of 3200
Calculating scores for Chunk 501 of 3200
Calculating scores for Chunk 551 of 3200
Calculating scores for Chunk 601 of 3200
Calculating scores for Chunk 651 of 3200
Calculating scores for Chunk 701 of 3200
Calculating scores for Chunk 751 of 3200
Calculating scores for Chunk 801 of 3200
Calculating scores for Chunk 851 of 3200
Calculating scores for Chunk 901 of 3200
Calculating scores for Chunk 951 of 3200
Calculating scores for Chunk 1001 of 3200
Calculating scores for Chunk 1051 of 3200
Calculating scores for Chunk 1101 of 3200
Calculating scores for Chunk 1151 of 3200
Calculating scores for Chunk 1201 of 3200
Calculating scores for Chunk 1251 of 3200
Calculating scores for Chunk 1301 of 3200
Calculating scores for Chunk 1351 of 3200
Calculating scores for Chunk 1401 of 3200
Calculating scores for Chunk 1451 of 3200
Calculating scores for Chunk 1501 of 3200
Calculating scores for Chunk 1551 of 3200
Calculating scores for Chunk 1601 of 3200
Calculating scores for Chunk 1651 of 3200
Calculating scores for Chunk 1701 of 3200
Calculating scores for Chunk 1751 of 3200
Calculating scores for Chunk 1801 of 3200
Calculating scores for Chunk 1851 of 3200
Calculating scores for Chunk 1901 of 3200
Calculating scores for Chunk 1951 of 3200
Calculating scores for Chunk 2001 of 3200
Calculating scores for Chunk 2051 of 3200
Calculating scores for Chunk 2101 of 3200
Calculating scores for Chunk 2151 of 3200
Calculating scores for Chunk 2201 of 3200
Calculating scores for Chunk 2251 of 3200
Calculating scores for Chunk 2301 of 3200
Calculating scores for Chunk 2351 of 3200
Calculating scores for Chunk 2401 of 3200
Calculating scores for Chunk 2451 of 3200
Calculating scores for Chunk 2501 of 3200
Calculating scores for Chunk 2551 of 3200
Calculating scores for Chunk 2601 of 3200
Calculating scores for Chunk 2651 of 3200
Calculating scores for Chunk 2701 of 3200
Calculating scores for Chunk 2751 of 3200
Calculating scores for Chunk 2801 of 3200
Calculating scores for Chunk 2851 of 3200
Calculating scores for Chunk 2901 of 3200
Calculating scores for Chunk 2951 of 3200
Calculating scores for Chunk 3001 of 3200
Calculating scores for Chunk 3051 of 3200
Calculating scores for Chunk 3101 of 3200
Calculating scores for Chunk 3151 of 3200

Profile saved in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic_prs_calc_with_pheno.profile

Correlation between score and phenotype is 0.8657, saved in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:55:24 2025 and ended at Mon May 12 20:55:43 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic.effects
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for one profile

Please note that ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic.effects is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 3189219 predictors from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic.effects

Calculating scores for Chunk 1 of 3200
Calculating scores for Chunk 51 of 3200
Calculating scores for Chunk 101 of 3200
Calculating scores for Chunk 151 of 3200
Calculating scores for Chunk 201 of 3200
Calculating scores for Chunk 251 of 3200
Calculating scores for Chunk 301 of 3200
Calculating scores for Chunk 351 of 3200
Calculating scores for Chunk 401 of 3200
Calculating scores for Chunk 451 of 3200
Calculating scores for Chunk 501 of 3200
Calculating scores for Chunk 551 of 3200
Calculating scores for Chunk 601 of 3200
Calculating scores for Chunk 651 of 3200
Calculating scores for Chunk 701 of 3200
Calculating scores for Chunk 751 of 3200
Calculating scores for Chunk 801 of 3200
Calculating scores for Chunk 851 of 3200
Calculating scores for Chunk 901 of 3200
Calculating scores for Chunk 951 of 3200
Calculating scores for Chunk 1001 of 3200
Calculating scores for Chunk 1051 of 3200
Calculating scores for Chunk 1101 of 3200
Calculating scores for Chunk 1151 of 3200
Calculating scores for Chunk 1201 of 3200
Calculating scores for Chunk 1251 of 3200
Calculating scores for Chunk 1301 of 3200
Calculating scores for Chunk 1351 of 3200
Calculating scores for Chunk 1401 of 3200
Calculating scores for Chunk 1451 of 3200
Calculating scores for Chunk 1501 of 3200
Calculating scores for Chunk 1551 of 3200
Calculating scores for Chunk 1601 of 3200
Calculating scores for Chunk 1651 of 3200
Calculating scores for Chunk 1701 of 3200
Calculating scores for Chunk 1751 of 3200
Calculating scores for Chunk 1801 of 3200
Calculating scores for Chunk 1851 of 3200
Calculating scores for Chunk 1901 of 3200
Calculating scores for Chunk 1951 of 3200
Calculating scores for Chunk 2001 of 3200
Calculating scores for Chunk 2051 of 3200
Calculating scores for Chunk 2101 of 3200
Calculating scores for Chunk 2151 of 3200
Calculating scores for Chunk 2201 of 3200
Calculating scores for Chunk 2251 of 3200
Calculating scores for Chunk 2301 of 3200
Calculating scores for Chunk 2351 of 3200
Calculating scores for Chunk 2401 of 3200
Calculating scores for Chunk 2451 of 3200
Calculating scores for Chunk 2501 of 3200
Calculating scores for Chunk 2551 of 3200
Calculating scores for Chunk 2601 of 3200
Calculating scores for Chunk 2651 of 3200
Calculating scores for Chunk 2701 of 3200
Calculating scores for Chunk 2751 of 3200
Calculating scores for Chunk 2801 of 3200
Calculating scores for Chunk 2851 of 3200
Calculating scores for Chunk 2901 of 3200
Calculating scores for Chunk 2951 of 3200
Calculating scores for Chunk 3001 of 3200
Calculating scores for Chunk 3051 of 3200
Calculating scores for Chunk 3101 of 3200
Calculating scores for Chunk 3151 of 3200

Profile saved in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_elastic_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:55:43 2025 and ended at Mon May 12 20:56:03 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## check the PRS is the same ##


###### calculate a PRS using the classical approach ######

## calculate linear association between SNPs and the phenotype ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--linear ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 4 covariates for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.combined")

Performing linear regression for Chunk 1 of 3200
Performing linear regression for Chunk 11 of 3200
Performing linear regression for Chunk 21 of 3200
Performing linear regression for Chunk 31 of 3200
Performing linear regression for Chunk 41 of 3200
Performing linear regression for Chunk 51 of 3200
Performing linear regression for Chunk 61 of 3200
Performing linear regression for Chunk 71 of 3200
Performing linear regression for Chunk 81 of 3200
Performing linear regression for Chunk 91 of 3200
Performing linear regression for Chunk 101 of 3200
Performing linear regression for Chunk 111 of 3200
Performing linear regression for Chunk 121 of 3200
Performing linear regression for Chunk 131 of 3200
Performing linear regression for Chunk 141 of 3200
Performing linear regression for Chunk 151 of 3200
Performing linear regression for Chunk 161 of 3200
Performing linear regression for Chunk 171 of 3200
Performing linear regression for Chunk 181 of 3200
Performing linear regression for Chunk 191 of 3200
Performing linear regression for Chunk 201 of 3200
Performing linear regression for Chunk 211 of 3200
Performing linear regression for Chunk 221 of 3200
Performing linear regression for Chunk 231 of 3200
Performing linear regression for Chunk 241 of 3200
Performing linear regression for Chunk 251 of 3200
Performing linear regression for Chunk 261 of 3200
Performing linear regression for Chunk 271 of 3200
Performing linear regression for Chunk 281 of 3200
Performing linear regression for Chunk 291 of 3200
Performing linear regression for Chunk 301 of 3200
Performing linear regression for Chunk 311 of 3200
Performing linear regression for Chunk 321 of 3200
Performing linear regression for Chunk 331 of 3200
Performing linear regression for Chunk 341 of 3200
Performing linear regression for Chunk 351 of 3200
Performing linear regression for Chunk 361 of 3200
Performing linear regression for Chunk 371 of 3200
Performing linear regression for Chunk 381 of 3200
Performing linear regression for Chunk 391 of 3200
Performing linear regression for Chunk 401 of 3200
Performing linear regression for Chunk 411 of 3200
Performing linear regression for Chunk 421 of 3200
Performing linear regression for Chunk 431 of 3200
Performing linear regression for Chunk 441 of 3200
Performing linear regression for Chunk 451 of 3200
Performing linear regression for Chunk 461 of 3200
Performing linear regression for Chunk 471 of 3200
Performing linear regression for Chunk 481 of 3200
Performing linear regression for Chunk 491 of 3200
Performing linear regression for Chunk 501 of 3200
Performing linear regression for Chunk 511 of 3200
Performing linear regression for Chunk 521 of 3200
Performing linear regression for Chunk 531 of 3200
Performing linear regression for Chunk 541 of 3200
Performing linear regression for Chunk 551 of 3200
Performing linear regression for Chunk 561 of 3200
Performing linear regression for Chunk 571 of 3200
Performing linear regression for Chunk 581 of 3200
Performing linear regression for Chunk 591 of 3200
Performing linear regression for Chunk 601 of 3200
Performing linear regression for Chunk 611 of 3200
Performing linear regression for Chunk 621 of 3200
Performing linear regression for Chunk 631 of 3200
Performing linear regression for Chunk 641 of 3200
Performing linear regression for Chunk 651 of 3200
Performing linear regression for Chunk 661 of 3200
Performing linear regression for Chunk 671 of 3200
Performing linear regression for Chunk 681 of 3200
Performing linear regression for Chunk 691 of 3200
Performing linear regression for Chunk 701 of 3200
Performing linear regression for Chunk 711 of 3200
Performing linear regression for Chunk 721 of 3200
Performing linear regression for Chunk 731 of 3200
Performing linear regression for Chunk 741 of 3200
Performing linear regression for Chunk 751 of 3200
Performing linear regression for Chunk 761 of 3200
Performing linear regression for Chunk 771 of 3200
Performing linear regression for Chunk 781 of 3200
Performing linear regression for Chunk 791 of 3200
Performing linear regression for Chunk 801 of 3200
Performing linear regression for Chunk 811 of 3200
Performing linear regression for Chunk 821 of 3200
Performing linear regression for Chunk 831 of 3200
Performing linear regression for Chunk 841 of 3200
Performing linear regression for Chunk 851 of 3200
Performing linear regression for Chunk 861 of 3200
Performing linear regression for Chunk 871 of 3200
Performing linear regression for Chunk 881 of 3200
Performing linear regression for Chunk 891 of 3200
Performing linear regression for Chunk 901 of 3200
Performing linear regression for Chunk 911 of 3200
Performing linear regression for Chunk 921 of 3200
Performing linear regression for Chunk 931 of 3200
Performing linear regression for Chunk 941 of 3200
Performing linear regression for Chunk 951 of 3200
Performing linear regression for Chunk 961 of 3200
Performing linear regression for Chunk 971 of 3200
Performing linear regression for Chunk 981 of 3200
Performing linear regression for Chunk 991 of 3200
Performing linear regression for Chunk 1001 of 3200
Performing linear regression for Chunk 1011 of 3200
Performing linear regression for Chunk 1021 of 3200
Performing linear regression for Chunk 1031 of 3200
Performing linear regression for Chunk 1041 of 3200
Performing linear regression for Chunk 1051 of 3200
Performing linear regression for Chunk 1061 of 3200
Performing linear regression for Chunk 1071 of 3200
Performing linear regression for Chunk 1081 of 3200
Performing linear regression for Chunk 1091 of 3200
Performing linear regression for Chunk 1101 of 3200
Performing linear regression for Chunk 1111 of 3200
Performing linear regression for Chunk 1121 of 3200
Performing linear regression for Chunk 1131 of 3200
Performing linear regression for Chunk 1141 of 3200
Performing linear regression for Chunk 1151 of 3200
Performing linear regression for Chunk 1161 of 3200
Performing linear regression for Chunk 1171 of 3200
Performing linear regression for Chunk 1181 of 3200
Performing linear regression for Chunk 1191 of 3200
Performing linear regression for Chunk 1201 of 3200
Performing linear regression for Chunk 1211 of 3200
Performing linear regression for Chunk 1221 of 3200
Performing linear regression for Chunk 1231 of 3200
Performing linear regression for Chunk 1241 of 3200
Performing linear regression for Chunk 1251 of 3200
Performing linear regression for Chunk 1261 of 3200
Performing linear regression for Chunk 1271 of 3200
Performing linear regression for Chunk 1281 of 3200
Performing linear regression for Chunk 1291 of 3200
Performing linear regression for Chunk 1301 of 3200
Performing linear regression for Chunk 1311 of 3200
Performing linear regression for Chunk 1321 of 3200
Performing linear regression for Chunk 1331 of 3200
Performing linear regression for Chunk 1341 of 3200
Performing linear regression for Chunk 1351 of 3200
Performing linear regression for Chunk 1361 of 3200
Performing linear regression for Chunk 1371 of 3200
Performing linear regression for Chunk 1381 of 3200
Performing linear regression for Chunk 1391 of 3200
Performing linear regression for Chunk 1401 of 3200
Performing linear regression for Chunk 1411 of 3200
Performing linear regression for Chunk 1421 of 3200
Performing linear regression for Chunk 1431 of 3200
Performing linear regression for Chunk 1441 of 3200
Performing linear regression for Chunk 1451 of 3200
Performing linear regression for Chunk 1461 of 3200
Performing linear regression for Chunk 1471 of 3200
Performing linear regression for Chunk 1481 of 3200
Performing linear regression for Chunk 1491 of 3200
Performing linear regression for Chunk 1501 of 3200
Performing linear regression for Chunk 1511 of 3200
Performing linear regression for Chunk 1521 of 3200
Performing linear regression for Chunk 1531 of 3200
Performing linear regression for Chunk 1541 of 3200
Performing linear regression for Chunk 1551 of 3200
Performing linear regression for Chunk 1561 of 3200
Performing linear regression for Chunk 1571 of 3200
Performing linear regression for Chunk 1581 of 3200
Performing linear regression for Chunk 1591 of 3200
Performing linear regression for Chunk 1601 of 3200
Performing linear regression for Chunk 1611 of 3200
Performing linear regression for Chunk 1621 of 3200
Performing linear regression for Chunk 1631 of 3200
Performing linear regression for Chunk 1641 of 3200
Performing linear regression for Chunk 1651 of 3200
Performing linear regression for Chunk 1661 of 3200
Performing linear regression for Chunk 1671 of 3200
Performing linear regression for Chunk 1681 of 3200
Performing linear regression for Chunk 1691 of 3200
Performing linear regression for Chunk 1701 of 3200
Performing linear regression for Chunk 1711 of 3200
Performing linear regression for Chunk 1721 of 3200
Performing linear regression for Chunk 1731 of 3200
Performing linear regression for Chunk 1741 of 3200
Performing linear regression for Chunk 1751 of 3200
Performing linear regression for Chunk 1761 of 3200
Performing linear regression for Chunk 1771 of 3200
Performing linear regression for Chunk 1781 of 3200
Performing linear regression for Chunk 1791 of 3200
Performing linear regression for Chunk 1801 of 3200
Performing linear regression for Chunk 1811 of 3200
Performing linear regression for Chunk 1821 of 3200
Performing linear regression for Chunk 1831 of 3200
Performing linear regression for Chunk 1841 of 3200
Performing linear regression for Chunk 1851 of 3200
Performing linear regression for Chunk 1861 of 3200
Performing linear regression for Chunk 1871 of 3200
Performing linear regression for Chunk 1881 of 3200
Performing linear regression for Chunk 1891 of 3200
Performing linear regression for Chunk 1901 of 3200
Performing linear regression for Chunk 1911 of 3200
Performing linear regression for Chunk 1921 of 3200
Performing linear regression for Chunk 1931 of 3200
Performing linear regression for Chunk 1941 of 3200
Performing linear regression for Chunk 1951 of 3200
Performing linear regression for Chunk 1961 of 3200
Performing linear regression for Chunk 1971 of 3200
Performing linear regression for Chunk 1981 of 3200
Performing linear regression for Chunk 1991 of 3200
Performing linear regression for Chunk 2001 of 3200
Performing linear regression for Chunk 2011 of 3200
Performing linear regression for Chunk 2021 of 3200
Performing linear regression for Chunk 2031 of 3200
Performing linear regression for Chunk 2041 of 3200
Performing linear regression for Chunk 2051 of 3200
Performing linear regression for Chunk 2061 of 3200
Performing linear regression for Chunk 2071 of 3200
Performing linear regression for Chunk 2081 of 3200
Performing linear regression for Chunk 2091 of 3200
Performing linear regression for Chunk 2101 of 3200
Performing linear regression for Chunk 2111 of 3200
Performing linear regression for Chunk 2121 of 3200
Performing linear regression for Chunk 2131 of 3200
Performing linear regression for Chunk 2141 of 3200
Performing linear regression for Chunk 2151 of 3200
Performing linear regression for Chunk 2161 of 3200
Performing linear regression for Chunk 2171 of 3200
Performing linear regression for Chunk 2181 of 3200
Performing linear regression for Chunk 2191 of 3200
Performing linear regression for Chunk 2201 of 3200
Performing linear regression for Chunk 2211 of 3200
Performing linear regression for Chunk 2221 of 3200
Performing linear regression for Chunk 2231 of 3200
Performing linear regression for Chunk 2241 of 3200
Performing linear regression for Chunk 2251 of 3200
Performing linear regression for Chunk 2261 of 3200
Performing linear regression for Chunk 2271 of 3200
Performing linear regression for Chunk 2281 of 3200
Performing linear regression for Chunk 2291 of 3200
Performing linear regression for Chunk 2301 of 3200
Performing linear regression for Chunk 2311 of 3200
Performing linear regression for Chunk 2321 of 3200
Performing linear regression for Chunk 2331 of 3200
Performing linear regression for Chunk 2341 of 3200
Performing linear regression for Chunk 2351 of 3200
Performing linear regression for Chunk 2361 of 3200
Performing linear regression for Chunk 2371 of 3200
Performing linear regression for Chunk 2381 of 3200
Performing linear regression for Chunk 2391 of 3200
Performing linear regression for Chunk 2401 of 3200
Performing linear regression for Chunk 2411 of 3200
Performing linear regression for Chunk 2421 of 3200
Performing linear regression for Chunk 2431 of 3200
Performing linear regression for Chunk 2441 of 3200
Performing linear regression for Chunk 2451 of 3200
Performing linear regression for Chunk 2461 of 3200
Performing linear regression for Chunk 2471 of 3200
Performing linear regression for Chunk 2481 of 3200
Performing linear regression for Chunk 2491 of 3200
Performing linear regression for Chunk 2501 of 3200
Performing linear regression for Chunk 2511 of 3200
Performing linear regression for Chunk 2521 of 3200
Performing linear regression for Chunk 2531 of 3200
Performing linear regression for Chunk 2541 of 3200
Performing linear regression for Chunk 2551 of 3200
Performing linear regression for Chunk 2561 of 3200
Performing linear regression for Chunk 2571 of 3200
Performing linear regression for Chunk 2581 of 3200
Performing linear regression for Chunk 2591 of 3200
Performing linear regression for Chunk 2601 of 3200
Performing linear regression for Chunk 2611 of 3200
Performing linear regression for Chunk 2621 of 3200
Performing linear regression for Chunk 2631 of 3200
Performing linear regression for Chunk 2641 of 3200
Performing linear regression for Chunk 2651 of 3200
Performing linear regression for Chunk 2661 of 3200
Performing linear regression for Chunk 2671 of 3200
Performing linear regression for Chunk 2681 of 3200
Performing linear regression for Chunk 2691 of 3200
Performing linear regression for Chunk 2701 of 3200
Performing linear regression for Chunk 2711 of 3200
Performing linear regression for Chunk 2721 of 3200
Performing linear regression for Chunk 2731 of 3200
Performing linear regression for Chunk 2741 of 3200
Performing linear regression for Chunk 2751 of 3200
Performing linear regression for Chunk 2761 of 3200
Performing linear regression for Chunk 2771 of 3200
Performing linear regression for Chunk 2781 of 3200
Performing linear regression for Chunk 2791 of 3200
Performing linear regression for Chunk 2801 of 3200
Performing linear regression for Chunk 2811 of 3200
Performing linear regression for Chunk 2821 of 3200
Performing linear regression for Chunk 2831 of 3200
Performing linear regression for Chunk 2841 of 3200
Performing linear regression for Chunk 2851 of 3200
Performing linear regression for Chunk 2861 of 3200
Performing linear regression for Chunk 2871 of 3200
Performing linear regression for Chunk 2881 of 3200
Performing linear regression for Chunk 2891 of 3200
Performing linear regression for Chunk 2901 of 3200
Performing linear regression for Chunk 2911 of 3200
Performing linear regression for Chunk 2921 of 3200
Performing linear regression for Chunk 2931 of 3200
Performing linear regression for Chunk 2941 of 3200
Performing linear regression for Chunk 2951 of 3200
Performing linear regression for Chunk 2961 of 3200
Performing linear regression for Chunk 2971 of 3200
Performing linear regression for Chunk 2981 of 3200
Performing linear regression for Chunk 2991 of 3200
Performing linear regression for Chunk 3001 of 3200
Performing linear regression for Chunk 3011 of 3200
Performing linear regression for Chunk 3021 of 3200
Performing linear regression for Chunk 3031 of 3200
Performing linear regression for Chunk 3041 of 3200
Performing linear regression for Chunk 3051 of 3200
Performing linear regression for Chunk 3061 of 3200
Performing linear regression for Chunk 3071 of 3200
Performing linear regression for Chunk 3081 of 3200
Performing linear regression for Chunk 3091 of 3200
Performing linear regression for Chunk 3101 of 3200
Performing linear regression for Chunk 3111 of 3200
Performing linear regression for Chunk 3121 of 3200
Performing linear regression for Chunk 3131 of 3200
Performing linear regression for Chunk 3141 of 3200
Performing linear regression for Chunk 3151 of 3200
Performing linear regression for Chunk 3161 of 3200
Performing linear regression for Chunk 3171 of 3200
Performing linear regression for Chunk 3181 of 3200
Performing linear regression for Chunk 3191 of 3200

Main results saved in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.assoc, with a summary version in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.summaries, p-values in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues and score file in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:56:03 2025 and ended at Mon May 12 20:56:20 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## define p-values cut-offs for thresholding ##

# load the p-values #

# get the minimum p-value #

# make a list of thresholds that are above the minimum p-value #

## obtain scores in a reduced set of SNPs after thresholding and cumpling ##

# perform thresholding considering the threshold 1 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_small_set_predictors_set_linear_clump_thresholding_1_predictors
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 1

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e+00, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

3189219 of the 3189219 predictors in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e+00
All of these are in the data

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading p-values from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr1_858952_G_A 7.3421e-01 | chr1_905373_T_C 6.4949e-01 | chr1_911428_C_T 1.5342e-01

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 25 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 127569; stay tuned for updates ;)
Pass 1: Thinning for Chunk 2001 of 127569; kept 5349 out of 50000 predictors (10.70%)
Pass 1: Thinning for Chunk 4001 of 127569; kept 10136 out of 100000 predictors (10.14%)
Pass 1: Thinning for Chunk 6001 of 127569; kept 15020 out of 150000 predictors (10.01%)
Pass 1: Thinning for Chunk 8001 of 127569; kept 19794 out of 200000 predictors (9.90%)
Pass 1: Thinning for Chunk 10001 of 127569; kept 24362 out of 250000 predictors (9.74%)
Pass 1: Thinning for Chunk 12001 of 127569; kept 28870 out of 300000 predictors (9.62%)
Pass 1: Thinning for Chunk 14001 of 127569; kept 33428 out of 350000 predictors (9.55%)
Pass 1: Thinning for Chunk 16001 of 127569; kept 38039 out of 400000 predictors (9.51%)
Pass 1: Thinning for Chunk 18001 of 127569; kept 42841 out of 450000 predictors (9.52%)
Pass 1: Thinning for Chunk 20001 of 127569; kept 48242 out of 500000 predictors (9.65%)
Pass 1: Thinning for Chunk 22001 of 127569; kept 52767 out of 550000 predictors (9.59%)
Pass 1: Thinning for Chunk 24001 of 127569; kept 57504 out of 600000 predictors (9.58%)
Pass 1: Thinning for Chunk 26001 of 127569; kept 61943 out of 650000 predictors (9.53%)
Pass 1: Thinning for Chunk 28001 of 127569; kept 66711 out of 700000 predictors (9.53%)
Pass 1: Thinning for Chunk 30001 of 127569; kept 71454 out of 750000 predictors (9.53%)
Pass 1: Thinning for Chunk 32001 of 127569; kept 76041 out of 800000 predictors (9.51%)
Pass 1: Thinning for Chunk 34001 of 127569; kept 80232 out of 850000 predictors (9.44%)
Pass 1: Thinning for Chunk 36001 of 127569; kept 84557 out of 900000 predictors (9.40%)
Pass 1: Thinning for Chunk 38001 of 127569; kept 88986 out of 950000 predictors (9.37%)
Pass 1: Thinning for Chunk 40001 of 127569; kept 93330 out of 1000000 predictors (9.33%)
Pass 1: Thinning for Chunk 42001 of 127569; kept 97951 out of 1050000 predictors (9.33%)
Pass 1: Thinning for Chunk 44001 of 127569; kept 102872 out of 1100000 predictors (9.35%)
Pass 1: Thinning for Chunk 46001 of 127569; kept 107212 out of 1150000 predictors (9.32%)
Pass 1: Thinning for Chunk 48001 of 127569; kept 112244 out of 1200000 predictors (9.35%)
Pass 1: Thinning for Chunk 50001 of 127569; kept 116743 out of 1250000 predictors (9.34%)
Pass 1: Thinning for Chunk 52001 of 127569; kept 120562 out of 1300000 predictors (9.27%)
Pass 1: Thinning for Chunk 54001 of 127569; kept 124884 out of 1350000 predictors (9.25%)
Pass 1: Thinning for Chunk 56001 of 127569; kept 129799 out of 1400000 predictors (9.27%)
Pass 1: Thinning for Chunk 58001 of 127569; kept 134057 out of 1450000 predictors (9.25%)
Pass 1: Thinning for Chunk 60001 of 127569; kept 138229 out of 1500000 predictors (9.22%)
Pass 1: Thinning for Chunk 62001 of 127569; kept 142252 out of 1550000 predictors (9.18%)
Pass 1: Thinning for Chunk 64001 of 127569; kept 147204 out of 1600000 predictors (9.20%)
Pass 1: Thinning for Chunk 66001 of 127569; kept 151677 out of 1650000 predictors (9.19%)
Pass 1: Thinning for Chunk 68001 of 127569; kept 155435 out of 1700000 predictors (9.14%)
Pass 1: Thinning for Chunk 70001 of 127569; kept 159941 out of 1750000 predictors (9.14%)
Pass 1: Thinning for Chunk 72001 of 127569; kept 164623 out of 1800000 predictors (9.15%)
Pass 1: Thinning for Chunk 74001 of 127569; kept 169049 out of 1850000 predictors (9.14%)
Pass 1: Thinning for Chunk 76001 of 127569; kept 173835 out of 1900000 predictors (9.15%)
Pass 1: Thinning for Chunk 78001 of 127569; kept 178721 out of 1950000 predictors (9.17%)
Pass 1: Thinning for Chunk 80001 of 127569; kept 183417 out of 2000000 predictors (9.17%)
Pass 1: Thinning for Chunk 82001 of 127569; kept 187490 out of 2050000 predictors (9.15%)
Pass 1: Thinning for Chunk 84001 of 127569; kept 192287 out of 2100000 predictors (9.16%)
Pass 1: Thinning for Chunk 86001 of 127569; kept 196676 out of 2150000 predictors (9.15%)
Pass 1: Thinning for Chunk 88001 of 127569; kept 201191 out of 2200000 predictors (9.15%)
Pass 1: Thinning for Chunk 90001 of 127569; kept 205945 out of 2250000 predictors (9.15%)
Pass 1: Thinning for Chunk 92001 of 127569; kept 210596 out of 2300000 predictors (9.16%)
Pass 1: Thinning for Chunk 94001 of 127569; kept 214955 out of 2350000 predictors (9.15%)
Pass 1: Thinning for Chunk 96001 of 127569; kept 219609 out of 2400000 predictors (9.15%)
Pass 1: Thinning for Chunk 98001 of 127569; kept 224693 out of 2450000 predictors (9.17%)
Pass 1: Thinning for Chunk 100001 of 127569; kept 229516 out of 2500000 predictors (9.18%)
Pass 1: Thinning for Chunk 102001 of 127569; kept 233905 out of 2550000 predictors (9.17%)
Pass 1: Thinning for Chunk 104001 of 127569; kept 238708 out of 2600000 predictors (9.18%)
Pass 1: Thinning for Chunk 106001 of 127569; kept 243035 out of 2650000 predictors (9.17%)
Pass 1: Thinning for Chunk 108001 of 127569; kept 248119 out of 2700000 predictors (9.19%)
Pass 1: Thinning for Chunk 110001 of 127569; kept 253379 out of 2750000 predictors (9.21%)
Pass 1: Thinning for Chunk 112001 of 127569; kept 258154 out of 2800000 predictors (9.22%)
Pass 1: Thinning for Chunk 114001 of 127569; kept 262988 out of 2850000 predictors (9.23%)
Pass 1: Thinning for Chunk 116001 of 127569; kept 268448 out of 2900000 predictors (9.26%)
Pass 1: Thinning for Chunk 118001 of 127569; kept 274063 out of 2950000 predictors (9.29%)
Pass 1: Thinning for Chunk 120001 of 127569; kept 278819 out of 3000000 predictors (9.29%)
Pass 1: Thinning for Chunk 122001 of 127569; kept 284236 out of 3050000 predictors (9.32%)
Pass 1: Thinning for Chunk 124001 of 127569; kept 289376 out of 3100000 predictors (9.33%)
Pass 1: Thinning for Chunk 126001 of 127569; kept 294426 out of 3150000 predictors (9.35%)

The bit-size has now been set to 124

Pass 2: Thinning for Chunk 1 of 2408; stay tuned for updates ;)
Pass 2: Thinning for Chunk 401 of 2408; kept 15562 out of 49600 predictors (31.38%)
Pass 2: Thinning for Chunk 801 of 2408; kept 31681 out of 99200 predictors (31.94%)
Pass 2: Thinning for Chunk 1201 of 2408; kept 46786 out of 148800 predictors (31.44%)
Pass 2: Thinning for Chunk 1601 of 2408; kept 64456 out of 198400 predictors (32.49%)
Pass 2: Thinning for Chunk 2001 of 2408; kept 80944 out of 248000 predictors (32.64%)
Pass 2: Thinning for Chunk 2401 of 2408; kept 101985 out of 297600 predictors (34.27%)

Thinning complete: 102539 predictors kept (saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_small_set_predictors_set_linear_clump_thresholding_1_predictors.in), 3086680 lost (./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_small_set_predictors_set_linear_clump_thresholding_1_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_small_set_predictors_set_linear_clump_thresholding_1_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:56:21 2025 and ended at Mon May 12 20:57:13 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_small_set_predictors_set_linear_clump_thresholding_1_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 102539 predictors to extract from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_small_set_predictors_set_linear_clump_thresholding_1_predictors.in

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 102539)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 4 covariates for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1.combined")

Performing linear regression for Chunk 1 of 115
Performing linear regression for Chunk 11 of 115
Performing linear regression for Chunk 21 of 115
Performing linear regression for Chunk 31 of 115
Performing linear regression for Chunk 41 of 115
Performing linear regression for Chunk 51 of 115
Performing linear regression for Chunk 61 of 115
Performing linear regression for Chunk 71 of 115
Performing linear regression for Chunk 81 of 115
Performing linear regression for Chunk 91 of 115
Performing linear regression for Chunk 101 of 115
Performing linear regression for Chunk 111 of 115

Main results saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1.assoc, with a summary version in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1.summaries, p-values in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1.pvalues and score file in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:57:13 2025 and ended at Mon May 12 20:57:19 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1.score
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 102539 predictors from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1.score

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 115
Calculating scores for Chunk 51 of 115
Calculating scores for Chunk 101 of 115

Profiles saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.3007 to 0.8763, saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:57:19 2025 and ended at Mon May 12 20:57:24 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1.score
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 102539 predictors from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1.score

Calculating scores for Chunk 1 of 115
Calculating scores for Chunk 51 of 115
Calculating scores for Chunk 101 of 115

Profiles saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1/beep_change_linear_clump_thresholding_1_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:57:24 2025 and ended at Mon May 12 20:57:29 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


# perform thresholding considering the threshold 0.1 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 0.1

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e-01, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

327106 of the 3189219 predictors in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e-01
All of these are in the data

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 327106)

Reading p-values from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr1_962184_T_C 3.5539e-02 | chr1_984039_T_C 3.8654e-02 | chr1_1094994_G_A 3.9304e-02

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 20 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 16356; stay tuned for updates ;)
Pass 1: Thinning for Chunk 2501 of 16356; kept 8240 out of 50000 predictors (16.48%)
Pass 1: Thinning for Chunk 5001 of 16356; kept 16030 out of 100000 predictors (16.03%)
Pass 1: Thinning for Chunk 7501 of 16356; kept 24219 out of 150000 predictors (16.15%)
Pass 1: Thinning for Chunk 10001 of 16356; kept 32265 out of 200000 predictors (16.13%)
Pass 1: Thinning for Chunk 12501 of 16356; kept 40206 out of 250000 predictors (16.08%)
Pass 1: Thinning for Chunk 15001 of 16356; kept 48769 out of 300000 predictors (16.26%)

The bit-size has now been set to 25

Pass 2: Thinning for Chunk 1 of 2144; stay tuned for updates ;)
Pass 2: Thinning for Chunk 2001 of 2144; kept 22837 out of 50000 predictors (45.67%)

Thinning complete: 24862 predictors kept (saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors.in), 302244 lost (./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:57:29 2025 and ended at Mon May 12 20:57:57 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 24862 predictors to extract from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors.in

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 24862)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 4 covariates for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1.combined")

Performing linear regression for Chunk 1 of 35
Performing linear regression for Chunk 11 of 35
Performing linear regression for Chunk 21 of 35
Performing linear regression for Chunk 31 of 35

Main results saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1.assoc, with a summary version in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1.summaries, p-values in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1.pvalues and score file in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:57:57 2025 and ended at Mon May 12 20:58:01 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1.score
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 24862 predictors from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1.score

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 35

Profiles saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.3007 to 0.8750, saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:58:01 2025 and ended at Mon May 12 20:58:06 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1.score
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 24862 predictors from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1.score

Calculating scores for Chunk 1 of 35

Profiles saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.1/beep_change_linear_clump_thresholding_0.1_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:58:06 2025 and ended at Mon May 12 20:58:11 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


# perform thresholding considering the threshold 0.01 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 0.01

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e-02, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

33074 of the 3189219 predictors in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e-02
All of these are in the data

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 33074)

Reading p-values from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr1_3417031_G_A 7.2980e-03 | chr1_3972702_C_T 9.6218e-03 | chr1_4061687_C_A 1.3533e-04

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 20 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 1654; stay tuned for updates ;)

The bit-size has now been set to 20

Pass 2: Thinning for Chunk 1 of 334; stay tuned for updates ;)

Thinning complete: 3704 predictors kept (saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors.in), 29370 lost (./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:58:11 2025 and ended at Mon May 12 20:58:38 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 3704 predictors to extract from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors.in

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3704)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 4 covariates for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01.combined")

Performing linear regression for Chunk 1 of 22
Performing linear regression for Chunk 11 of 22
Performing linear regression for Chunk 21 of 22

Main results saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01.assoc, with a summary version in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01.summaries, p-values in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01.pvalues and score file in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:58:38 2025 and ended at Mon May 12 20:58:42 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01.score
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 3704 predictors from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01.score

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 22

Profiles saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.3007 to 0.8677, saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:58:42 2025 and ended at Mon May 12 20:58:47 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01.score
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 3704 predictors from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01.score

Calculating scores for Chunk 1 of 22

Profiles saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.01/beep_change_linear_clump_thresholding_0.01_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:58:47 2025 and ended at Mon May 12 20:58:51 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


# perform thresholding considering the threshold 0.001 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 0.001

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e-03, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

3254 of the 3189219 predictors in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e-03
All of these are in the data

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3254)

Reading p-values from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr1_4061687_C_A 1.3533e-04 | chr1_7786246_C_G 5.3093e-04 | chr1_7786247_T_A 5.3093e-04

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 20 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 163; stay tuned for updates ;)

The bit-size has now been set to 20

Pass 2: Thinning for Chunk 1 of 40; stay tuned for updates ;)

Thinning complete: 489 predictors kept (saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors.in), 2765 lost (./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:58:51 2025 and ended at Mon May 12 20:59:17 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 489 predictors to extract from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors.in

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 489)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 4 covariates for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001.combined")

Performing linear regression for Chunk 1 of 22
Performing linear regression for Chunk 11 of 22
Performing linear regression for Chunk 21 of 22

Main results saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001.assoc, with a summary version in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001.summaries, p-values in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001.pvalues and score file in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:59:17 2025 and ended at Mon May 12 20:59:21 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001.score
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 489 predictors from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001.score

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 22

Profiles saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.3007 to 0.8179, saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:59:21 2025 and ended at Mon May 12 20:59:25 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001.score
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 489 predictors from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001.score

Calculating scores for Chunk 1 of 22

Profiles saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.001/beep_change_linear_clump_thresholding_0.001_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:59:25 2025 and ended at Mon May 12 20:59:29 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


# perform thresholding considering the threshold 0.0001 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 0.0001

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e-04, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

217 of the 3189219 predictors in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e-04
All of these are in the data

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 217)

Reading p-values from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr1_68384728_T_A 6.0154e-05 | chr1_68384919_G_C 6.0154e-05 | chr1_68385124_T_C 6.0154e-05

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 20 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 11; stay tuned for updates ;)

The bit-size has now been set to 20

Pass 2: Thinning for Chunk 1 of 4; stay tuned for updates ;)

Thinning complete: 38 predictors kept (saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors.in), 179 lost (./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:59:29 2025 and ended at Mon May 12 20:59:54 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 38 predictors to extract from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors.in

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 38)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 4 covariates for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001.combined")

Performing linear regression for Chunk 1 of 17
Performing linear regression for Chunk 11 of 17

Main results saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001.assoc, with a summary version in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001.summaries, p-values in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001.pvalues and score file in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:59:54 2025 and ended at Mon May 12 20:59:58 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001.score
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 38 predictors from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001.score

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 17

Profiles saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.3007 to 0.5566, saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:59:58 2025 and ended at Mon May 12 21:00:02 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001.score
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 38 predictors from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001.score

Calculating scores for Chunk 1 of 17

Profiles saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_0.0001/beep_change_linear_clump_thresholding_0.0001_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 21:00:02 2025 and ended at Mon May 12 21:00:06 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


# perform thresholding considering the threshold 1e-05 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 1e-05

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e-05, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

28 of the 3189219 predictors in ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e-05
All of these are in the data

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 28)

Reading p-values from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr3_187336176_T_C 1.6773e-06 | chr3_187336418_T_C 1.9805e-06 | chr3_187336496_T_C 2.9909e-06

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 20 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 2; stay tuned for updates ;)

The bit-size has now been set to 20

Pass 2: Thinning for Chunk 1 of 1; stay tuned for updates ;)

Thinning complete: 6 predictors kept (saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors.in), 22 lost (./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 21:00:06 2025 and ended at Mon May 12 21:00:29 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 6 predictors to extract from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors.in

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 6)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 4 covariates for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05.combined")

Performing linear regression for Chunk 1 of 6

Main results saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05.assoc, with a summary version in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05.summaries, p-values in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05.pvalues and score file in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 21:00:29 2025 and ended at Mon May 12 21:00:32 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05.score
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 6 predictors from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05.score

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/beep_change/beep_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 6

Profiles saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.3007 to 0.3007, saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 21:00:32 2025 and ended at Mon May 12 21:00:36 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05.score
--bfile ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/beep_change_filesets/beep_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 6 predictors from ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05.score

Calculating scores for Chunk 1 of 6

Profiles saved in ./results/final_results/analysis_full_data/beep_change/clump_thresholding_1e-05/beep_change_linear_clump_thresholding_1e-05_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 21:00:36 2025 and ended at Mon May 12 21:00:40 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


###### check we have used the correct samples in both analyses ######

## load the FAM files used for training and test ##

## samples in files generated by elastic net ##

## samples in files generated by linear ##

###### plot quantiles of PRS against phenotype ######

## prepare folders ##


## load the phenotype data before transformation ##

## process it ##

# get only the samples finally included in modelling #

# split the ID into FID and IID #

# check that the new variables has been correctly create #

## open the plot ##

# Create a figure with 8 subplots arranged in a grid (e.g., 2 rows x 4 columns) #

## Flatten the axes array for easier iteration ##

## create a list of models ##

## iterate across models ##

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          7           -0.37750             0.40              1.1550
1          3           -1.07500             0.01              0.9500
2          1           -2.90000            -0.99              0.3750
3         10           -0.21750             0.70              1.5750
4          4           -0.97500             0.10              1.0250
5         20            1.30000             2.60              4.9250
6         16            0.45000             1.40              2.0000
7         12            0.32500             1.10              1.7750
8         15            0.40000             1.20              1.7750
9         19            1.00000             2.00              3.0500
10        17            0.67500             1.50              2.4000
11         6           -0.45000             0.20              0.8000
12         2           -1.37250             0.00              0.6750
13         5           -0.70000             0.20              0.9750
14        18            1.12500             1.70              3.0250
15        11            0.20000             0.90              1.5750
16        13            0.07500             0.90              1.7000
17         9            0.00000             0.90              1.6000
18         8           -0.30000             0.50              1.4000
19        14            0.45975             1.10              2.2875

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          8             0.0000              0.6              1.3000
1          3            -1.2675              0.0              0.6750
2          1            -2.9000             -1.0              0.3750
3         11             0.1925              1.0              1.6750
4          5            -0.5500              0.3              1.1000
5         19             1.0000              2.0              3.3825
6         14             0.6000              1.1              1.9000
7          9             0.1000              0.8              1.7750
8         12             0.0000              1.0              1.6750
9         16             0.6500              1.4              2.1500
10        17             0.6750              1.5              2.1750
11        10            -0.1500              0.9              1.6000
12         4            -0.8750              0.1              0.9500
13         6            -0.7250              0.3              0.8000
14         7            -0.6875              0.4              0.8000
15        20             1.5000              2.6              4.9250
16        18             0.9500              1.8              2.7500
17        13             0.3000              1.0              1.5750
18         2            -1.5500             -0.1              0.6000
19        15             0.7000              1.3              1.9000

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          8            -0.2750              0.5               1.300
1          4            -1.2750              0.1               0.800
2          3            -1.0000              0.1               0.675
3          1            -2.9000             -1.0               0.375
4         11             0.2250              1.0               1.600
5          5            -0.5500              0.3               0.975
6         19             1.0500              2.0               2.675
7         14             0.5450              1.1               1.900
8          9             0.1000              0.7               1.400
9         12             0.0000              1.0               1.700
10        16             0.6250              1.4               2.150
11        17             0.6750              1.5               2.350
12        10            -0.1500              0.8               1.750
13         7            -0.7325              0.4               0.800
14         6            -0.8000              0.3               1.075
15        18             1.0250              1.7               2.550
16        20             1.3750              2.6               4.925
17        13             0.3500              1.1               1.600
18         2            -1.5000              0.0               0.675
19        15             0.7000              1.2               1.900

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          9             0.0075             0.70              1.8000
1          3            -1.2000             0.00              0.8750
2          4            -1.0750             0.00              0.8000
3          1            -2.9000            -0.99              0.3750
4         12             0.1250             1.00              1.6000
5          5            -0.8750             0.20              1.0000
6         19             1.1250             2.00              3.4075
7         15             0.6250             1.20              2.3750
8         16             0.8250             1.30              2.1750
9         18             0.9000             1.80              2.5750
10        10             0.0000             0.70              1.6000
11        17             0.8250             1.50              2.1500
12         7            -0.3000             0.40              1.3100
13         2            -1.5500            -0.10              0.6750
14        20             1.3500             2.60              4.9250
15        13             0.2175             1.10              1.7000
16        11             0.1250             1.00              1.7000
17         8            -0.3000             0.60              1.4500
18         6            -0.7750             0.30              1.1750
19        14             0.3225             1.06              1.7000

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0         10             0.2000             0.90              1.8250
1          4            -1.4475             0.20              1.0750
2          3            -1.1750             0.00              0.8000
3          1            -2.9000            -0.60              0.3750
4         13             0.1750             1.02              1.7000
5          5            -1.0000             0.20              0.9500
6         19             1.0000             1.80              2.7500
7         15             0.4000             1.20              2.4000
8          7            -1.0775             0.35              1.2775
9         17             0.7250             1.50              2.8925
10         9             0.0000             0.70              1.7750
11        16             0.1500             1.50              2.2750
12        11            -0.0750             0.80              1.5000
13        12             0.1225             0.92              1.8750
14        14             0.3000             1.10              1.7000
15        20             1.2000             2.50              4.9250
16         2            -1.7500             0.00              0.8000
17         6            -0.9000             0.40              1.0750
18        18             0.7500             1.60              2.6250
19         8            -0.1000             0.60              1.5000

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0         10            -0.7500             0.70              2.0250
1          4            -1.3450             0.30              1.8975
2          1            -2.5750             0.10              1.0000
3          9            -0.8250             0.70              1.9500
4         14            -0.6100             1.15              2.7550
5         16             0.0000             1.30              3.0500
6         18             0.2750             1.40              3.6075
7         13            -0.8000             0.80              1.9500
8         15            -0.8250             1.10              2.1000
9         11            -0.2750             0.60              2.0000
10        17            -0.4675             1.10              2.6000
11         6            -1.1750             0.60              1.8500
12        19             0.4000             1.40              2.7500
13        20             0.8000             2.00              4.8500
14         8            -1.0500             0.80              2.0000
15        12            -0.4000             1.00              2.1500
16         2            -1.6250             0.30              1.7000
17         5            -1.3250             0.60              1.5750
18         7            -1.5550             0.55              2.1550
19         3            -1.1750             0.30              1.5500

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0         14           -0.56250             1.00             2.66250
1          6           -1.66000             0.70             2.16000
2          1           -1.97500             0.40             1.78750
3          7           -1.45250             0.80             1.90500
4         10           -0.99500             0.70             1.98500
5          9           -1.32000             0.70             2.76950
6         15           -0.18525             1.00             2.40000
7          4           -0.85500             0.70             2.20000
8         17           -0.06500             1.10             3.76500
9         19            0.00000             1.50             4.75000
10         3           -1.08250             0.80             2.20000
11         5           -0.93000             0.90             2.02000
12        16           -1.08000             0.90             2.08000
13         2           -2.16000             0.50             1.93500
14        12           -1.33500             0.65             3.40175
15        18           -0.55500             1.40             3.04000
16        13           -0.84500             1.00             2.80000
17        11           -1.00000             0.80             2.30000
18         8            0.12750             0.65             1.17250

# plot the results #

# finish the plot #

###### manhattan plots ######

## load assoc results to pandas ##
         Chromosome           Predictor  Basepair  ... CallRate MachR2  SPA_Status
0                 1     chr1_858952_G_A    858952  ...      1.0    NaN    NOT_USED
1                 1     chr1_905373_T_C    905373  ...      1.0    NaN    NOT_USED
2                 1     chr1_911428_C_T    911428  ...      1.0    NaN    NOT_USED
3                 1     chr1_918870_A_G    918870  ...      1.0    NaN    NOT_USED
4                 1     chr1_931513_T_C    931513  ...      1.0    NaN    NOT_USED
...             ...                 ...       ...  ...      ...    ...         ...
3189214          22  chr22_50749145_C_T  50749145  ...      1.0    NaN    NOT_USED
3189215          22  chr22_50749470_T_C  50749470  ...      1.0    NaN    NOT_USED
3189216          22  chr22_50749890_T_G  50749890  ...      1.0    NaN    NOT_USED
3189217          22  chr22_50751123_G_T  50751123  ...      1.0    NaN    NOT_USED
3189218          22  chr22_50752652_C_G  50752652  ...      1.0    NaN    NOT_USED

[3189219 rows x 13 columns]

## check we have the correct columns ##

## check we have the correct dtypes ##

## calculate -log_10(pvalue) ##

## convert chromosome to category and then sort by it ##

## sort rows by chromosome and basepair position ##

## Creates a new column called ind in the assoc_results DataFrame ##

## Groups the assoc_results DataFrame by the Chromosome column ##

## make manhattan plot ##

# open the plot #

# Add a title to the plot #

# Define a colorblind-friendly palette #

# iterate across chromosomes #

# set the x-axis ticks and labels of these ticks #

# add p-value thresholds as horizontal lines #

# set axis limits #

# set the axis label #

# Increase font size for tick labels #

# add a legend to the subplot #

# save the plot as a static image #

###### compress the results ######

## remove bed and bim plink files initialles used as input (compressed files already created) ##


## elastic outputs ##


## linear outputs ##

# first raw outputs before clumping #


# then outputs after clumping #







#######################################
#######################################
FINISH
#######################################
#######################################
