
#######################################
#######################################
checking function to print nicely: header 1
#######################################
#######################################

###### checking function to print nicely: header 2 ######

## checking function to print nicely: header 3 ##

# checking function to print nicely: header 4 #

#######################################
#######################################
check behaviour run_bash
#######################################
#######################################

###### see working directory ######
/home/UGR002/dsalazar/climahealth/combat_genes


###### list files/folders there ######
03a_association_analyses.sif
03b_prs_calculation_100_iter_large_set_predictors_beep_change.out
03b_prs_calculation_100_iter_large_set_predictors_distance_change.out
03b_prs_calculation_100_iter_large_set_predictors_vo2_change.out
03b_prs_calculation_100_iter_large_set_predictors_weight_change.out
03b_prs_calculation_100_iter_small_set_predictors_beep_change.out
03b_prs_calculation_100_iter_small_set_predictors_distance_change.out
03b_prs_calculation_100_iter_small_set_predictors_vo2_change.out
03b_prs_calculation_100_iter_small_set_predictors_weight_change.out
03d_processing_results.out
03e_manhattan_plots_beep_change.out
03e_manhattan_plots_distance_change.out
03e_manhattan_plots_vo2_change.out
data
results
scripts


#######################################
#######################################
For phenotype vo2_change, and the small dataset of covariates
#######################################
#######################################

###### initial preparations ######

## create folders for results ##


## load the phenotype data ##
                 family_id AGRF code  ...  Week 1 Pred VO2max  vo2_change
0     combat_ILGSA24-17303  0200ADMM  ...            0.269686   -0.326436
1     combat_ILGSA24-17303  0200ASJM  ...            1.433991   -2.454810
2     combat_ILGSA24-17303  0200BHNM  ...           -1.713535   -0.918671
3     combat_ILGSA24-17303  0200CBOM  ...           -0.922440   -0.708965
4     combat_ILGSA24-17303  0200CDFM  ...           -0.153565   -0.881675
...                    ...       ...  ...                 ...         ...
1013  combat_ILGSA24-17873  8098ATDN  ...            0.394904    0.424420
1014  combat_ILGSA24-17873  8098RSJN  ...           -0.234031   -1.025520
1015  combat_ILGSA24-17873  8098TSAN  ...            0.807651   -0.637772
1016  combat_ILGSA24-17873  8099AJNN  ...            0.269686    0.392239
1017  combat_ILGSA24-17873  8099COSN  ...            0.394904    0.148574

[1018 rows x 9 columns]

## specify the covariates ##
Index(['PCA5', 'PCA8', 'PCA13', 'sex_code', 'Week 1 Body Mass',
       'Week 1 Pred VO2max'],
      dtype='object')

## decompress bim and bed files with all sample generated after NA cleaning ##


###### prepare LDAK inputs ######

###### calculate elastic PRS ######

## run the PRS with --elastic ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--elastic ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--LOCO NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Constructing elastic net PRS

Will consider five values for the predictor scaling (alpha = -1, -0.75, -0.5, -0.25 and 0); to instead specify the value, use "--power" (or use "--powerfile" to provide a range of values)

Will use the default prior parameter choices (saved in the file ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic.parameters); to instead specify your own, use "--parameters"

Will select the best prior parameters via cross-validation, using 0.10 randomly-picked test samples (use "--cv-proportion" to change this proportion, "--cv-samples" to explicitly specify the test samples, or "--cv-skip" to turn off cross-validation)

Will always include the LOCO polygenic contribution, regardless of their estimated accuracy

When constructing PRS, will scan the data at most 10 times (change this using "--num-scans")

All heritability estimates must be between 0.01 and 0.8000 (change the upper bound using "--max-her")

Will use either three or ten random vectors for Monte Carlo operations (decided based on the number of samples); change this number using "--num-random-vectors"

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 5 covariates for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic.combined")

When performing cross-validation, will use 917 samples to train models and 101 to test their accuracy

Warning, to process the data requires 6.1 Gb; sorry this can not be reduced

Warning, to perform the analysis requires approximately 2.1 Gb; sorry, this can not be reduced

Estimating per-predictor heritabilities using Randomized Haseman-Elston Regression with 10 random vectors

Will divide the predictors into 20 partitions (change this using "--num-divides")
Will exclude chunks containing a predictor with estimated variance explained greater than 0.0982 (change this using "--max-cor")

Calculating traces for Chunk 1 of 12458
Calculating traces for Chunk 201 of 12458
Calculating traces for Chunk 401 of 12458
Calculating traces for Chunk 601 of 12458
Calculating traces for Chunk 801 of 12458
Calculating traces for Chunk 1001 of 12458
Calculating traces for Chunk 1201 of 12458
Calculating traces for Chunk 1401 of 12458
Calculating traces for Chunk 1601 of 12458
Calculating traces for Chunk 1801 of 12458
Calculating traces for Chunk 2001 of 12458
Calculating traces for Chunk 2201 of 12458
Calculating traces for Chunk 2401 of 12458
Calculating traces for Chunk 2601 of 12458
Calculating traces for Chunk 2801 of 12458
Calculating traces for Chunk 3001 of 12458
Calculating traces for Chunk 3201 of 12458
Calculating traces for Chunk 3401 of 12458
Calculating traces for Chunk 3601 of 12458
Calculating traces for Chunk 3801 of 12458
Calculating traces for Chunk 4001 of 12458
Calculating traces for Chunk 4201 of 12458
Calculating traces for Chunk 4401 of 12458
Calculating traces for Chunk 4601 of 12458
Calculating traces for Chunk 4801 of 12458
Calculating traces for Chunk 5001 of 12458
Calculating traces for Chunk 5201 of 12458
Calculating traces for Chunk 5401 of 12458
Calculating traces for Chunk 5601 of 12458
Calculating traces for Chunk 5801 of 12458
Calculating traces for Chunk 6001 of 12458
Calculating traces for Chunk 6201 of 12458
Calculating traces for Chunk 6401 of 12458
Calculating traces for Chunk 6601 of 12458
Calculating traces for Chunk 6801 of 12458
Calculating traces for Chunk 7001 of 12458
Calculating traces for Chunk 7201 of 12458
Calculating traces for Chunk 7401 of 12458
Calculating traces for Chunk 7601 of 12458
Calculating traces for Chunk 7801 of 12458
Calculating traces for Chunk 8001 of 12458
Calculating traces for Chunk 8201 of 12458
Calculating traces for Chunk 8401 of 12458
Calculating traces for Chunk 8601 of 12458
Calculating traces for Chunk 8801 of 12458
Calculating traces for Chunk 9001 of 12458
Calculating traces for Chunk 9201 of 12458
Calculating traces for Chunk 9401 of 12458
Calculating traces for Chunk 9601 of 12458
Calculating traces for Chunk 9801 of 12458
Calculating traces for Chunk 10001 of 12458
Calculating traces for Chunk 10201 of 12458
Calculating traces for Chunk 10401 of 12458
Calculating traces for Chunk 10601 of 12458
Calculating traces for Chunk 10801 of 12458
Calculating traces for Chunk 11001 of 12458
Calculating traces for Chunk 11201 of 12458
Calculating traces for Chunk 11401 of 12458
Calculating traces for Chunk 11601 of 12458
Calculating traces for Chunk 11801 of 12458
Calculating traces for Chunk 12001 of 12458
Calculating traces for Chunk 12201 of 12458
Calculating traces for Chunk 12401 of 12458

Best power is -1.0000 and estimated heritability is 0.5915

Time check: have so far spent 0.02 hours

Constructing 10 PRS using training samples
Will also make 33 MCMC REML models (using all samples)

Scan 1: estimating training effect sizes for Chunk 1 of 12458
Scan 1: estimating training effect sizes for Chunk 201 of 12458
Scan 1: estimating training effect sizes for Chunk 401 of 12458
Scan 1: estimating training effect sizes for Chunk 601 of 12458
Scan 1: estimating training effect sizes for Chunk 801 of 12458
Scan 1: estimating training effect sizes for Chunk 1001 of 12458
Scan 1: estimating training effect sizes for Chunk 1201 of 12458
Scan 1: estimating training effect sizes for Chunk 1401 of 12458
Scan 1: estimating training effect sizes for Chunk 1601 of 12458
Scan 1: estimating training effect sizes for Chunk 1801 of 12458
Scan 1: estimating training effect sizes for Chunk 2001 of 12458
Scan 1: estimating training effect sizes for Chunk 2201 of 12458
Scan 1: estimating training effect sizes for Chunk 2401 of 12458
Scan 1: estimating training effect sizes for Chunk 2601 of 12458
Scan 1: estimating training effect sizes for Chunk 2801 of 12458
Scan 1: estimating training effect sizes for Chunk 3001 of 12458
Scan 1: estimating training effect sizes for Chunk 3201 of 12458
Scan 1: estimating training effect sizes for Chunk 3401 of 12458
Scan 1: estimating training effect sizes for Chunk 3601 of 12458
Scan 1: estimating training effect sizes for Chunk 3801 of 12458
Scan 1: estimating training effect sizes for Chunk 4001 of 12458
Scan 1: estimating training effect sizes for Chunk 4201 of 12458
Scan 1: estimating training effect sizes for Chunk 4401 of 12458
Scan 1: estimating training effect sizes for Chunk 4601 of 12458
Scan 1: estimating training effect sizes for Chunk 4801 of 12458
Scan 1: estimating training effect sizes for Chunk 5001 of 12458
Scan 1: estimating training effect sizes for Chunk 5201 of 12458
Scan 1: estimating training effect sizes for Chunk 5401 of 12458
Scan 1: estimating training effect sizes for Chunk 5601 of 12458
Scan 1: estimating training effect sizes for Chunk 5801 of 12458
Scan 1: estimating training effect sizes for Chunk 6001 of 12458
Scan 1: estimating training effect sizes for Chunk 6201 of 12458
Scan 1: estimating training effect sizes for Chunk 6401 of 12458
Scan 1: estimating training effect sizes for Chunk 6601 of 12458
Scan 1: estimating training effect sizes for Chunk 6801 of 12458
Scan 1: estimating training effect sizes for Chunk 7001 of 12458
Scan 1: estimating training effect sizes for Chunk 7201 of 12458
Scan 1: estimating training effect sizes for Chunk 7401 of 12458
Scan 1: estimating training effect sizes for Chunk 7601 of 12458
Scan 1: estimating training effect sizes for Chunk 7801 of 12458
Scan 1: estimating training effect sizes for Chunk 8001 of 12458
Scan 1: estimating training effect sizes for Chunk 8201 of 12458
Scan 1: estimating training effect sizes for Chunk 8401 of 12458
Scan 1: estimating training effect sizes for Chunk 8601 of 12458
Scan 1: estimating training effect sizes for Chunk 8801 of 12458
Scan 1: estimating training effect sizes for Chunk 9001 of 12458
Scan 1: estimating training effect sizes for Chunk 9201 of 12458
Scan 1: estimating training effect sizes for Chunk 9401 of 12458
Scan 1: estimating training effect sizes for Chunk 9601 of 12458
Scan 1: estimating training effect sizes for Chunk 9801 of 12458
Scan 1: estimating training effect sizes for Chunk 10001 of 12458
Scan 1: estimating training effect sizes for Chunk 10201 of 12458
Scan 1: estimating training effect sizes for Chunk 10401 of 12458
Scan 1: estimating training effect sizes for Chunk 10601 of 12458
Scan 1: estimating training effect sizes for Chunk 10801 of 12458
Scan 1: estimating training effect sizes for Chunk 11001 of 12458
Scan 1: estimating training effect sizes for Chunk 11201 of 12458
Scan 1: estimating training effect sizes for Chunk 11401 of 12458
Scan 1: estimating training effect sizes for Chunk 11601 of 12458
Scan 1: estimating training effect sizes for Chunk 11801 of 12458
Scan 1: estimating training effect sizes for Chunk 12001 of 12458
Scan 1: estimating training effect sizes for Chunk 12201 of 12458
Scan 1: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.70

Scan 2: estimating training effect sizes for Chunk 1 of 12458
Scan 2: estimating training effect sizes for Chunk 201 of 12458
Scan 2: estimating training effect sizes for Chunk 401 of 12458
Scan 2: estimating training effect sizes for Chunk 601 of 12458
Scan 2: estimating training effect sizes for Chunk 801 of 12458
Scan 2: estimating training effect sizes for Chunk 1001 of 12458
Scan 2: estimating training effect sizes for Chunk 1201 of 12458
Scan 2: estimating training effect sizes for Chunk 1401 of 12458
Scan 2: estimating training effect sizes for Chunk 1601 of 12458
Scan 2: estimating training effect sizes for Chunk 1801 of 12458
Scan 2: estimating training effect sizes for Chunk 2001 of 12458
Scan 2: estimating training effect sizes for Chunk 2201 of 12458
Scan 2: estimating training effect sizes for Chunk 2401 of 12458
Scan 2: estimating training effect sizes for Chunk 2601 of 12458
Scan 2: estimating training effect sizes for Chunk 2801 of 12458
Scan 2: estimating training effect sizes for Chunk 3001 of 12458
Scan 2: estimating training effect sizes for Chunk 3201 of 12458
Scan 2: estimating training effect sizes for Chunk 3401 of 12458
Scan 2: estimating training effect sizes for Chunk 3601 of 12458
Scan 2: estimating training effect sizes for Chunk 3801 of 12458
Scan 2: estimating training effect sizes for Chunk 4001 of 12458
Scan 2: estimating training effect sizes for Chunk 4201 of 12458
Scan 2: estimating training effect sizes for Chunk 4401 of 12458
Scan 2: estimating training effect sizes for Chunk 4601 of 12458
Scan 2: estimating training effect sizes for Chunk 4801 of 12458
Scan 2: estimating training effect sizes for Chunk 5001 of 12458
Scan 2: estimating training effect sizes for Chunk 5201 of 12458
Scan 2: estimating training effect sizes for Chunk 5401 of 12458
Scan 2: estimating training effect sizes for Chunk 5601 of 12458
Scan 2: estimating training effect sizes for Chunk 5801 of 12458
Scan 2: estimating training effect sizes for Chunk 6001 of 12458
Scan 2: estimating training effect sizes for Chunk 6201 of 12458
Scan 2: estimating training effect sizes for Chunk 6401 of 12458
Scan 2: estimating training effect sizes for Chunk 6601 of 12458
Scan 2: estimating training effect sizes for Chunk 6801 of 12458
Scan 2: estimating training effect sizes for Chunk 7001 of 12458
Scan 2: estimating training effect sizes for Chunk 7201 of 12458
Scan 2: estimating training effect sizes for Chunk 7401 of 12458
Scan 2: estimating training effect sizes for Chunk 7601 of 12458
Scan 2: estimating training effect sizes for Chunk 7801 of 12458
Scan 2: estimating training effect sizes for Chunk 8001 of 12458
Scan 2: estimating training effect sizes for Chunk 8201 of 12458
Scan 2: estimating training effect sizes for Chunk 8401 of 12458
Scan 2: estimating training effect sizes for Chunk 8601 of 12458
Scan 2: estimating training effect sizes for Chunk 8801 of 12458
Scan 2: estimating training effect sizes for Chunk 9001 of 12458
Scan 2: estimating training effect sizes for Chunk 9201 of 12458
Scan 2: estimating training effect sizes for Chunk 9401 of 12458
Scan 2: estimating training effect sizes for Chunk 9601 of 12458
Scan 2: estimating training effect sizes for Chunk 9801 of 12458
Scan 2: estimating training effect sizes for Chunk 10001 of 12458
Scan 2: estimating training effect sizes for Chunk 10201 of 12458
Scan 2: estimating training effect sizes for Chunk 10401 of 12458
Scan 2: estimating training effect sizes for Chunk 10601 of 12458
Scan 2: estimating training effect sizes for Chunk 10801 of 12458
Scan 2: estimating training effect sizes for Chunk 11001 of 12458
Scan 2: estimating training effect sizes for Chunk 11201 of 12458
Scan 2: estimating training effect sizes for Chunk 11401 of 12458
Scan 2: estimating training effect sizes for Chunk 11601 of 12458
Scan 2: estimating training effect sizes for Chunk 11801 of 12458
Scan 2: estimating training effect sizes for Chunk 12001 of 12458
Scan 2: estimating training effect sizes for Chunk 12201 of 12458
Scan 2: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.25

Scan 3: estimating training effect sizes for Chunk 1 of 12458
Scan 3: estimating training effect sizes for Chunk 201 of 12458
Scan 3: estimating training effect sizes for Chunk 401 of 12458
Scan 3: estimating training effect sizes for Chunk 601 of 12458
Scan 3: estimating training effect sizes for Chunk 801 of 12458
Scan 3: estimating training effect sizes for Chunk 1001 of 12458
Scan 3: estimating training effect sizes for Chunk 1201 of 12458
Scan 3: estimating training effect sizes for Chunk 1401 of 12458
Scan 3: estimating training effect sizes for Chunk 1601 of 12458
Scan 3: estimating training effect sizes for Chunk 1801 of 12458
Scan 3: estimating training effect sizes for Chunk 2001 of 12458
Scan 3: estimating training effect sizes for Chunk 2201 of 12458
Scan 3: estimating training effect sizes for Chunk 2401 of 12458
Scan 3: estimating training effect sizes for Chunk 2601 of 12458
Scan 3: estimating training effect sizes for Chunk 2801 of 12458
Scan 3: estimating training effect sizes for Chunk 3001 of 12458
Scan 3: estimating training effect sizes for Chunk 3201 of 12458
Scan 3: estimating training effect sizes for Chunk 3401 of 12458
Scan 3: estimating training effect sizes for Chunk 3601 of 12458
Scan 3: estimating training effect sizes for Chunk 3801 of 12458
Scan 3: estimating training effect sizes for Chunk 4001 of 12458
Scan 3: estimating training effect sizes for Chunk 4201 of 12458
Scan 3: estimating training effect sizes for Chunk 4401 of 12458
Scan 3: estimating training effect sizes for Chunk 4601 of 12458
Scan 3: estimating training effect sizes for Chunk 4801 of 12458
Scan 3: estimating training effect sizes for Chunk 5001 of 12458
Scan 3: estimating training effect sizes for Chunk 5201 of 12458
Scan 3: estimating training effect sizes for Chunk 5401 of 12458
Scan 3: estimating training effect sizes for Chunk 5601 of 12458
Scan 3: estimating training effect sizes for Chunk 5801 of 12458
Scan 3: estimating training effect sizes for Chunk 6001 of 12458
Scan 3: estimating training effect sizes for Chunk 6201 of 12458
Scan 3: estimating training effect sizes for Chunk 6401 of 12458
Scan 3: estimating training effect sizes for Chunk 6601 of 12458
Scan 3: estimating training effect sizes for Chunk 6801 of 12458
Scan 3: estimating training effect sizes for Chunk 7001 of 12458
Scan 3: estimating training effect sizes for Chunk 7201 of 12458
Scan 3: estimating training effect sizes for Chunk 7401 of 12458
Scan 3: estimating training effect sizes for Chunk 7601 of 12458
Scan 3: estimating training effect sizes for Chunk 7801 of 12458
Scan 3: estimating training effect sizes for Chunk 8001 of 12458
Scan 3: estimating training effect sizes for Chunk 8201 of 12458
Scan 3: estimating training effect sizes for Chunk 8401 of 12458
Scan 3: estimating training effect sizes for Chunk 8601 of 12458
Scan 3: estimating training effect sizes for Chunk 8801 of 12458
Scan 3: estimating training effect sizes for Chunk 9001 of 12458
Scan 3: estimating training effect sizes for Chunk 9201 of 12458
Scan 3: estimating training effect sizes for Chunk 9401 of 12458
Scan 3: estimating training effect sizes for Chunk 9601 of 12458
Scan 3: estimating training effect sizes for Chunk 9801 of 12458
Scan 3: estimating training effect sizes for Chunk 10001 of 12458
Scan 3: estimating training effect sizes for Chunk 10201 of 12458
Scan 3: estimating training effect sizes for Chunk 10401 of 12458
Scan 3: estimating training effect sizes for Chunk 10601 of 12458
Scan 3: estimating training effect sizes for Chunk 10801 of 12458
Scan 3: estimating training effect sizes for Chunk 11001 of 12458
Scan 3: estimating training effect sizes for Chunk 11201 of 12458
Scan 3: estimating training effect sizes for Chunk 11401 of 12458
Scan 3: estimating training effect sizes for Chunk 11601 of 12458
Scan 3: estimating training effect sizes for Chunk 11801 of 12458
Scan 3: estimating training effect sizes for Chunk 12001 of 12458
Scan 3: estimating training effect sizes for Chunk 12201 of 12458
Scan 3: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.02

Scan 4: estimating training effect sizes for Chunk 1 of 12458
Scan 4: estimating training effect sizes for Chunk 201 of 12458
Scan 4: estimating training effect sizes for Chunk 401 of 12458
Scan 4: estimating training effect sizes for Chunk 601 of 12458
Scan 4: estimating training effect sizes for Chunk 801 of 12458
Scan 4: estimating training effect sizes for Chunk 1001 of 12458
Scan 4: estimating training effect sizes for Chunk 1201 of 12458
Scan 4: estimating training effect sizes for Chunk 1401 of 12458
Scan 4: estimating training effect sizes for Chunk 1601 of 12458
Scan 4: estimating training effect sizes for Chunk 1801 of 12458
Scan 4: estimating training effect sizes for Chunk 2001 of 12458
Scan 4: estimating training effect sizes for Chunk 2201 of 12458
Scan 4: estimating training effect sizes for Chunk 2401 of 12458
Scan 4: estimating training effect sizes for Chunk 2601 of 12458
Scan 4: estimating training effect sizes for Chunk 2801 of 12458
Scan 4: estimating training effect sizes for Chunk 3001 of 12458
Scan 4: estimating training effect sizes for Chunk 3201 of 12458
Scan 4: estimating training effect sizes for Chunk 3401 of 12458
Scan 4: estimating training effect sizes for Chunk 3601 of 12458
Scan 4: estimating training effect sizes for Chunk 3801 of 12458
Scan 4: estimating training effect sizes for Chunk 4001 of 12458
Scan 4: estimating training effect sizes for Chunk 4201 of 12458
Scan 4: estimating training effect sizes for Chunk 4401 of 12458
Scan 4: estimating training effect sizes for Chunk 4601 of 12458
Scan 4: estimating training effect sizes for Chunk 4801 of 12458
Scan 4: estimating training effect sizes for Chunk 5001 of 12458
Scan 4: estimating training effect sizes for Chunk 5201 of 12458
Scan 4: estimating training effect sizes for Chunk 5401 of 12458
Scan 4: estimating training effect sizes for Chunk 5601 of 12458
Scan 4: estimating training effect sizes for Chunk 5801 of 12458
Scan 4: estimating training effect sizes for Chunk 6001 of 12458
Scan 4: estimating training effect sizes for Chunk 6201 of 12458
Scan 4: estimating training effect sizes for Chunk 6401 of 12458
Scan 4: estimating training effect sizes for Chunk 6601 of 12458
Scan 4: estimating training effect sizes for Chunk 6801 of 12458
Scan 4: estimating training effect sizes for Chunk 7001 of 12458
Scan 4: estimating training effect sizes for Chunk 7201 of 12458
Scan 4: estimating training effect sizes for Chunk 7401 of 12458
Scan 4: estimating training effect sizes for Chunk 7601 of 12458
Scan 4: estimating training effect sizes for Chunk 7801 of 12458
Scan 4: estimating training effect sizes for Chunk 8001 of 12458
Scan 4: estimating training effect sizes for Chunk 8201 of 12458
Scan 4: estimating training effect sizes for Chunk 8401 of 12458
Scan 4: estimating training effect sizes for Chunk 8601 of 12458
Scan 4: estimating training effect sizes for Chunk 8801 of 12458
Scan 4: estimating training effect sizes for Chunk 9001 of 12458
Scan 4: estimating training effect sizes for Chunk 9201 of 12458
Scan 4: estimating training effect sizes for Chunk 9401 of 12458
Scan 4: estimating training effect sizes for Chunk 9601 of 12458
Scan 4: estimating training effect sizes for Chunk 9801 of 12458
Scan 4: estimating training effect sizes for Chunk 10001 of 12458
Scan 4: estimating training effect sizes for Chunk 10201 of 12458
Scan 4: estimating training effect sizes for Chunk 10401 of 12458
Scan 4: estimating training effect sizes for Chunk 10601 of 12458
Scan 4: estimating training effect sizes for Chunk 10801 of 12458
Scan 4: estimating training effect sizes for Chunk 11001 of 12458
Scan 4: estimating training effect sizes for Chunk 11201 of 12458
Scan 4: estimating training effect sizes for Chunk 11401 of 12458
Scan 4: estimating training effect sizes for Chunk 11601 of 12458
Scan 4: estimating training effect sizes for Chunk 11801 of 12458
Scan 4: estimating training effect sizes for Chunk 12001 of 12458
Scan 4: estimating training effect sizes for Chunk 12201 of 12458
Scan 4: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.00

Scan 5: estimating training effect sizes for Chunk 1 of 12458
Scan 5: estimating training effect sizes for Chunk 201 of 12458
Scan 5: estimating training effect sizes for Chunk 401 of 12458
Scan 5: estimating training effect sizes for Chunk 601 of 12458
Scan 5: estimating training effect sizes for Chunk 801 of 12458
Scan 5: estimating training effect sizes for Chunk 1001 of 12458
Scan 5: estimating training effect sizes for Chunk 1201 of 12458
Scan 5: estimating training effect sizes for Chunk 1401 of 12458
Scan 5: estimating training effect sizes for Chunk 1601 of 12458
Scan 5: estimating training effect sizes for Chunk 1801 of 12458
Scan 5: estimating training effect sizes for Chunk 2001 of 12458
Scan 5: estimating training effect sizes for Chunk 2201 of 12458
Scan 5: estimating training effect sizes for Chunk 2401 of 12458
Scan 5: estimating training effect sizes for Chunk 2601 of 12458
Scan 5: estimating training effect sizes for Chunk 2801 of 12458
Scan 5: estimating training effect sizes for Chunk 3001 of 12458
Scan 5: estimating training effect sizes for Chunk 3201 of 12458
Scan 5: estimating training effect sizes for Chunk 3401 of 12458
Scan 5: estimating training effect sizes for Chunk 3601 of 12458
Scan 5: estimating training effect sizes for Chunk 3801 of 12458
Scan 5: estimating training effect sizes for Chunk 4001 of 12458
Scan 5: estimating training effect sizes for Chunk 4201 of 12458
Scan 5: estimating training effect sizes for Chunk 4401 of 12458
Scan 5: estimating training effect sizes for Chunk 4601 of 12458
Scan 5: estimating training effect sizes for Chunk 4801 of 12458
Scan 5: estimating training effect sizes for Chunk 5001 of 12458
Scan 5: estimating training effect sizes for Chunk 5201 of 12458
Scan 5: estimating training effect sizes for Chunk 5401 of 12458
Scan 5: estimating training effect sizes for Chunk 5601 of 12458
Scan 5: estimating training effect sizes for Chunk 5801 of 12458
Scan 5: estimating training effect sizes for Chunk 6001 of 12458
Scan 5: estimating training effect sizes for Chunk 6201 of 12458
Scan 5: estimating training effect sizes for Chunk 6401 of 12458
Scan 5: estimating training effect sizes for Chunk 6601 of 12458
Scan 5: estimating training effect sizes for Chunk 6801 of 12458
Scan 5: estimating training effect sizes for Chunk 7001 of 12458
Scan 5: estimating training effect sizes for Chunk 7201 of 12458
Scan 5: estimating training effect sizes for Chunk 7401 of 12458
Scan 5: estimating training effect sizes for Chunk 7601 of 12458
Scan 5: estimating training effect sizes for Chunk 7801 of 12458
Scan 5: estimating training effect sizes for Chunk 8001 of 12458
Scan 5: estimating training effect sizes for Chunk 8201 of 12458
Scan 5: estimating training effect sizes for Chunk 8401 of 12458
Scan 5: estimating training effect sizes for Chunk 8601 of 12458
Scan 5: estimating training effect sizes for Chunk 8801 of 12458
Scan 5: estimating training effect sizes for Chunk 9001 of 12458
Scan 5: estimating training effect sizes for Chunk 9201 of 12458
Scan 5: estimating training effect sizes for Chunk 9401 of 12458
Scan 5: estimating training effect sizes for Chunk 9601 of 12458
Scan 5: estimating training effect sizes for Chunk 9801 of 12458
Scan 5: estimating training effect sizes for Chunk 10001 of 12458
Scan 5: estimating training effect sizes for Chunk 10201 of 12458
Scan 5: estimating training effect sizes for Chunk 10401 of 12458
Scan 5: estimating training effect sizes for Chunk 10601 of 12458
Scan 5: estimating training effect sizes for Chunk 10801 of 12458
Scan 5: estimating training effect sizes for Chunk 11001 of 12458
Scan 5: estimating training effect sizes for Chunk 11201 of 12458
Scan 5: estimating training effect sizes for Chunk 11401 of 12458
Scan 5: estimating training effect sizes for Chunk 11601 of 12458
Scan 5: estimating training effect sizes for Chunk 11801 of 12458
Scan 5: estimating training effect sizes for Chunk 12001 of 12458
Scan 5: estimating training effect sizes for Chunk 12201 of 12458
Scan 5: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.00

Scan 6: estimating training effect sizes for Chunk 1 of 12458
Scan 6: estimating training effect sizes for Chunk 201 of 12458
Scan 6: estimating training effect sizes for Chunk 401 of 12458
Scan 6: estimating training effect sizes for Chunk 601 of 12458
Scan 6: estimating training effect sizes for Chunk 801 of 12458
Scan 6: estimating training effect sizes for Chunk 1001 of 12458
Scan 6: estimating training effect sizes for Chunk 1201 of 12458
Scan 6: estimating training effect sizes for Chunk 1401 of 12458
Scan 6: estimating training effect sizes for Chunk 1601 of 12458
Scan 6: estimating training effect sizes for Chunk 1801 of 12458
Scan 6: estimating training effect sizes for Chunk 2001 of 12458
Scan 6: estimating training effect sizes for Chunk 2201 of 12458
Scan 6: estimating training effect sizes for Chunk 2401 of 12458
Scan 6: estimating training effect sizes for Chunk 2601 of 12458
Scan 6: estimating training effect sizes for Chunk 2801 of 12458
Scan 6: estimating training effect sizes for Chunk 3001 of 12458
Scan 6: estimating training effect sizes for Chunk 3201 of 12458
Scan 6: estimating training effect sizes for Chunk 3401 of 12458
Scan 6: estimating training effect sizes for Chunk 3601 of 12458
Scan 6: estimating training effect sizes for Chunk 3801 of 12458
Scan 6: estimating training effect sizes for Chunk 4001 of 12458
Scan 6: estimating training effect sizes for Chunk 4201 of 12458
Scan 6: estimating training effect sizes for Chunk 4401 of 12458
Scan 6: estimating training effect sizes for Chunk 4601 of 12458
Scan 6: estimating training effect sizes for Chunk 4801 of 12458
Scan 6: estimating training effect sizes for Chunk 5001 of 12458
Scan 6: estimating training effect sizes for Chunk 5201 of 12458
Scan 6: estimating training effect sizes for Chunk 5401 of 12458
Scan 6: estimating training effect sizes for Chunk 5601 of 12458
Scan 6: estimating training effect sizes for Chunk 5801 of 12458
Scan 6: estimating training effect sizes for Chunk 6001 of 12458
Scan 6: estimating training effect sizes for Chunk 6201 of 12458
Scan 6: estimating training effect sizes for Chunk 6401 of 12458
Scan 6: estimating training effect sizes for Chunk 6601 of 12458
Scan 6: estimating training effect sizes for Chunk 6801 of 12458
Scan 6: estimating training effect sizes for Chunk 7001 of 12458
Scan 6: estimating training effect sizes for Chunk 7201 of 12458
Scan 6: estimating training effect sizes for Chunk 7401 of 12458
Scan 6: estimating training effect sizes for Chunk 7601 of 12458
Scan 6: estimating training effect sizes for Chunk 7801 of 12458
Scan 6: estimating training effect sizes for Chunk 8001 of 12458
Scan 6: estimating training effect sizes for Chunk 8201 of 12458
Scan 6: estimating training effect sizes for Chunk 8401 of 12458
Scan 6: estimating training effect sizes for Chunk 8601 of 12458
Scan 6: estimating training effect sizes for Chunk 8801 of 12458
Scan 6: estimating training effect sizes for Chunk 9001 of 12458
Scan 6: estimating training effect sizes for Chunk 9201 of 12458
Scan 6: estimating training effect sizes for Chunk 9401 of 12458
Scan 6: estimating training effect sizes for Chunk 9601 of 12458
Scan 6: estimating training effect sizes for Chunk 9801 of 12458
Scan 6: estimating training effect sizes for Chunk 10001 of 12458
Scan 6: estimating training effect sizes for Chunk 10201 of 12458
Scan 6: estimating training effect sizes for Chunk 10401 of 12458
Scan 6: estimating training effect sizes for Chunk 10601 of 12458
Scan 6: estimating training effect sizes for Chunk 10801 of 12458
Scan 6: estimating training effect sizes for Chunk 11001 of 12458
Scan 6: estimating training effect sizes for Chunk 11201 of 12458
Scan 6: estimating training effect sizes for Chunk 11401 of 12458
Scan 6: estimating training effect sizes for Chunk 11601 of 12458
Scan 6: estimating training effect sizes for Chunk 11801 of 12458
Scan 6: estimating training effect sizes for Chunk 12001 of 12458
Scan 6: estimating training effect sizes for Chunk 12201 of 12458
Scan 6: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.00

Scan 7: estimating training effect sizes for Chunk 1 of 12458
Scan 7: estimating training effect sizes for Chunk 201 of 12458
Scan 7: estimating training effect sizes for Chunk 401 of 12458
Scan 7: estimating training effect sizes for Chunk 601 of 12458
Scan 7: estimating training effect sizes for Chunk 801 of 12458
Scan 7: estimating training effect sizes for Chunk 1001 of 12458
Scan 7: estimating training effect sizes for Chunk 1201 of 12458
Scan 7: estimating training effect sizes for Chunk 1401 of 12458
Scan 7: estimating training effect sizes for Chunk 1601 of 12458
Scan 7: estimating training effect sizes for Chunk 1801 of 12458
Scan 7: estimating training effect sizes for Chunk 2001 of 12458
Scan 7: estimating training effect sizes for Chunk 2201 of 12458
Scan 7: estimating training effect sizes for Chunk 2401 of 12458
Scan 7: estimating training effect sizes for Chunk 2601 of 12458
Scan 7: estimating training effect sizes for Chunk 2801 of 12458
Scan 7: estimating training effect sizes for Chunk 3001 of 12458
Scan 7: estimating training effect sizes for Chunk 3201 of 12458
Scan 7: estimating training effect sizes for Chunk 3401 of 12458
Scan 7: estimating training effect sizes for Chunk 3601 of 12458
Scan 7: estimating training effect sizes for Chunk 3801 of 12458
Scan 7: estimating training effect sizes for Chunk 4001 of 12458
Scan 7: estimating training effect sizes for Chunk 4201 of 12458
Scan 7: estimating training effect sizes for Chunk 4401 of 12458
Scan 7: estimating training effect sizes for Chunk 4601 of 12458
Scan 7: estimating training effect sizes for Chunk 4801 of 12458
Scan 7: estimating training effect sizes for Chunk 5001 of 12458
Scan 7: estimating training effect sizes for Chunk 5201 of 12458
Scan 7: estimating training effect sizes for Chunk 5401 of 12458
Scan 7: estimating training effect sizes for Chunk 5601 of 12458
Scan 7: estimating training effect sizes for Chunk 5801 of 12458
Scan 7: estimating training effect sizes for Chunk 6001 of 12458
Scan 7: estimating training effect sizes for Chunk 6201 of 12458
Scan 7: estimating training effect sizes for Chunk 6401 of 12458
Scan 7: estimating training effect sizes for Chunk 6601 of 12458
Scan 7: estimating training effect sizes for Chunk 6801 of 12458
Scan 7: estimating training effect sizes for Chunk 7001 of 12458
Scan 7: estimating training effect sizes for Chunk 7201 of 12458
Scan 7: estimating training effect sizes for Chunk 7401 of 12458
Scan 7: estimating training effect sizes for Chunk 7601 of 12458
Scan 7: estimating training effect sizes for Chunk 7801 of 12458
Scan 7: estimating training effect sizes for Chunk 8001 of 12458
Scan 7: estimating training effect sizes for Chunk 8201 of 12458
Scan 7: estimating training effect sizes for Chunk 8401 of 12458
Scan 7: estimating training effect sizes for Chunk 8601 of 12458
Scan 7: estimating training effect sizes for Chunk 8801 of 12458
Scan 7: estimating training effect sizes for Chunk 9001 of 12458
Scan 7: estimating training effect sizes for Chunk 9201 of 12458
Scan 7: estimating training effect sizes for Chunk 9401 of 12458
Scan 7: estimating training effect sizes for Chunk 9601 of 12458
Scan 7: estimating training effect sizes for Chunk 9801 of 12458
Scan 7: estimating training effect sizes for Chunk 10001 of 12458
Scan 7: estimating training effect sizes for Chunk 10201 of 12458
Scan 7: estimating training effect sizes for Chunk 10401 of 12458
Scan 7: estimating training effect sizes for Chunk 10601 of 12458
Scan 7: estimating training effect sizes for Chunk 10801 of 12458
Scan 7: estimating training effect sizes for Chunk 11001 of 12458
Scan 7: estimating training effect sizes for Chunk 11201 of 12458
Scan 7: estimating training effect sizes for Chunk 11401 of 12458
Scan 7: estimating training effect sizes for Chunk 11601 of 12458
Scan 7: estimating training effect sizes for Chunk 11801 of 12458
Scan 7: estimating training effect sizes for Chunk 12001 of 12458
Scan 7: estimating training effect sizes for Chunk 12201 of 12458
Scan 7: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.00

Scan 8: estimating training effect sizes for Chunk 1 of 12458
Scan 8: estimating training effect sizes for Chunk 201 of 12458
Scan 8: estimating training effect sizes for Chunk 401 of 12458
Scan 8: estimating training effect sizes for Chunk 601 of 12458
Scan 8: estimating training effect sizes for Chunk 801 of 12458
Scan 8: estimating training effect sizes for Chunk 1001 of 12458
Scan 8: estimating training effect sizes for Chunk 1201 of 12458
Scan 8: estimating training effect sizes for Chunk 1401 of 12458
Scan 8: estimating training effect sizes for Chunk 1601 of 12458
Scan 8: estimating training effect sizes for Chunk 1801 of 12458
Scan 8: estimating training effect sizes for Chunk 2001 of 12458
Scan 8: estimating training effect sizes for Chunk 2201 of 12458
Scan 8: estimating training effect sizes for Chunk 2401 of 12458
Scan 8: estimating training effect sizes for Chunk 2601 of 12458
Scan 8: estimating training effect sizes for Chunk 2801 of 12458
Scan 8: estimating training effect sizes for Chunk 3001 of 12458
Scan 8: estimating training effect sizes for Chunk 3201 of 12458
Scan 8: estimating training effect sizes for Chunk 3401 of 12458
Scan 8: estimating training effect sizes for Chunk 3601 of 12458
Scan 8: estimating training effect sizes for Chunk 3801 of 12458
Scan 8: estimating training effect sizes for Chunk 4001 of 12458
Scan 8: estimating training effect sizes for Chunk 4201 of 12458
Scan 8: estimating training effect sizes for Chunk 4401 of 12458
Scan 8: estimating training effect sizes for Chunk 4601 of 12458
Scan 8: estimating training effect sizes for Chunk 4801 of 12458
Scan 8: estimating training effect sizes for Chunk 5001 of 12458
Scan 8: estimating training effect sizes for Chunk 5201 of 12458
Scan 8: estimating training effect sizes for Chunk 5401 of 12458
Scan 8: estimating training effect sizes for Chunk 5601 of 12458
Scan 8: estimating training effect sizes for Chunk 5801 of 12458
Scan 8: estimating training effect sizes for Chunk 6001 of 12458
Scan 8: estimating training effect sizes for Chunk 6201 of 12458
Scan 8: estimating training effect sizes for Chunk 6401 of 12458
Scan 8: estimating training effect sizes for Chunk 6601 of 12458
Scan 8: estimating training effect sizes for Chunk 6801 of 12458
Scan 8: estimating training effect sizes for Chunk 7001 of 12458
Scan 8: estimating training effect sizes for Chunk 7201 of 12458
Scan 8: estimating training effect sizes for Chunk 7401 of 12458
Scan 8: estimating training effect sizes for Chunk 7601 of 12458
Scan 8: estimating training effect sizes for Chunk 7801 of 12458
Scan 8: estimating training effect sizes for Chunk 8001 of 12458
Scan 8: estimating training effect sizes for Chunk 8201 of 12458
Scan 8: estimating training effect sizes for Chunk 8401 of 12458
Scan 8: estimating training effect sizes for Chunk 8601 of 12458
Scan 8: estimating training effect sizes for Chunk 8801 of 12458
Scan 8: estimating training effect sizes for Chunk 9001 of 12458
Scan 8: estimating training effect sizes for Chunk 9201 of 12458
Scan 8: estimating training effect sizes for Chunk 9401 of 12458
Scan 8: estimating training effect sizes for Chunk 9601 of 12458
Scan 8: estimating training effect sizes for Chunk 9801 of 12458
Scan 8: estimating training effect sizes for Chunk 10001 of 12458
Scan 8: estimating training effect sizes for Chunk 10201 of 12458
Scan 8: estimating training effect sizes for Chunk 10401 of 12458
Scan 8: estimating training effect sizes for Chunk 10601 of 12458
Scan 8: estimating training effect sizes for Chunk 10801 of 12458
Scan 8: estimating training effect sizes for Chunk 11001 of 12458
Scan 8: estimating training effect sizes for Chunk 11201 of 12458
Scan 8: estimating training effect sizes for Chunk 11401 of 12458
Scan 8: estimating training effect sizes for Chunk 11601 of 12458
Scan 8: estimating training effect sizes for Chunk 11801 of 12458
Scan 8: estimating training effect sizes for Chunk 12001 of 12458
Scan 8: estimating training effect sizes for Chunk 12201 of 12458
Scan 8: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.00

Scan 9: estimating training effect sizes for Chunk 1 of 12458
Scan 9: estimating training effect sizes for Chunk 201 of 12458
Scan 9: estimating training effect sizes for Chunk 401 of 12458
Scan 9: estimating training effect sizes for Chunk 601 of 12458
Scan 9: estimating training effect sizes for Chunk 801 of 12458
Scan 9: estimating training effect sizes for Chunk 1001 of 12458
Scan 9: estimating training effect sizes for Chunk 1201 of 12458
Scan 9: estimating training effect sizes for Chunk 1401 of 12458
Scan 9: estimating training effect sizes for Chunk 1601 of 12458
Scan 9: estimating training effect sizes for Chunk 1801 of 12458
Scan 9: estimating training effect sizes for Chunk 2001 of 12458
Scan 9: estimating training effect sizes for Chunk 2201 of 12458
Scan 9: estimating training effect sizes for Chunk 2401 of 12458
Scan 9: estimating training effect sizes for Chunk 2601 of 12458
Scan 9: estimating training effect sizes for Chunk 2801 of 12458
Scan 9: estimating training effect sizes for Chunk 3001 of 12458
Scan 9: estimating training effect sizes for Chunk 3201 of 12458
Scan 9: estimating training effect sizes for Chunk 3401 of 12458
Scan 9: estimating training effect sizes for Chunk 3601 of 12458
Scan 9: estimating training effect sizes for Chunk 3801 of 12458
Scan 9: estimating training effect sizes for Chunk 4001 of 12458
Scan 9: estimating training effect sizes for Chunk 4201 of 12458
Scan 9: estimating training effect sizes for Chunk 4401 of 12458
Scan 9: estimating training effect sizes for Chunk 4601 of 12458
Scan 9: estimating training effect sizes for Chunk 4801 of 12458
Scan 9: estimating training effect sizes for Chunk 5001 of 12458
Scan 9: estimating training effect sizes for Chunk 5201 of 12458
Scan 9: estimating training effect sizes for Chunk 5401 of 12458
Scan 9: estimating training effect sizes for Chunk 5601 of 12458
Scan 9: estimating training effect sizes for Chunk 5801 of 12458
Scan 9: estimating training effect sizes for Chunk 6001 of 12458
Scan 9: estimating training effect sizes for Chunk 6201 of 12458
Scan 9: estimating training effect sizes for Chunk 6401 of 12458
Scan 9: estimating training effect sizes for Chunk 6601 of 12458
Scan 9: estimating training effect sizes for Chunk 6801 of 12458
Scan 9: estimating training effect sizes for Chunk 7001 of 12458
Scan 9: estimating training effect sizes for Chunk 7201 of 12458
Scan 9: estimating training effect sizes for Chunk 7401 of 12458
Scan 9: estimating training effect sizes for Chunk 7601 of 12458
Scan 9: estimating training effect sizes for Chunk 7801 of 12458
Scan 9: estimating training effect sizes for Chunk 8001 of 12458
Scan 9: estimating training effect sizes for Chunk 8201 of 12458
Scan 9: estimating training effect sizes for Chunk 8401 of 12458
Scan 9: estimating training effect sizes for Chunk 8601 of 12458
Scan 9: estimating training effect sizes for Chunk 8801 of 12458
Scan 9: estimating training effect sizes for Chunk 9001 of 12458
Scan 9: estimating training effect sizes for Chunk 9201 of 12458
Scan 9: estimating training effect sizes for Chunk 9401 of 12458
Scan 9: estimating training effect sizes for Chunk 9601 of 12458
Scan 9: estimating training effect sizes for Chunk 9801 of 12458
Scan 9: estimating training effect sizes for Chunk 10001 of 12458
Scan 9: estimating training effect sizes for Chunk 10201 of 12458
Scan 9: estimating training effect sizes for Chunk 10401 of 12458
Scan 9: estimating training effect sizes for Chunk 10601 of 12458
Scan 9: estimating training effect sizes for Chunk 10801 of 12458
Scan 9: estimating training effect sizes for Chunk 11001 of 12458
Scan 9: estimating training effect sizes for Chunk 11201 of 12458
Scan 9: estimating training effect sizes for Chunk 11401 of 12458
Scan 9: estimating training effect sizes for Chunk 11601 of 12458
Scan 9: estimating training effect sizes for Chunk 11801 of 12458
Scan 9: estimating training effect sizes for Chunk 12001 of 12458
Scan 9: estimating training effect sizes for Chunk 12201 of 12458
Scan 9: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.00

Scan 10: estimating training effect sizes for Chunk 1 of 12458
Scan 10: estimating training effect sizes for Chunk 201 of 12458
Scan 10: estimating training effect sizes for Chunk 401 of 12458
Scan 10: estimating training effect sizes for Chunk 601 of 12458
Scan 10: estimating training effect sizes for Chunk 801 of 12458
Scan 10: estimating training effect sizes for Chunk 1001 of 12458
Scan 10: estimating training effect sizes for Chunk 1201 of 12458
Scan 10: estimating training effect sizes for Chunk 1401 of 12458
Scan 10: estimating training effect sizes for Chunk 1601 of 12458
Scan 10: estimating training effect sizes for Chunk 1801 of 12458
Scan 10: estimating training effect sizes for Chunk 2001 of 12458
Scan 10: estimating training effect sizes for Chunk 2201 of 12458
Scan 10: estimating training effect sizes for Chunk 2401 of 12458
Scan 10: estimating training effect sizes for Chunk 2601 of 12458
Scan 10: estimating training effect sizes for Chunk 2801 of 12458
Scan 10: estimating training effect sizes for Chunk 3001 of 12458
Scan 10: estimating training effect sizes for Chunk 3201 of 12458
Scan 10: estimating training effect sizes for Chunk 3401 of 12458
Scan 10: estimating training effect sizes for Chunk 3601 of 12458
Scan 10: estimating training effect sizes for Chunk 3801 of 12458
Scan 10: estimating training effect sizes for Chunk 4001 of 12458
Scan 10: estimating training effect sizes for Chunk 4201 of 12458
Scan 10: estimating training effect sizes for Chunk 4401 of 12458
Scan 10: estimating training effect sizes for Chunk 4601 of 12458
Scan 10: estimating training effect sizes for Chunk 4801 of 12458
Scan 10: estimating training effect sizes for Chunk 5001 of 12458
Scan 10: estimating training effect sizes for Chunk 5201 of 12458
Scan 10: estimating training effect sizes for Chunk 5401 of 12458
Scan 10: estimating training effect sizes for Chunk 5601 of 12458
Scan 10: estimating training effect sizes for Chunk 5801 of 12458
Scan 10: estimating training effect sizes for Chunk 6001 of 12458
Scan 10: estimating training effect sizes for Chunk 6201 of 12458
Scan 10: estimating training effect sizes for Chunk 6401 of 12458
Scan 10: estimating training effect sizes for Chunk 6601 of 12458
Scan 10: estimating training effect sizes for Chunk 6801 of 12458
Scan 10: estimating training effect sizes for Chunk 7001 of 12458
Scan 10: estimating training effect sizes for Chunk 7201 of 12458
Scan 10: estimating training effect sizes for Chunk 7401 of 12458
Scan 10: estimating training effect sizes for Chunk 7601 of 12458
Scan 10: estimating training effect sizes for Chunk 7801 of 12458
Scan 10: estimating training effect sizes for Chunk 8001 of 12458
Scan 10: estimating training effect sizes for Chunk 8201 of 12458
Scan 10: estimating training effect sizes for Chunk 8401 of 12458
Scan 10: estimating training effect sizes for Chunk 8601 of 12458
Scan 10: estimating training effect sizes for Chunk 8801 of 12458
Scan 10: estimating training effect sizes for Chunk 9001 of 12458
Scan 10: estimating training effect sizes for Chunk 9201 of 12458
Scan 10: estimating training effect sizes for Chunk 9401 of 12458
Scan 10: estimating training effect sizes for Chunk 9601 of 12458
Scan 10: estimating training effect sizes for Chunk 9801 of 12458
Scan 10: estimating training effect sizes for Chunk 10001 of 12458
Scan 10: estimating training effect sizes for Chunk 10201 of 12458
Scan 10: estimating training effect sizes for Chunk 10401 of 12458
Scan 10: estimating training effect sizes for Chunk 10601 of 12458
Scan 10: estimating training effect sizes for Chunk 10801 of 12458
Scan 10: estimating training effect sizes for Chunk 11001 of 12458
Scan 10: estimating training effect sizes for Chunk 11201 of 12458
Scan 10: estimating training effect sizes for Chunk 11401 of 12458
Scan 10: estimating training effect sizes for Chunk 11601 of 12458
Scan 10: estimating training effect sizes for Chunk 11801 of 12458
Scan 10: estimating training effect sizes for Chunk 12001 of 12458
Scan 10: estimating training effect sizes for Chunk 12201 of 12458
Scan 10: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.00

Warning, Variational Bayes did not converge after 10 scans (this is not normally a problem)

The revised estimate of heritability is 0.0100

Measuring accuracy of each model
Model 1: heritability 0.0100, p 0.0000, f2 1.0000, mean squared error 0.9923
Model 2: heritability 0.0100, p 0.5000, f2 0.5000, mean squared error 0.9923
Model 3: heritability 0.0100, p 0.5000, f2 0.3000, mean squared error 0.9932
Model 4: heritability 0.0100, p 0.5000, f2 0.1000, mean squared error 0.9980
Model 5: heritability 0.0100, p 0.1000, f2 0.5000, mean squared error 0.9938
Model 6: heritability 0.0100, p 0.1000, f2 0.3000, mean squared error 0.9960
Model 7: heritability 0.0100, p 0.1000, f2 0.1000, mean squared error 1.0012
Model 8: heritability 0.0100, p 0.0100, f2 0.5000, mean squared error 0.9947
Model 9: heritability 0.0100, p 0.0100, f2 0.3000, mean squared error 0.9944
Model 10: heritability 0.0100, p 0.0100, f2 0.1000, mean squared error 0.9926

Time check: have so far spent 0.24 hours

Constructing final PRS (heritability 0.0100, p 0.0000, f2 1.0000) using all samples

Scan 1: estimating final effect sizes for Chunk 1 of 12458
Scan 1: estimating final effect sizes for Chunk 201 of 12458
Scan 1: estimating final effect sizes for Chunk 401 of 12458
Scan 1: estimating final effect sizes for Chunk 601 of 12458
Scan 1: estimating final effect sizes for Chunk 801 of 12458
Scan 1: estimating final effect sizes for Chunk 1001 of 12458
Scan 1: estimating final effect sizes for Chunk 1201 of 12458
Scan 1: estimating final effect sizes for Chunk 1401 of 12458
Scan 1: estimating final effect sizes for Chunk 1601 of 12458
Scan 1: estimating final effect sizes for Chunk 1801 of 12458
Scan 1: estimating final effect sizes for Chunk 2001 of 12458
Scan 1: estimating final effect sizes for Chunk 2201 of 12458
Scan 1: estimating final effect sizes for Chunk 2401 of 12458
Scan 1: estimating final effect sizes for Chunk 2601 of 12458
Scan 1: estimating final effect sizes for Chunk 2801 of 12458
Scan 1: estimating final effect sizes for Chunk 3001 of 12458
Scan 1: estimating final effect sizes for Chunk 3201 of 12458
Scan 1: estimating final effect sizes for Chunk 3401 of 12458
Scan 1: estimating final effect sizes for Chunk 3601 of 12458
Scan 1: estimating final effect sizes for Chunk 3801 of 12458
Scan 1: estimating final effect sizes for Chunk 4001 of 12458
Scan 1: estimating final effect sizes for Chunk 4201 of 12458
Scan 1: estimating final effect sizes for Chunk 4401 of 12458
Scan 1: estimating final effect sizes for Chunk 4601 of 12458
Scan 1: estimating final effect sizes for Chunk 4801 of 12458
Scan 1: estimating final effect sizes for Chunk 5001 of 12458
Scan 1: estimating final effect sizes for Chunk 5201 of 12458
Scan 1: estimating final effect sizes for Chunk 5401 of 12458
Scan 1: estimating final effect sizes for Chunk 5601 of 12458
Scan 1: estimating final effect sizes for Chunk 5801 of 12458
Scan 1: estimating final effect sizes for Chunk 6001 of 12458
Scan 1: estimating final effect sizes for Chunk 6201 of 12458
Scan 1: estimating final effect sizes for Chunk 6401 of 12458
Scan 1: estimating final effect sizes for Chunk 6601 of 12458
Scan 1: estimating final effect sizes for Chunk 6801 of 12458
Scan 1: estimating final effect sizes for Chunk 7001 of 12458
Scan 1: estimating final effect sizes for Chunk 7201 of 12458
Scan 1: estimating final effect sizes for Chunk 7401 of 12458
Scan 1: estimating final effect sizes for Chunk 7601 of 12458
Scan 1: estimating final effect sizes for Chunk 7801 of 12458
Scan 1: estimating final effect sizes for Chunk 8001 of 12458
Scan 1: estimating final effect sizes for Chunk 8201 of 12458
Scan 1: estimating final effect sizes for Chunk 8401 of 12458
Scan 1: estimating final effect sizes for Chunk 8601 of 12458
Scan 1: estimating final effect sizes for Chunk 8801 of 12458
Scan 1: estimating final effect sizes for Chunk 9001 of 12458
Scan 1: estimating final effect sizes for Chunk 9201 of 12458
Scan 1: estimating final effect sizes for Chunk 9401 of 12458
Scan 1: estimating final effect sizes for Chunk 9601 of 12458
Scan 1: estimating final effect sizes for Chunk 9801 of 12458
Scan 1: estimating final effect sizes for Chunk 10001 of 12458
Scan 1: estimating final effect sizes for Chunk 10201 of 12458
Scan 1: estimating final effect sizes for Chunk 10401 of 12458
Scan 1: estimating final effect sizes for Chunk 10601 of 12458
Scan 1: estimating final effect sizes for Chunk 10801 of 12458
Scan 1: estimating final effect sizes for Chunk 11001 of 12458
Scan 1: estimating final effect sizes for Chunk 11201 of 12458
Scan 1: estimating final effect sizes for Chunk 11401 of 12458
Scan 1: estimating final effect sizes for Chunk 11601 of 12458
Scan 1: estimating final effect sizes for Chunk 11801 of 12458
Scan 1: estimating final effect sizes for Chunk 12001 of 12458
Scan 1: estimating final effect sizes for Chunk 12201 of 12458
Scan 1: estimating final effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.00

Scan 2: estimating final effect sizes for Chunk 1 of 12429
Scan 2: estimating final effect sizes for Chunk 201 of 12429
Scan 2: estimating final effect sizes for Chunk 401 of 12429
Scan 2: estimating final effect sizes for Chunk 601 of 12429
Scan 2: estimating final effect sizes for Chunk 801 of 12429
Scan 2: estimating final effect sizes for Chunk 1001 of 12429
Scan 2: estimating final effect sizes for Chunk 1201 of 12429
Scan 2: estimating final effect sizes for Chunk 1401 of 12429
Scan 2: estimating final effect sizes for Chunk 1601 of 12429
Scan 2: estimating final effect sizes for Chunk 1801 of 12429
Scan 2: estimating final effect sizes for Chunk 2001 of 12429
Scan 2: estimating final effect sizes for Chunk 2201 of 12429
Scan 2: estimating final effect sizes for Chunk 2401 of 12429
Scan 2: estimating final effect sizes for Chunk 2601 of 12429
Scan 2: estimating final effect sizes for Chunk 2801 of 12429
Scan 2: estimating final effect sizes for Chunk 3001 of 12429
Scan 2: estimating final effect sizes for Chunk 3201 of 12429
Scan 2: estimating final effect sizes for Chunk 3401 of 12429
Scan 2: estimating final effect sizes for Chunk 3601 of 12429
Scan 2: estimating final effect sizes for Chunk 3801 of 12429
Scan 2: estimating final effect sizes for Chunk 4001 of 12429
Scan 2: estimating final effect sizes for Chunk 4201 of 12429
Scan 2: estimating final effect sizes for Chunk 4401 of 12429
Scan 2: estimating final effect sizes for Chunk 4601 of 12429
Scan 2: estimating final effect sizes for Chunk 4801 of 12429
Scan 2: estimating final effect sizes for Chunk 5001 of 12429
Scan 2: estimating final effect sizes for Chunk 5201 of 12429
Scan 2: estimating final effect sizes for Chunk 5401 of 12429
Scan 2: estimating final effect sizes for Chunk 5601 of 12429
Scan 2: estimating final effect sizes for Chunk 5801 of 12429
Scan 2: estimating final effect sizes for Chunk 6001 of 12429
Scan 2: estimating final effect sizes for Chunk 6201 of 12429
Scan 2: estimating final effect sizes for Chunk 6401 of 12429
Scan 2: estimating final effect sizes for Chunk 6601 of 12429
Scan 2: estimating final effect sizes for Chunk 6801 of 12429
Scan 2: estimating final effect sizes for Chunk 7001 of 12429
Scan 2: estimating final effect sizes for Chunk 7201 of 12429
Scan 2: estimating final effect sizes for Chunk 7401 of 12429
Scan 2: estimating final effect sizes for Chunk 7601 of 12429
Scan 2: estimating final effect sizes for Chunk 7801 of 12429
Scan 2: estimating final effect sizes for Chunk 8001 of 12429
Scan 2: estimating final effect sizes for Chunk 8201 of 12429
Scan 2: estimating final effect sizes for Chunk 8401 of 12429
Scan 2: estimating final effect sizes for Chunk 8601 of 12429
Scan 2: estimating final effect sizes for Chunk 8801 of 12429
Scan 2: estimating final effect sizes for Chunk 9001 of 12429
Scan 2: estimating final effect sizes for Chunk 9201 of 12429
Scan 2: estimating final effect sizes for Chunk 9401 of 12429
Scan 2: estimating final effect sizes for Chunk 9601 of 12429
Scan 2: estimating final effect sizes for Chunk 9801 of 12429
Scan 2: estimating final effect sizes for Chunk 10001 of 12429
Scan 2: estimating final effect sizes for Chunk 10201 of 12429
Scan 2: estimating final effect sizes for Chunk 10401 of 12429
Scan 2: estimating final effect sizes for Chunk 10601 of 12429
Scan 2: estimating final effect sizes for Chunk 10801 of 12429
Scan 2: estimating final effect sizes for Chunk 11001 of 12429
Scan 2: estimating final effect sizes for Chunk 11201 of 12429
Scan 2: estimating final effect sizes for Chunk 11401 of 12429
Scan 2: estimating final effect sizes for Chunk 11601 of 12429
Scan 2: estimating final effect sizes for Chunk 11801 of 12429
Scan 2: estimating final effect sizes for Chunk 12001 of 12429
Scan 2: estimating final effect sizes for Chunk 12201 of 12429
Scan 2: estimating final effect sizes for Chunk 12401 of 12429
Average number of iterations per chunk: 1.00

Best-fitting model saved in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic.effects, with posterior probabilities in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic.probs

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 08:45:14 2025 and ended at Sun May 25 08:59:48 2025
The elapsed time was 0.24 hours
Given the command used one thread, this means the CPU time was also 0.24 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic.effects
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for one profile

Please note that ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic.effects is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 3189219 predictors from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic.effects

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 3200
Calculating scores for Chunk 51 of 3200
Calculating scores for Chunk 101 of 3200
Calculating scores for Chunk 151 of 3200
Calculating scores for Chunk 201 of 3200
Calculating scores for Chunk 251 of 3200
Calculating scores for Chunk 301 of 3200
Calculating scores for Chunk 351 of 3200
Calculating scores for Chunk 401 of 3200
Calculating scores for Chunk 451 of 3200
Calculating scores for Chunk 501 of 3200
Calculating scores for Chunk 551 of 3200
Calculating scores for Chunk 601 of 3200
Calculating scores for Chunk 651 of 3200
Calculating scores for Chunk 701 of 3200
Calculating scores for Chunk 751 of 3200
Calculating scores for Chunk 801 of 3200
Calculating scores for Chunk 851 of 3200
Calculating scores for Chunk 901 of 3200
Calculating scores for Chunk 951 of 3200
Calculating scores for Chunk 1001 of 3200
Calculating scores for Chunk 1051 of 3200
Calculating scores for Chunk 1101 of 3200
Calculating scores for Chunk 1151 of 3200
Calculating scores for Chunk 1201 of 3200
Calculating scores for Chunk 1251 of 3200
Calculating scores for Chunk 1301 of 3200
Calculating scores for Chunk 1351 of 3200
Calculating scores for Chunk 1401 of 3200
Calculating scores for Chunk 1451 of 3200
Calculating scores for Chunk 1501 of 3200
Calculating scores for Chunk 1551 of 3200
Calculating scores for Chunk 1601 of 3200
Calculating scores for Chunk 1651 of 3200
Calculating scores for Chunk 1701 of 3200
Calculating scores for Chunk 1751 of 3200
Calculating scores for Chunk 1801 of 3200
Calculating scores for Chunk 1851 of 3200
Calculating scores for Chunk 1901 of 3200
Calculating scores for Chunk 1951 of 3200
Calculating scores for Chunk 2001 of 3200
Calculating scores for Chunk 2051 of 3200
Calculating scores for Chunk 2101 of 3200
Calculating scores for Chunk 2151 of 3200
Calculating scores for Chunk 2201 of 3200
Calculating scores for Chunk 2251 of 3200
Calculating scores for Chunk 2301 of 3200
Calculating scores for Chunk 2351 of 3200
Calculating scores for Chunk 2401 of 3200
Calculating scores for Chunk 2451 of 3200
Calculating scores for Chunk 2501 of 3200
Calculating scores for Chunk 2551 of 3200
Calculating scores for Chunk 2601 of 3200
Calculating scores for Chunk 2651 of 3200
Calculating scores for Chunk 2701 of 3200
Calculating scores for Chunk 2751 of 3200
Calculating scores for Chunk 2801 of 3200
Calculating scores for Chunk 2851 of 3200
Calculating scores for Chunk 2901 of 3200
Calculating scores for Chunk 2951 of 3200
Calculating scores for Chunk 3001 of 3200
Calculating scores for Chunk 3051 of 3200
Calculating scores for Chunk 3101 of 3200
Calculating scores for Chunk 3151 of 3200

Profile saved in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic_prs_calc_with_pheno.profile

Correlation between score and phenotype is 0.8660, saved in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 08:59:48 2025 and ended at Sun May 25 09:00:20 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic.effects
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for one profile

Please note that ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic.effects is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 3189219 predictors from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic.effects

Calculating scores for Chunk 1 of 3200
Calculating scores for Chunk 51 of 3200
Calculating scores for Chunk 101 of 3200
Calculating scores for Chunk 151 of 3200
Calculating scores for Chunk 201 of 3200
Calculating scores for Chunk 251 of 3200
Calculating scores for Chunk 301 of 3200
Calculating scores for Chunk 351 of 3200
Calculating scores for Chunk 401 of 3200
Calculating scores for Chunk 451 of 3200
Calculating scores for Chunk 501 of 3200
Calculating scores for Chunk 551 of 3200
Calculating scores for Chunk 601 of 3200
Calculating scores for Chunk 651 of 3200
Calculating scores for Chunk 701 of 3200
Calculating scores for Chunk 751 of 3200
Calculating scores for Chunk 801 of 3200
Calculating scores for Chunk 851 of 3200
Calculating scores for Chunk 901 of 3200
Calculating scores for Chunk 951 of 3200
Calculating scores for Chunk 1001 of 3200
Calculating scores for Chunk 1051 of 3200
Calculating scores for Chunk 1101 of 3200
Calculating scores for Chunk 1151 of 3200
Calculating scores for Chunk 1201 of 3200
Calculating scores for Chunk 1251 of 3200
Calculating scores for Chunk 1301 of 3200
Calculating scores for Chunk 1351 of 3200
Calculating scores for Chunk 1401 of 3200
Calculating scores for Chunk 1451 of 3200
Calculating scores for Chunk 1501 of 3200
Calculating scores for Chunk 1551 of 3200
Calculating scores for Chunk 1601 of 3200
Calculating scores for Chunk 1651 of 3200
Calculating scores for Chunk 1701 of 3200
Calculating scores for Chunk 1751 of 3200
Calculating scores for Chunk 1801 of 3200
Calculating scores for Chunk 1851 of 3200
Calculating scores for Chunk 1901 of 3200
Calculating scores for Chunk 1951 of 3200
Calculating scores for Chunk 2001 of 3200
Calculating scores for Chunk 2051 of 3200
Calculating scores for Chunk 2101 of 3200
Calculating scores for Chunk 2151 of 3200
Calculating scores for Chunk 2201 of 3200
Calculating scores for Chunk 2251 of 3200
Calculating scores for Chunk 2301 of 3200
Calculating scores for Chunk 2351 of 3200
Calculating scores for Chunk 2401 of 3200
Calculating scores for Chunk 2451 of 3200
Calculating scores for Chunk 2501 of 3200
Calculating scores for Chunk 2551 of 3200
Calculating scores for Chunk 2601 of 3200
Calculating scores for Chunk 2651 of 3200
Calculating scores for Chunk 2701 of 3200
Calculating scores for Chunk 2751 of 3200
Calculating scores for Chunk 2801 of 3200
Calculating scores for Chunk 2851 of 3200
Calculating scores for Chunk 2901 of 3200
Calculating scores for Chunk 2951 of 3200
Calculating scores for Chunk 3001 of 3200
Calculating scores for Chunk 3051 of 3200
Calculating scores for Chunk 3101 of 3200
Calculating scores for Chunk 3151 of 3200

Profile saved in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:00:20 2025 and ended at Sun May 25 09:00:51 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## check the PRS is the same ##


###### calculate a PRS using the classical approach ######

## calculate linear association between SNPs and the phenotype ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--linear ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 5 covariates for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.combined")

Performing linear regression for Chunk 1 of 3200
Performing linear regression for Chunk 11 of 3200
Performing linear regression for Chunk 21 of 3200
Performing linear regression for Chunk 31 of 3200
Performing linear regression for Chunk 41 of 3200
Performing linear regression for Chunk 51 of 3200
Performing linear regression for Chunk 61 of 3200
Performing linear regression for Chunk 71 of 3200
Performing linear regression for Chunk 81 of 3200
Performing linear regression for Chunk 91 of 3200
Performing linear regression for Chunk 101 of 3200
Performing linear regression for Chunk 111 of 3200
Performing linear regression for Chunk 121 of 3200
Performing linear regression for Chunk 131 of 3200
Performing linear regression for Chunk 141 of 3200
Performing linear regression for Chunk 151 of 3200
Performing linear regression for Chunk 161 of 3200
Performing linear regression for Chunk 171 of 3200
Performing linear regression for Chunk 181 of 3200
Performing linear regression for Chunk 191 of 3200
Performing linear regression for Chunk 201 of 3200
Performing linear regression for Chunk 211 of 3200
Performing linear regression for Chunk 221 of 3200
Performing linear regression for Chunk 231 of 3200
Performing linear regression for Chunk 241 of 3200
Performing linear regression for Chunk 251 of 3200
Performing linear regression for Chunk 261 of 3200
Performing linear regression for Chunk 271 of 3200
Performing linear regression for Chunk 281 of 3200
Performing linear regression for Chunk 291 of 3200
Performing linear regression for Chunk 301 of 3200
Performing linear regression for Chunk 311 of 3200
Performing linear regression for Chunk 321 of 3200
Performing linear regression for Chunk 331 of 3200
Performing linear regression for Chunk 341 of 3200
Performing linear regression for Chunk 351 of 3200
Performing linear regression for Chunk 361 of 3200
Performing linear regression for Chunk 371 of 3200
Performing linear regression for Chunk 381 of 3200
Performing linear regression for Chunk 391 of 3200
Performing linear regression for Chunk 401 of 3200
Performing linear regression for Chunk 411 of 3200
Performing linear regression for Chunk 421 of 3200
Performing linear regression for Chunk 431 of 3200
Performing linear regression for Chunk 441 of 3200
Performing linear regression for Chunk 451 of 3200
Performing linear regression for Chunk 461 of 3200
Performing linear regression for Chunk 471 of 3200
Performing linear regression for Chunk 481 of 3200
Performing linear regression for Chunk 491 of 3200
Performing linear regression for Chunk 501 of 3200
Performing linear regression for Chunk 511 of 3200
Performing linear regression for Chunk 521 of 3200
Performing linear regression for Chunk 531 of 3200
Performing linear regression for Chunk 541 of 3200
Performing linear regression for Chunk 551 of 3200
Performing linear regression for Chunk 561 of 3200
Performing linear regression for Chunk 571 of 3200
Performing linear regression for Chunk 581 of 3200
Performing linear regression for Chunk 591 of 3200
Performing linear regression for Chunk 601 of 3200
Performing linear regression for Chunk 611 of 3200
Performing linear regression for Chunk 621 of 3200
Performing linear regression for Chunk 631 of 3200
Performing linear regression for Chunk 641 of 3200
Performing linear regression for Chunk 651 of 3200
Performing linear regression for Chunk 661 of 3200
Performing linear regression for Chunk 671 of 3200
Performing linear regression for Chunk 681 of 3200
Performing linear regression for Chunk 691 of 3200
Performing linear regression for Chunk 701 of 3200
Performing linear regression for Chunk 711 of 3200
Performing linear regression for Chunk 721 of 3200
Performing linear regression for Chunk 731 of 3200
Performing linear regression for Chunk 741 of 3200
Performing linear regression for Chunk 751 of 3200
Performing linear regression for Chunk 761 of 3200
Performing linear regression for Chunk 771 of 3200
Performing linear regression for Chunk 781 of 3200
Performing linear regression for Chunk 791 of 3200
Performing linear regression for Chunk 801 of 3200
Performing linear regression for Chunk 811 of 3200
Performing linear regression for Chunk 821 of 3200
Performing linear regression for Chunk 831 of 3200
Performing linear regression for Chunk 841 of 3200
Performing linear regression for Chunk 851 of 3200
Performing linear regression for Chunk 861 of 3200
Performing linear regression for Chunk 871 of 3200
Performing linear regression for Chunk 881 of 3200
Performing linear regression for Chunk 891 of 3200
Performing linear regression for Chunk 901 of 3200
Performing linear regression for Chunk 911 of 3200
Performing linear regression for Chunk 921 of 3200
Performing linear regression for Chunk 931 of 3200
Performing linear regression for Chunk 941 of 3200
Performing linear regression for Chunk 951 of 3200
Performing linear regression for Chunk 961 of 3200
Performing linear regression for Chunk 971 of 3200
Performing linear regression for Chunk 981 of 3200
Performing linear regression for Chunk 991 of 3200
Performing linear regression for Chunk 1001 of 3200
Performing linear regression for Chunk 1011 of 3200
Performing linear regression for Chunk 1021 of 3200
Performing linear regression for Chunk 1031 of 3200
Performing linear regression for Chunk 1041 of 3200
Performing linear regression for Chunk 1051 of 3200
Performing linear regression for Chunk 1061 of 3200
Performing linear regression for Chunk 1071 of 3200
Performing linear regression for Chunk 1081 of 3200
Performing linear regression for Chunk 1091 of 3200
Performing linear regression for Chunk 1101 of 3200
Performing linear regression for Chunk 1111 of 3200
Performing linear regression for Chunk 1121 of 3200
Performing linear regression for Chunk 1131 of 3200
Performing linear regression for Chunk 1141 of 3200
Performing linear regression for Chunk 1151 of 3200
Performing linear regression for Chunk 1161 of 3200
Performing linear regression for Chunk 1171 of 3200
Performing linear regression for Chunk 1181 of 3200
Performing linear regression for Chunk 1191 of 3200
Performing linear regression for Chunk 1201 of 3200
Performing linear regression for Chunk 1211 of 3200
Performing linear regression for Chunk 1221 of 3200
Performing linear regression for Chunk 1231 of 3200
Performing linear regression for Chunk 1241 of 3200
Performing linear regression for Chunk 1251 of 3200
Performing linear regression for Chunk 1261 of 3200
Performing linear regression for Chunk 1271 of 3200
Performing linear regression for Chunk 1281 of 3200
Performing linear regression for Chunk 1291 of 3200
Performing linear regression for Chunk 1301 of 3200
Performing linear regression for Chunk 1311 of 3200
Performing linear regression for Chunk 1321 of 3200
Performing linear regression for Chunk 1331 of 3200
Performing linear regression for Chunk 1341 of 3200
Performing linear regression for Chunk 1351 of 3200
Performing linear regression for Chunk 1361 of 3200
Performing linear regression for Chunk 1371 of 3200
Performing linear regression for Chunk 1381 of 3200
Performing linear regression for Chunk 1391 of 3200
Performing linear regression for Chunk 1401 of 3200
Performing linear regression for Chunk 1411 of 3200
Performing linear regression for Chunk 1421 of 3200
Performing linear regression for Chunk 1431 of 3200
Performing linear regression for Chunk 1441 of 3200
Performing linear regression for Chunk 1451 of 3200
Performing linear regression for Chunk 1461 of 3200
Performing linear regression for Chunk 1471 of 3200
Performing linear regression for Chunk 1481 of 3200
Performing linear regression for Chunk 1491 of 3200
Performing linear regression for Chunk 1501 of 3200
Performing linear regression for Chunk 1511 of 3200
Performing linear regression for Chunk 1521 of 3200
Performing linear regression for Chunk 1531 of 3200
Performing linear regression for Chunk 1541 of 3200
Performing linear regression for Chunk 1551 of 3200
Performing linear regression for Chunk 1561 of 3200
Performing linear regression for Chunk 1571 of 3200
Performing linear regression for Chunk 1581 of 3200
Performing linear regression for Chunk 1591 of 3200
Performing linear regression for Chunk 1601 of 3200
Performing linear regression for Chunk 1611 of 3200
Performing linear regression for Chunk 1621 of 3200
Performing linear regression for Chunk 1631 of 3200
Performing linear regression for Chunk 1641 of 3200
Performing linear regression for Chunk 1651 of 3200
Performing linear regression for Chunk 1661 of 3200
Performing linear regression for Chunk 1671 of 3200
Performing linear regression for Chunk 1681 of 3200
Performing linear regression for Chunk 1691 of 3200
Performing linear regression for Chunk 1701 of 3200
Performing linear regression for Chunk 1711 of 3200
Performing linear regression for Chunk 1721 of 3200
Performing linear regression for Chunk 1731 of 3200
Performing linear regression for Chunk 1741 of 3200
Performing linear regression for Chunk 1751 of 3200
Performing linear regression for Chunk 1761 of 3200
Performing linear regression for Chunk 1771 of 3200
Performing linear regression for Chunk 1781 of 3200
Performing linear regression for Chunk 1791 of 3200
Performing linear regression for Chunk 1801 of 3200
Performing linear regression for Chunk 1811 of 3200
Performing linear regression for Chunk 1821 of 3200
Performing linear regression for Chunk 1831 of 3200
Performing linear regression for Chunk 1841 of 3200
Performing linear regression for Chunk 1851 of 3200
Performing linear regression for Chunk 1861 of 3200
Performing linear regression for Chunk 1871 of 3200
Performing linear regression for Chunk 1881 of 3200
Performing linear regression for Chunk 1891 of 3200
Performing linear regression for Chunk 1901 of 3200
Performing linear regression for Chunk 1911 of 3200
Performing linear regression for Chunk 1921 of 3200
Performing linear regression for Chunk 1931 of 3200
Performing linear regression for Chunk 1941 of 3200
Performing linear regression for Chunk 1951 of 3200
Performing linear regression for Chunk 1961 of 3200
Performing linear regression for Chunk 1971 of 3200
Performing linear regression for Chunk 1981 of 3200
Performing linear regression for Chunk 1991 of 3200
Performing linear regression for Chunk 2001 of 3200
Performing linear regression for Chunk 2011 of 3200
Performing linear regression for Chunk 2021 of 3200
Performing linear regression for Chunk 2031 of 3200
Performing linear regression for Chunk 2041 of 3200
Performing linear regression for Chunk 2051 of 3200
Performing linear regression for Chunk 2061 of 3200
Performing linear regression for Chunk 2071 of 3200
Performing linear regression for Chunk 2081 of 3200
Performing linear regression for Chunk 2091 of 3200
Performing linear regression for Chunk 2101 of 3200
Performing linear regression for Chunk 2111 of 3200
Performing linear regression for Chunk 2121 of 3200
Performing linear regression for Chunk 2131 of 3200
Performing linear regression for Chunk 2141 of 3200
Performing linear regression for Chunk 2151 of 3200
Performing linear regression for Chunk 2161 of 3200
Performing linear regression for Chunk 2171 of 3200
Performing linear regression for Chunk 2181 of 3200
Performing linear regression for Chunk 2191 of 3200
Performing linear regression for Chunk 2201 of 3200
Performing linear regression for Chunk 2211 of 3200
Performing linear regression for Chunk 2221 of 3200
Performing linear regression for Chunk 2231 of 3200
Performing linear regression for Chunk 2241 of 3200
Performing linear regression for Chunk 2251 of 3200
Performing linear regression for Chunk 2261 of 3200
Performing linear regression for Chunk 2271 of 3200
Performing linear regression for Chunk 2281 of 3200
Performing linear regression for Chunk 2291 of 3200
Performing linear regression for Chunk 2301 of 3200
Performing linear regression for Chunk 2311 of 3200
Performing linear regression for Chunk 2321 of 3200
Performing linear regression for Chunk 2331 of 3200
Performing linear regression for Chunk 2341 of 3200
Performing linear regression for Chunk 2351 of 3200
Performing linear regression for Chunk 2361 of 3200
Performing linear regression for Chunk 2371 of 3200
Performing linear regression for Chunk 2381 of 3200
Performing linear regression for Chunk 2391 of 3200
Performing linear regression for Chunk 2401 of 3200
Performing linear regression for Chunk 2411 of 3200
Performing linear regression for Chunk 2421 of 3200
Performing linear regression for Chunk 2431 of 3200
Performing linear regression for Chunk 2441 of 3200
Performing linear regression for Chunk 2451 of 3200
Performing linear regression for Chunk 2461 of 3200
Performing linear regression for Chunk 2471 of 3200
Performing linear regression for Chunk 2481 of 3200
Performing linear regression for Chunk 2491 of 3200
Performing linear regression for Chunk 2501 of 3200
Performing linear regression for Chunk 2511 of 3200
Performing linear regression for Chunk 2521 of 3200
Performing linear regression for Chunk 2531 of 3200
Performing linear regression for Chunk 2541 of 3200
Performing linear regression for Chunk 2551 of 3200
Performing linear regression for Chunk 2561 of 3200
Performing linear regression for Chunk 2571 of 3200
Performing linear regression for Chunk 2581 of 3200
Performing linear regression for Chunk 2591 of 3200
Performing linear regression for Chunk 2601 of 3200
Performing linear regression for Chunk 2611 of 3200
Performing linear regression for Chunk 2621 of 3200
Performing linear regression for Chunk 2631 of 3200
Performing linear regression for Chunk 2641 of 3200
Performing linear regression for Chunk 2651 of 3200
Performing linear regression for Chunk 2661 of 3200
Performing linear regression for Chunk 2671 of 3200
Performing linear regression for Chunk 2681 of 3200
Performing linear regression for Chunk 2691 of 3200
Performing linear regression for Chunk 2701 of 3200
Performing linear regression for Chunk 2711 of 3200
Performing linear regression for Chunk 2721 of 3200
Performing linear regression for Chunk 2731 of 3200
Performing linear regression for Chunk 2741 of 3200
Performing linear regression for Chunk 2751 of 3200
Performing linear regression for Chunk 2761 of 3200
Performing linear regression for Chunk 2771 of 3200
Performing linear regression for Chunk 2781 of 3200
Performing linear regression for Chunk 2791 of 3200
Performing linear regression for Chunk 2801 of 3200
Performing linear regression for Chunk 2811 of 3200
Performing linear regression for Chunk 2821 of 3200
Performing linear regression for Chunk 2831 of 3200
Performing linear regression for Chunk 2841 of 3200
Performing linear regression for Chunk 2851 of 3200
Performing linear regression for Chunk 2861 of 3200
Performing linear regression for Chunk 2871 of 3200
Performing linear regression for Chunk 2881 of 3200
Performing linear regression for Chunk 2891 of 3200
Performing linear regression for Chunk 2901 of 3200
Performing linear regression for Chunk 2911 of 3200
Performing linear regression for Chunk 2921 of 3200
Performing linear regression for Chunk 2931 of 3200
Performing linear regression for Chunk 2941 of 3200
Performing linear regression for Chunk 2951 of 3200
Performing linear regression for Chunk 2961 of 3200
Performing linear regression for Chunk 2971 of 3200
Performing linear regression for Chunk 2981 of 3200
Performing linear regression for Chunk 2991 of 3200
Performing linear regression for Chunk 3001 of 3200
Performing linear regression for Chunk 3011 of 3200
Performing linear regression for Chunk 3021 of 3200
Performing linear regression for Chunk 3031 of 3200
Performing linear regression for Chunk 3041 of 3200
Performing linear regression for Chunk 3051 of 3200
Performing linear regression for Chunk 3061 of 3200
Performing linear regression for Chunk 3071 of 3200
Performing linear regression for Chunk 3081 of 3200
Performing linear regression for Chunk 3091 of 3200
Performing linear regression for Chunk 3101 of 3200
Performing linear regression for Chunk 3111 of 3200
Performing linear regression for Chunk 3121 of 3200
Performing linear regression for Chunk 3131 of 3200
Performing linear regression for Chunk 3141 of 3200
Performing linear regression for Chunk 3151 of 3200
Performing linear regression for Chunk 3161 of 3200
Performing linear regression for Chunk 3171 of 3200
Performing linear regression for Chunk 3181 of 3200
Performing linear regression for Chunk 3191 of 3200

Main results saved in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.assoc, with a summary version in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.summaries, p-values in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues and score file in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:00:51 2025 and ended at Sun May 25 09:01:24 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## define p-values cut-offs for thresholding ##

# load the p-values #

# get the minimum p-value #

# make a list of thresholds that are above the minimum p-value #

## obtain scores in a reduced set of SNPs after thresholding and cumpling ##

# perform thresholding considering the threshold 1 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_small_set_predictors_set_linear_clump_thresholding_1_predictors
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 1

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e+00, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

3189219 of the 3189219 predictors in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e+00
All of these are in the data

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading p-values from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr1_858952_G_A 7.9889e-01 | chr1_905373_T_C 8.4011e-01 | chr1_911428_C_T 2.4461e-01

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 25 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 127569; stay tuned for updates ;)
Pass 1: Thinning for Chunk 2001 of 127569; kept 5361 out of 50000 predictors (10.72%)
Pass 1: Thinning for Chunk 4001 of 127569; kept 10172 out of 100000 predictors (10.17%)
Pass 1: Thinning for Chunk 6001 of 127569; kept 15074 out of 150000 predictors (10.05%)
Pass 1: Thinning for Chunk 8001 of 127569; kept 19867 out of 200000 predictors (9.93%)
Pass 1: Thinning for Chunk 10001 of 127569; kept 24403 out of 250000 predictors (9.76%)
Pass 1: Thinning for Chunk 12001 of 127569; kept 28915 out of 300000 predictors (9.64%)
Pass 1: Thinning for Chunk 14001 of 127569; kept 33492 out of 350000 predictors (9.57%)
Pass 1: Thinning for Chunk 16001 of 127569; kept 38100 out of 400000 predictors (9.53%)
Pass 1: Thinning for Chunk 18001 of 127569; kept 42904 out of 450000 predictors (9.53%)
Pass 1: Thinning for Chunk 20001 of 127569; kept 48300 out of 500000 predictors (9.66%)
Pass 1: Thinning for Chunk 22001 of 127569; kept 52809 out of 550000 predictors (9.60%)
Pass 1: Thinning for Chunk 24001 of 127569; kept 57579 out of 600000 predictors (9.60%)
Pass 1: Thinning for Chunk 26001 of 127569; kept 62051 out of 650000 predictors (9.55%)
Pass 1: Thinning for Chunk 28001 of 127569; kept 66815 out of 700000 predictors (9.54%)
Pass 1: Thinning for Chunk 30001 of 127569; kept 71575 out of 750000 predictors (9.54%)
Pass 1: Thinning for Chunk 32001 of 127569; kept 76182 out of 800000 predictors (9.52%)
Pass 1: Thinning for Chunk 34001 of 127569; kept 80369 out of 850000 predictors (9.46%)
Pass 1: Thinning for Chunk 36001 of 127569; kept 84688 out of 900000 predictors (9.41%)
Pass 1: Thinning for Chunk 38001 of 127569; kept 89154 out of 950000 predictors (9.38%)
Pass 1: Thinning for Chunk 40001 of 127569; kept 93462 out of 1000000 predictors (9.35%)
Pass 1: Thinning for Chunk 42001 of 127569; kept 98087 out of 1050000 predictors (9.34%)
Pass 1: Thinning for Chunk 44001 of 127569; kept 103033 out of 1100000 predictors (9.37%)
Pass 1: Thinning for Chunk 46001 of 127569; kept 107398 out of 1150000 predictors (9.34%)
Pass 1: Thinning for Chunk 48001 of 127569; kept 112434 out of 1200000 predictors (9.37%)
Pass 1: Thinning for Chunk 50001 of 127569; kept 116908 out of 1250000 predictors (9.35%)
Pass 1: Thinning for Chunk 52001 of 127569; kept 120727 out of 1300000 predictors (9.29%)
Pass 1: Thinning for Chunk 54001 of 127569; kept 125043 out of 1350000 predictors (9.26%)
Pass 1: Thinning for Chunk 56001 of 127569; kept 129989 out of 1400000 predictors (9.28%)
Pass 1: Thinning for Chunk 58001 of 127569; kept 134245 out of 1450000 predictors (9.26%)
Pass 1: Thinning for Chunk 60001 of 127569; kept 138438 out of 1500000 predictors (9.23%)
Pass 1: Thinning for Chunk 62001 of 127569; kept 142485 out of 1550000 predictors (9.19%)
Pass 1: Thinning for Chunk 64001 of 127569; kept 147423 out of 1600000 predictors (9.21%)
Pass 1: Thinning for Chunk 66001 of 127569; kept 151873 out of 1650000 predictors (9.20%)
Pass 1: Thinning for Chunk 68001 of 127569; kept 155640 out of 1700000 predictors (9.16%)
Pass 1: Thinning for Chunk 70001 of 127569; kept 160151 out of 1750000 predictors (9.15%)
Pass 1: Thinning for Chunk 72001 of 127569; kept 164831 out of 1800000 predictors (9.16%)
Pass 1: Thinning for Chunk 74001 of 127569; kept 169256 out of 1850000 predictors (9.15%)
Pass 1: Thinning for Chunk 76001 of 127569; kept 174056 out of 1900000 predictors (9.16%)
Pass 1: Thinning for Chunk 78001 of 127569; kept 178935 out of 1950000 predictors (9.18%)
Pass 1: Thinning for Chunk 80001 of 127569; kept 183614 out of 2000000 predictors (9.18%)
Pass 1: Thinning for Chunk 82001 of 127569; kept 187706 out of 2050000 predictors (9.16%)
Pass 1: Thinning for Chunk 84001 of 127569; kept 192509 out of 2100000 predictors (9.17%)
Pass 1: Thinning for Chunk 86001 of 127569; kept 196930 out of 2150000 predictors (9.16%)
Pass 1: Thinning for Chunk 88001 of 127569; kept 201399 out of 2200000 predictors (9.15%)
Pass 1: Thinning for Chunk 90001 of 127569; kept 206160 out of 2250000 predictors (9.16%)
Pass 1: Thinning for Chunk 92001 of 127569; kept 210812 out of 2300000 predictors (9.17%)
Pass 1: Thinning for Chunk 94001 of 127569; kept 215181 out of 2350000 predictors (9.16%)
Pass 1: Thinning for Chunk 96001 of 127569; kept 219813 out of 2400000 predictors (9.16%)
Pass 1: Thinning for Chunk 98001 of 127569; kept 224912 out of 2450000 predictors (9.18%)
Pass 1: Thinning for Chunk 100001 of 127569; kept 229749 out of 2500000 predictors (9.19%)
Pass 1: Thinning for Chunk 102001 of 127569; kept 234124 out of 2550000 predictors (9.18%)
Pass 1: Thinning for Chunk 104001 of 127569; kept 238895 out of 2600000 predictors (9.19%)
Pass 1: Thinning for Chunk 106001 of 127569; kept 243245 out of 2650000 predictors (9.18%)
Pass 1: Thinning for Chunk 108001 of 127569; kept 248313 out of 2700000 predictors (9.20%)
Pass 1: Thinning for Chunk 110001 of 127569; kept 253570 out of 2750000 predictors (9.22%)
Pass 1: Thinning for Chunk 112001 of 127569; kept 258345 out of 2800000 predictors (9.23%)
Pass 1: Thinning for Chunk 114001 of 127569; kept 263144 out of 2850000 predictors (9.23%)
Pass 1: Thinning for Chunk 116001 of 127569; kept 268629 out of 2900000 predictors (9.26%)
Pass 1: Thinning for Chunk 118001 of 127569; kept 274263 out of 2950000 predictors (9.30%)
Pass 1: Thinning for Chunk 120001 of 127569; kept 278998 out of 3000000 predictors (9.30%)
Pass 1: Thinning for Chunk 122001 of 127569; kept 284446 out of 3050000 predictors (9.33%)
Pass 1: Thinning for Chunk 124001 of 127569; kept 289594 out of 3100000 predictors (9.34%)
Pass 1: Thinning for Chunk 126001 of 127569; kept 294644 out of 3150000 predictors (9.35%)

The bit-size has now been set to 124

Pass 2: Thinning for Chunk 1 of 2410; stay tuned for updates ;)
Pass 2: Thinning for Chunk 401 of 2410; kept 15514 out of 49600 predictors (31.28%)
Pass 2: Thinning for Chunk 801 of 2410; kept 31625 out of 99200 predictors (31.88%)
Pass 2: Thinning for Chunk 1201 of 2410; kept 46651 out of 148800 predictors (31.35%)
Pass 2: Thinning for Chunk 1601 of 2410; kept 64288 out of 198400 predictors (32.40%)
Pass 2: Thinning for Chunk 2001 of 2410; kept 80687 out of 248000 predictors (32.54%)
Pass 2: Thinning for Chunk 2401 of 2410; kept 101836 out of 297600 predictors (34.22%)

Thinning complete: 102461 predictors kept (saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_small_set_predictors_set_linear_clump_thresholding_1_predictors.in), 3086758 lost (./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_small_set_predictors_set_linear_clump_thresholding_1_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_small_set_predictors_set_linear_clump_thresholding_1_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:01:26 2025 and ended at Sun May 25 09:03:03 2025
The elapsed time was 0.03 hours
Given the command used one thread, this means the CPU time was also 0.03 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_small_set_predictors_set_linear_clump_thresholding_1_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 102461 predictors to extract from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_small_set_predictors_set_linear_clump_thresholding_1_predictors.in

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 102461)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 5 covariates for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1.combined")

Performing linear regression for Chunk 1 of 115
Performing linear regression for Chunk 11 of 115
Performing linear regression for Chunk 21 of 115
Performing linear regression for Chunk 31 of 115
Performing linear regression for Chunk 41 of 115
Performing linear regression for Chunk 51 of 115
Performing linear regression for Chunk 61 of 115
Performing linear regression for Chunk 71 of 115
Performing linear regression for Chunk 81 of 115
Performing linear regression for Chunk 91 of 115
Performing linear regression for Chunk 101 of 115
Performing linear regression for Chunk 111 of 115

Main results saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1.assoc, with a summary version in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1.summaries, p-values in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1.pvalues and score file in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:03:03 2025 and ended at Sun May 25 09:03:11 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1.score
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 102461 predictors from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1.score

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 115
Calculating scores for Chunk 51 of 115
Calculating scores for Chunk 101 of 115

Profiles saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.2734 to 0.8768, saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:03:11 2025 and ended at Sun May 25 09:03:19 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1.score
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 102461 predictors from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1.score

Calculating scores for Chunk 1 of 115
Calculating scores for Chunk 51 of 115
Calculating scores for Chunk 101 of 115

Profiles saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:03:19 2025 and ended at Sun May 25 09:03:27 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


# perform thresholding considering the threshold 0.1 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 0.1

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e-01, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

329245 of the 3189219 predictors in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e-01
All of these are in the data

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 329245)

Reading p-values from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr1_962184_T_C 3.0774e-02 | chr1_984039_T_C 4.1330e-02 | chr1_1094994_G_A 3.1431e-02

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 20 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 16463; stay tuned for updates ;)
Pass 1: Thinning for Chunk 2501 of 16463; kept 8238 out of 50000 predictors (16.48%)
Pass 1: Thinning for Chunk 5001 of 16463; kept 15825 out of 100000 predictors (15.82%)
Pass 1: Thinning for Chunk 7501 of 16463; kept 24196 out of 150000 predictors (16.13%)
Pass 1: Thinning for Chunk 10001 of 16463; kept 32202 out of 200000 predictors (16.10%)
Pass 1: Thinning for Chunk 12501 of 16463; kept 39878 out of 250000 predictors (15.95%)
Pass 1: Thinning for Chunk 15001 of 16463; kept 48346 out of 300000 predictors (16.12%)

The bit-size has now been set to 25

Pass 2: Thinning for Chunk 1 of 2143; stay tuned for updates ;)
Pass 2: Thinning for Chunk 2001 of 2143; kept 22818 out of 50000 predictors (45.64%)

Thinning complete: 24843 predictors kept (saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors.in), 304402 lost (./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:03:27 2025 and ended at Sun May 25 09:04:11 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 24843 predictors to extract from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors.in

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 24843)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 5 covariates for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1.combined")

Performing linear regression for Chunk 1 of 35
Performing linear regression for Chunk 11 of 35
Performing linear regression for Chunk 21 of 35
Performing linear regression for Chunk 31 of 35

Main results saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1.assoc, with a summary version in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1.summaries, p-values in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1.pvalues and score file in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:04:11 2025 and ended at Sun May 25 09:04:19 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1.score
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 24843 predictors from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1.score

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 35

Profiles saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.2734 to 0.8764, saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:04:19 2025 and ended at Sun May 25 09:04:27 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1.score
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 24843 predictors from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1.score

Calculating scores for Chunk 1 of 35

Profiles saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:04:27 2025 and ended at Sun May 25 09:04:34 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


# perform thresholding considering the threshold 0.01 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 0.01

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e-02, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

33452 of the 3189219 predictors in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e-02
All of these are in the data

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 33452)

Reading p-values from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr1_3972702_C_T 5.0617e-03 | chr1_4061687_C_A 8.4956e-05 | chr1_4163015_G_A 6.2272e-03

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 20 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 1673; stay tuned for updates ;)

The bit-size has now been set to 20

Pass 2: Thinning for Chunk 1 of 336; stay tuned for updates ;)

Thinning complete: 3748 predictors kept (saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors.in), 29704 lost (./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:04:34 2025 and ended at Sun May 25 09:05:13 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 3748 predictors to extract from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors.in

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3748)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 5 covariates for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01.combined")

Performing linear regression for Chunk 1 of 22
Performing linear regression for Chunk 11 of 22
Performing linear regression for Chunk 21 of 22

Main results saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01.assoc, with a summary version in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01.summaries, p-values in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01.pvalues and score file in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:05:13 2025 and ended at Sun May 25 09:05:20 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01.score
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 3748 predictors from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01.score

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 22

Profiles saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.2734 to 0.8679, saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:05:20 2025 and ended at Sun May 25 09:05:26 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01.score
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 3748 predictors from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01.score

Calculating scores for Chunk 1 of 22

Profiles saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:05:26 2025 and ended at Sun May 25 09:05:33 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


# perform thresholding considering the threshold 0.001 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 0.001

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e-03, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

3332 of the 3189219 predictors in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e-03
All of these are in the data

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3332)

Reading p-values from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr1_4061687_C_A 8.4956e-05 | chr1_7770202_T_C 3.4378e-04 | chr1_7776738_A_G 4.3470e-04

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 20 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 167; stay tuned for updates ;)

The bit-size has now been set to 20

Pass 2: Thinning for Chunk 1 of 40; stay tuned for updates ;)

Thinning complete: 488 predictors kept (saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors.in), 2844 lost (./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:05:33 2025 and ended at Sun May 25 09:06:12 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 488 predictors to extract from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors.in

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 488)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 5 covariates for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001.combined")

Performing linear regression for Chunk 1 of 22
Performing linear regression for Chunk 11 of 22
Performing linear regression for Chunk 21 of 22

Main results saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001.assoc, with a summary version in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001.summaries, p-values in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001.pvalues and score file in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:06:12 2025 and ended at Sun May 25 09:06:19 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001.score
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 488 predictors from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001.score

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 22

Profiles saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.2734 to 0.8220, saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:06:19 2025 and ended at Sun May 25 09:06:26 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001.score
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 488 predictors from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001.score

Calculating scores for Chunk 1 of 22

Profiles saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:06:26 2025 and ended at Sun May 25 09:06:33 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


# perform thresholding considering the threshold 0.0001 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 0.0001

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e-04, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

226 of the 3189219 predictors in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e-04
All of these are in the data

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 226)

Reading p-values from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr1_4061687_C_A 8.4956e-05 | chr1_22352632_C_T 7.7733e-05 | chr1_68384728_T_A 2.3351e-05

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 20 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 12; stay tuned for updates ;)

The bit-size has now been set to 20

Pass 2: Thinning for Chunk 1 of 4; stay tuned for updates ;)

Thinning complete: 43 predictors kept (saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors.in), 183 lost (./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:06:33 2025 and ended at Sun May 25 09:07:11 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 43 predictors to extract from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors.in

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 43)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 5 covariates for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001.combined")

Performing linear regression for Chunk 1 of 15
Performing linear regression for Chunk 11 of 15

Main results saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001.assoc, with a summary version in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001.summaries, p-values in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001.pvalues and score file in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:07:11 2025 and ended at Sun May 25 09:07:18 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001.score
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 43 predictors from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001.score

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 15

Profiles saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.2734 to 0.5573, saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:07:18 2025 and ended at Sun May 25 09:07:24 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001.score
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 43 predictors from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001.score

Calculating scores for Chunk 1 of 15

Profiles saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:07:24 2025 and ended at Sun May 25 09:07:31 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


# perform thresholding considering the threshold 1e-05 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 1e-05

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e-05, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

34 of the 3189219 predictors in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e-05
All of these are in the data

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 34)

Reading p-values from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr3_187336176_T_C 4.2381e-06 | chr3_187336418_T_C 4.2458e-06 | chr3_187336496_T_C 6.3265e-06

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 20 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 2; stay tuned for updates ;)

The bit-size has now been set to 20

Pass 2: Thinning for Chunk 1 of 1; stay tuned for updates ;)

Thinning complete: 5 predictors kept (saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors.in), 29 lost (./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:07:31 2025 and ended at Sun May 25 09:08:11 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 5 predictors to extract from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors.in

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 5)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 5 covariates for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05.combined")

Performing linear regression for Chunk 1 of 4

Main results saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05.assoc, with a summary version in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05.summaries, p-values in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05.pvalues and score file in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:08:11 2025 and ended at Sun May 25 09:08:17 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05.score
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 5 predictors from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05.score

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 4

Profiles saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.2734 to 0.2734, saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:08:17 2025 and ended at Sun May 25 09:08:24 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05.score
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 5 predictors from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05.score

Calculating scores for Chunk 1 of 4

Profiles saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Sun May 25 09:08:24 2025 and ended at Sun May 25 09:08:30 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


###### check we have used the correct samples in both analyses ######

## load the FAM files used for training and test ##

## samples in files generated by elastic net ##

## samples in files generated by linear ##

###### plot quantiles of PRS against phenotype ######

## prepare folders ##


## load the phenotype data before transformation ##

## process it ##

# get only the samples finally included in modelling #

# split the ID into FID and IID #

# check that the new variables has been correctly create #

## open the plot ##

# Create a figure with 8 subplots arranged in a grid (e.g., 2 rows x 4 columns) #

## Flatten the axes array for easier iteration ##

## create a list of models ##

## iterate across models ##

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          7            -0.6100              1.7              4.4325
1          3            -4.7000              0.3              3.3750
2          2            -4.4750              0.0              2.3000
3          1            -9.8750             -3.3              1.4000
4          9             0.0000              2.4              5.3250
5          4            -3.5000              0.7              3.5250
6         15             1.6250              4.0              6.6250
7         11             0.5250              3.3              5.5500
8         12             1.0250              3.7              5.7000
9         19             3.6000              7.0              8.7250
10        17             3.2000              5.3              8.1000
11         5            -2.5750              0.2              2.5250
12         8            -1.7000              1.5              6.0500
13         6            -0.9750              1.0              3.1750
14        10            -0.6000              2.7              5.4000
15        20             3.8250              8.6             16.5500
16        13             0.2500              3.4              5.7000
17        16             1.7000              4.4              7.2250
18        14             0.8225              3.7              6.3000
19        18             3.7250              6.1              9.4250

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          7            -0.9325              1.6              3.4325
1          3            -4.8500              0.0              2.2250
2          1            -9.8750             -3.3              1.3000
3         10             0.0750              2.8              5.4000
4          5            -3.4250              0.6              3.5250
5         13             1.6250              3.6              5.6000
6          9             0.0750              2.4              5.9250
7         17             3.2250              5.3              7.6750
8          4            -1.0000              0.9              3.3750
9         18             3.7250              6.3              9.2000
10         6            -2.7500              0.7              3.0750
11         8            -1.5000              1.5              4.1250
12        20             5.2500              8.8             16.5500
13        11             0.6000              3.4              5.6250
14        16             1.3500              4.2              7.0500
15        12             0.2000              3.6              5.6250
16        19             3.1250              6.9              9.1000
17         2            -5.3750              0.0              2.2250
18        14             2.1225              3.7              6.3000
19        15             1.7750              4.4              7.1500

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          8            -1.0500             2.00               4.200
1          3            -4.7000             0.30               2.450
2          1            -9.8750            -3.30               1.300
3         10             0.0750             2.50               5.400
4          5            -2.7500             0.60               3.300
5         14             1.5450             3.60               6.610
6         12             0.1500             3.40               5.700
7         17             3.2000             5.10               8.425
8         19             3.1250             6.90               9.100
9         16             1.4000             4.20               7.750
10         7            -1.1525             1.25               3.275
11         6            -3.4750             1.30               3.500
12         9             0.0000             2.30               5.800
13        18             3.5750             5.90               8.250
14        20             5.4000             8.60              16.550
15        11             0.6500             3.40               5.400
16        13             1.8500             3.70               5.600
17         2            -5.3750            -0.30               2.300
18         4            -0.7000             0.60               3.200
19        15             2.3250             4.10               6.225

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          8            -1.0750             1.90               4.150
1          3            -4.2500             0.00               3.275
2          4            -3.9750             0.30               2.600
3          1            -9.8750            -3.30               1.850
4         12             0.4000             3.10               5.000
5          6            -3.4250             1.00               3.375
6         14             1.5225             3.60               5.700
7         13             1.0750             3.70               5.925
8         10             0.0750             3.10               5.700
9         16             1.4750             4.50               7.750
10        18             3.3500             6.30              10.250
11        11             0.0000             3.40               5.400
12         5            -2.6000             0.40               2.750
13         7            -1.0550             2.05               4.500
14        20             5.2250             8.60              16.550
15         2            -5.4000            -0.30               2.000
16        15             2.3000             3.90               7.600
17         9             0.4000             1.90               5.000
18        17             2.9000             5.10               7.225
19        19             3.7250             6.90               8.800

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0         10             0.1000             2.40              5.3000
1          6            -2.6250             1.30              4.8750
2          3            -4.0000             0.00              3.2000
3          2            -5.0000             0.00              2.7500
4         12             0.0750             3.00              5.5500
5          4            -4.1750             0.30              3.4000
6         11             0.3750             3.10              5.9250
7         15             1.4500             4.20              7.3500
8          8            -1.2500             1.60              4.0750
9         14             0.7675             3.70              6.3975
10        17             2.4750             5.30              8.7500
11        18             2.4750             5.70              8.2500
12        19             3.1500             6.00             10.5500
13         9            -1.0000             2.10              6.2250
14         1            -9.8750            -3.30              2.2250
15        16             0.3750             4.00              8.6750
16        20             4.2750             8.50             16.5500
17         7            -2.7850             1.75              5.3775
18        13             1.2500             3.80              6.3750
19         5            -1.0000             1.00              3.9500

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          6            -3.9250              1.7              6.3750
1          1            -9.8750              0.0              3.3750
2         16             0.0750              4.0              8.6750
3         15             0.6000              4.2              8.9250
4         12            -2.8500              3.2              7.2250
5         13            -2.7750              3.8              7.3750
6         10            -2.9250              2.4              7.6000
7         14            -0.2325              3.7             13.2750
8         18            -1.0000              4.1              8.8750
9          4            -4.6500              1.3              5.4500
10        20             2.3500              6.4             16.5500
11        17             0.6750              4.4              8.8750
12         9            -2.4000              2.3              7.0750
13         3            -4.6250              1.3              5.6000
14         2            -5.9250              1.2              5.4000
15         5            -3.3500              1.5              5.6000
16        11            -0.8750              3.1              6.9000
17        19             1.1250              5.0             10.3250
18         8            -4.6750              1.8              5.7000
19         7            -4.1850              2.3              6.3075

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0         12            -2.3350             4.00             12.5725
1          8            -4.4825             2.50              7.8775
2          1            -6.8800             1.60              6.5050
3          9            -6.1800             2.40              8.4800
4          5            -1.3500             2.30              7.2750
5          7            -5.0825             2.80              8.1300
6          2            -3.5000             2.10              5.4000
7         11             1.0800             3.40              6.9400
8         14            -1.0450             4.70              9.3800
9         16            -1.7050             4.05             11.4375
10        13            -2.8375             4.50              9.4575
11         3            -5.1000             2.30              7.5000
12         6            -4.0800             2.60              8.4200
13        15            -0.6200             3.80             14.1400
14        10            -4.2400             2.60              9.8075
15         4            -1.3350             2.60              5.6525

# plot the results #

# finish the plot #

###### manhattan plots ######

## load assoc results to pandas ##
         Chromosome           Predictor  Basepair  ... CallRate MachR2  SPA_Status
0                 1     chr1_858952_G_A    858952  ...      1.0    NaN    NOT_USED
1                 1     chr1_905373_T_C    905373  ...      1.0    NaN    NOT_USED
2                 1     chr1_911428_C_T    911428  ...      1.0    NaN    NOT_USED
3                 1     chr1_918870_A_G    918870  ...      1.0    NaN    NOT_USED
4                 1     chr1_931513_T_C    931513  ...      1.0    NaN    NOT_USED
...             ...                 ...       ...  ...      ...    ...         ...
3189214          22  chr22_50749145_C_T  50749145  ...      1.0    NaN    NOT_USED
3189215          22  chr22_50749470_T_C  50749470  ...      1.0    NaN    NOT_USED
3189216          22  chr22_50749890_T_G  50749890  ...      1.0    NaN    NOT_USED
3189217          22  chr22_50751123_G_T  50751123  ...      1.0    NaN    NOT_USED
3189218          22  chr22_50752652_C_G  50752652  ...      1.0    NaN    NOT_USED

[3189219 rows x 13 columns]

## check we have the correct columns ##

## check we have the correct dtypes ##

## calculate -log_10(pvalue) ##

## convert chromosome to category and then sort by it ##

## sort rows by chromosome and basepair position ##

## Creates a new column called ind in the assoc_results DataFrame ##

## Groups the assoc_results DataFrame by the Chromosome column ##

## make manhattan plot ##

# open the plot #

# Add a title to the plot #

# Define a colorblind-friendly palette #

# iterate across chromosomes #

# set the x-axis ticks and labels of these ticks #

# add p-value thresholds as horizontal lines #

# set axis limits #

# set the axis label #

# Increase font size for tick labels #

# add a legend to the subplot #

# save the plot as a static image #

###### compress the results ######

## remove bed and bim plink files initialles used as input (compressed files already created) ##


## elastic outputs ##


## linear outputs ##

# first raw outputs before clumping #


# then outputs after clumping #







#######################################
#######################################
FINISH
#######################################
#######################################
