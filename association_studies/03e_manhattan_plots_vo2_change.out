
#######################################
#######################################
checking function to print nicely: header 1
#######################################
#######################################

###### checking function to print nicely: header 2 ######

## checking function to print nicely: header 3 ##

# checking function to print nicely: header 4 #

#######################################
#######################################
check behaviour run_bash
#######################################
#######################################

###### see working directory ######
/home/dftortosa/diego_docs/science/other_projects/australian_army_bishop/heavy_analyses/australian_army_bishop/association_studies


###### list files/folders there ######
03a_association_analyses.sif
03a_phenotype_prep.out
03aa_final_pca_calc.out
03b_prs_calculation_100_iter_large_set_predictors_beep_change.out
03b_prs_calculation_100_iter_large_set_predictors_distance_change.out
03b_prs_calculation_100_iter_large_set_predictors_vo2_change.out
03b_prs_calculation_100_iter_large_set_predictors_weight_change.out
03b_prs_calculation_100_iter_small_set_predictors_beep_change.out
03b_prs_calculation_100_iter_small_set_predictors_distance_change.out
03b_prs_calculation_100_iter_small_set_predictors_vo2_change.out
03b_prs_calculation_100_iter_small_set_predictors_weight_change.out
03c_prepare_slurm_files.out
03d_processing_results.out
03e_manhattan_plots_beep_change.out
03e_manhattan_plots_distance_change.out
03e_manhattan_plots_vo2_change.out
03e_manhattan_plots_weight_change.out
03f_bat_smt_analyses_bat.out
03f_bat_smt_analyses_metabolic.out
03f_bat_smt_analyses_smt.out
data
literature
results
scripts


#######################################
#######################################
For phenotype vo2_change, and the small dataset of covariates
#######################################
#######################################

###### initial preparations ######

## create folders for results ##


## load the phenotype data ##
                 family_id AGRF code  ...  Week 1 Pred VO2max  vo2_change
0     combat_ILGSA24-17303  0200ADMM  ...            0.269686   -0.331645
1     combat_ILGSA24-17303  0200ASJM  ...            1.420329   -2.046929
2     combat_ILGSA24-17303  0200BHNM  ...           -1.724345   -0.930017
3     combat_ILGSA24-17303  0200CBOM  ...           -0.922440   -0.721711
4     combat_ILGSA24-17303  0200CDFM  ...           -0.156062   -0.892645
...                    ...       ...  ...                 ...         ...
1013  combat_ILGSA24-17873  8098ATDN  ...            0.394904    0.416334
1014  combat_ILGSA24-17873  8098RSJN  ...           -0.234031   -1.038123
1015  combat_ILGSA24-17873  8098TSAN  ...            0.807651   -0.649914
1016  combat_ILGSA24-17873  8099AJNN  ...            0.269686    0.384258
1017  combat_ILGSA24-17873  8099COSN  ...            0.394904    0.138603

[1018 rows x 10 columns]

## specify the covariates ##
Index(['PCA5', 'PCA8', 'PCA13', 'PCA16', 'sex_code', 'Week 1 Body Mass',
       'Week 1 Pred VO2max'],
      dtype='object')

## decompress bim and bed files with all sample generated after NA cleaning ##


###### prepare LDAK inputs ######

###### calculate elastic PRS ######

## run the PRS with --elastic ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--elastic ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--LOCO NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Constructing elastic net PRS

Will consider five values for the predictor scaling (alpha = -1, -0.75, -0.5, -0.25 and 0); to instead specify the value, use "--power" (or use "--powerfile" to provide a range of values)

Will use the default prior parameter choices (saved in the file ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic.parameters); to instead specify your own, use "--parameters"

Will select the best prior parameters via cross-validation, using 0.10 randomly-picked test samples (use "--cv-proportion" to change this proportion, "--cv-samples" to explicitly specify the test samples, or "--cv-skip" to turn off cross-validation)

Will always include the LOCO polygenic contribution, regardless of their estimated accuracy

When constructing PRS, will scan the data at most 10 times (change this using "--num-scans")

All heritability estimates must be between 0.01 and 0.8000 (change the upper bound using "--max-her")

Will use either three or ten random vectors for Monte Carlo operations (decided based on the number of samples); change this number using "--num-random-vectors"

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 6 covariates for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic.combined")

When performing cross-validation, will use 917 samples to train models and 101 to test their accuracy

Warning, to process the data requires 6.1 Gb; sorry this can not be reduced

Warning, to perform the analysis requires approximately 2.1 Gb; sorry, this can not be reduced

Estimating per-predictor heritabilities using Randomized Haseman-Elston Regression with 10 random vectors

Will divide the predictors into 20 partitions (change this using "--num-divides")
Will exclude chunks containing a predictor with estimated variance explained greater than 0.0982 (change this using "--max-cor")

Calculating traces for Chunk 1 of 12458
Calculating traces for Chunk 201 of 12458
Calculating traces for Chunk 401 of 12458
Calculating traces for Chunk 601 of 12458
Calculating traces for Chunk 801 of 12458
Calculating traces for Chunk 1001 of 12458
Calculating traces for Chunk 1201 of 12458
Calculating traces for Chunk 1401 of 12458
Calculating traces for Chunk 1601 of 12458
Calculating traces for Chunk 1801 of 12458
Calculating traces for Chunk 2001 of 12458
Calculating traces for Chunk 2201 of 12458
Calculating traces for Chunk 2401 of 12458
Calculating traces for Chunk 2601 of 12458
Calculating traces for Chunk 2801 of 12458
Calculating traces for Chunk 3001 of 12458
Calculating traces for Chunk 3201 of 12458
Calculating traces for Chunk 3401 of 12458
Calculating traces for Chunk 3601 of 12458
Calculating traces for Chunk 3801 of 12458
Calculating traces for Chunk 4001 of 12458
Calculating traces for Chunk 4201 of 12458
Calculating traces for Chunk 4401 of 12458
Calculating traces for Chunk 4601 of 12458
Calculating traces for Chunk 4801 of 12458
Calculating traces for Chunk 5001 of 12458
Calculating traces for Chunk 5201 of 12458
Calculating traces for Chunk 5401 of 12458
Calculating traces for Chunk 5601 of 12458
Calculating traces for Chunk 5801 of 12458
Calculating traces for Chunk 6001 of 12458
Calculating traces for Chunk 6201 of 12458
Calculating traces for Chunk 6401 of 12458
Calculating traces for Chunk 6601 of 12458
Calculating traces for Chunk 6801 of 12458
Calculating traces for Chunk 7001 of 12458
Calculating traces for Chunk 7201 of 12458
Calculating traces for Chunk 7401 of 12458
Calculating traces for Chunk 7601 of 12458
Calculating traces for Chunk 7801 of 12458
Calculating traces for Chunk 8001 of 12458
Calculating traces for Chunk 8201 of 12458
Calculating traces for Chunk 8401 of 12458
Calculating traces for Chunk 8601 of 12458
Calculating traces for Chunk 8801 of 12458
Calculating traces for Chunk 9001 of 12458
Calculating traces for Chunk 9201 of 12458
Calculating traces for Chunk 9401 of 12458
Calculating traces for Chunk 9601 of 12458
Calculating traces for Chunk 9801 of 12458
Calculating traces for Chunk 10001 of 12458
Calculating traces for Chunk 10201 of 12458
Calculating traces for Chunk 10401 of 12458
Calculating traces for Chunk 10601 of 12458
Calculating traces for Chunk 10801 of 12458
Calculating traces for Chunk 11001 of 12458
Calculating traces for Chunk 11201 of 12458
Calculating traces for Chunk 11401 of 12458
Calculating traces for Chunk 11601 of 12458
Calculating traces for Chunk 11801 of 12458
Calculating traces for Chunk 12001 of 12458
Calculating traces for Chunk 12201 of 12458
Calculating traces for Chunk 12401 of 12458

Best power is -1.0000 and estimated heritability is 0.8138
Warning, the heritability is very high, so has been reduced to 0.8000

Time check: have so far spent 0.02 hours

Constructing 10 PRS using training samples
Will also make 33 MCMC REML models (using all samples)

Scan 1: estimating training effect sizes for Chunk 1 of 12458
Scan 1: estimating training effect sizes for Chunk 201 of 12458
Scan 1: estimating training effect sizes for Chunk 401 of 12458
Scan 1: estimating training effect sizes for Chunk 601 of 12458
Scan 1: estimating training effect sizes for Chunk 801 of 12458
Scan 1: estimating training effect sizes for Chunk 1001 of 12458
Scan 1: estimating training effect sizes for Chunk 1201 of 12458
Scan 1: estimating training effect sizes for Chunk 1401 of 12458
Scan 1: estimating training effect sizes for Chunk 1601 of 12458
Scan 1: estimating training effect sizes for Chunk 1801 of 12458
Scan 1: estimating training effect sizes for Chunk 2001 of 12458
Scan 1: estimating training effect sizes for Chunk 2201 of 12458
Scan 1: estimating training effect sizes for Chunk 2401 of 12458
Scan 1: estimating training effect sizes for Chunk 2601 of 12458
Scan 1: estimating training effect sizes for Chunk 2801 of 12458
Scan 1: estimating training effect sizes for Chunk 3001 of 12458
Scan 1: estimating training effect sizes for Chunk 3201 of 12458
Scan 1: estimating training effect sizes for Chunk 3401 of 12458
Scan 1: estimating training effect sizes for Chunk 3601 of 12458
Scan 1: estimating training effect sizes for Chunk 3801 of 12458
Scan 1: estimating training effect sizes for Chunk 4001 of 12458
Scan 1: estimating training effect sizes for Chunk 4201 of 12458
Scan 1: estimating training effect sizes for Chunk 4401 of 12458
Scan 1: estimating training effect sizes for Chunk 4601 of 12458
Scan 1: estimating training effect sizes for Chunk 4801 of 12458
Scan 1: estimating training effect sizes for Chunk 5001 of 12458
Scan 1: estimating training effect sizes for Chunk 5201 of 12458
Scan 1: estimating training effect sizes for Chunk 5401 of 12458
Scan 1: estimating training effect sizes for Chunk 5601 of 12458
Scan 1: estimating training effect sizes for Chunk 5801 of 12458
Scan 1: estimating training effect sizes for Chunk 6001 of 12458
Scan 1: estimating training effect sizes for Chunk 6201 of 12458
Scan 1: estimating training effect sizes for Chunk 6401 of 12458
Scan 1: estimating training effect sizes for Chunk 6601 of 12458
Scan 1: estimating training effect sizes for Chunk 6801 of 12458
Scan 1: estimating training effect sizes for Chunk 7001 of 12458
Scan 1: estimating training effect sizes for Chunk 7201 of 12458
Scan 1: estimating training effect sizes for Chunk 7401 of 12458
Scan 1: estimating training effect sizes for Chunk 7601 of 12458
Scan 1: estimating training effect sizes for Chunk 7801 of 12458
Scan 1: estimating training effect sizes for Chunk 8001 of 12458
Scan 1: estimating training effect sizes for Chunk 8201 of 12458
Scan 1: estimating training effect sizes for Chunk 8401 of 12458
Scan 1: estimating training effect sizes for Chunk 8601 of 12458
Scan 1: estimating training effect sizes for Chunk 8801 of 12458
Scan 1: estimating training effect sizes for Chunk 9001 of 12458
Scan 1: estimating training effect sizes for Chunk 9201 of 12458
Scan 1: estimating training effect sizes for Chunk 9401 of 12458
Scan 1: estimating training effect sizes for Chunk 9601 of 12458
Scan 1: estimating training effect sizes for Chunk 9801 of 12458
Scan 1: estimating training effect sizes for Chunk 10001 of 12458
Scan 1: estimating training effect sizes for Chunk 10201 of 12458
Scan 1: estimating training effect sizes for Chunk 10401 of 12458
Scan 1: estimating training effect sizes for Chunk 10601 of 12458
Scan 1: estimating training effect sizes for Chunk 10801 of 12458
Scan 1: estimating training effect sizes for Chunk 11001 of 12458
Scan 1: estimating training effect sizes for Chunk 11201 of 12458
Scan 1: estimating training effect sizes for Chunk 11401 of 12458
Scan 1: estimating training effect sizes for Chunk 11601 of 12458
Scan 1: estimating training effect sizes for Chunk 11801 of 12458
Scan 1: estimating training effect sizes for Chunk 12001 of 12458
Scan 1: estimating training effect sizes for Chunk 12201 of 12458
Scan 1: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 3.52

Scan 2: estimating training effect sizes for Chunk 1 of 12458
Scan 2: estimating training effect sizes for Chunk 201 of 12458
Scan 2: estimating training effect sizes for Chunk 401 of 12458
Scan 2: estimating training effect sizes for Chunk 601 of 12458
Scan 2: estimating training effect sizes for Chunk 801 of 12458
Scan 2: estimating training effect sizes for Chunk 1001 of 12458
Scan 2: estimating training effect sizes for Chunk 1201 of 12458
Scan 2: estimating training effect sizes for Chunk 1401 of 12458
Scan 2: estimating training effect sizes for Chunk 1601 of 12458
Scan 2: estimating training effect sizes for Chunk 1801 of 12458
Scan 2: estimating training effect sizes for Chunk 2001 of 12458
Scan 2: estimating training effect sizes for Chunk 2201 of 12458
Scan 2: estimating training effect sizes for Chunk 2401 of 12458
Scan 2: estimating training effect sizes for Chunk 2601 of 12458
Scan 2: estimating training effect sizes for Chunk 2801 of 12458
Scan 2: estimating training effect sizes for Chunk 3001 of 12458
Scan 2: estimating training effect sizes for Chunk 3201 of 12458
Scan 2: estimating training effect sizes for Chunk 3401 of 12458
Scan 2: estimating training effect sizes for Chunk 3601 of 12458
Scan 2: estimating training effect sizes for Chunk 3801 of 12458
Scan 2: estimating training effect sizes for Chunk 4001 of 12458
Scan 2: estimating training effect sizes for Chunk 4201 of 12458
Scan 2: estimating training effect sizes for Chunk 4401 of 12458
Scan 2: estimating training effect sizes for Chunk 4601 of 12458
Scan 2: estimating training effect sizes for Chunk 4801 of 12458
Scan 2: estimating training effect sizes for Chunk 5001 of 12458
Scan 2: estimating training effect sizes for Chunk 5201 of 12458
Scan 2: estimating training effect sizes for Chunk 5401 of 12458
Scan 2: estimating training effect sizes for Chunk 5601 of 12458
Scan 2: estimating training effect sizes for Chunk 5801 of 12458
Scan 2: estimating training effect sizes for Chunk 6001 of 12458
Scan 2: estimating training effect sizes for Chunk 6201 of 12458
Scan 2: estimating training effect sizes for Chunk 6401 of 12458
Scan 2: estimating training effect sizes for Chunk 6601 of 12458
Scan 2: estimating training effect sizes for Chunk 6801 of 12458
Scan 2: estimating training effect sizes for Chunk 7001 of 12458
Scan 2: estimating training effect sizes for Chunk 7201 of 12458
Scan 2: estimating training effect sizes for Chunk 7401 of 12458
Scan 2: estimating training effect sizes for Chunk 7601 of 12458
Scan 2: estimating training effect sizes for Chunk 7801 of 12458
Scan 2: estimating training effect sizes for Chunk 8001 of 12458
Scan 2: estimating training effect sizes for Chunk 8201 of 12458
Scan 2: estimating training effect sizes for Chunk 8401 of 12458
Scan 2: estimating training effect sizes for Chunk 8601 of 12458
Scan 2: estimating training effect sizes for Chunk 8801 of 12458
Scan 2: estimating training effect sizes for Chunk 9001 of 12458
Scan 2: estimating training effect sizes for Chunk 9201 of 12458
Scan 2: estimating training effect sizes for Chunk 9401 of 12458
Scan 2: estimating training effect sizes for Chunk 9601 of 12458
Scan 2: estimating training effect sizes for Chunk 9801 of 12458
Scan 2: estimating training effect sizes for Chunk 10001 of 12458
Scan 2: estimating training effect sizes for Chunk 10201 of 12458
Scan 2: estimating training effect sizes for Chunk 10401 of 12458
Scan 2: estimating training effect sizes for Chunk 10601 of 12458
Scan 2: estimating training effect sizes for Chunk 10801 of 12458
Scan 2: estimating training effect sizes for Chunk 11001 of 12458
Scan 2: estimating training effect sizes for Chunk 11201 of 12458
Scan 2: estimating training effect sizes for Chunk 11401 of 12458
Scan 2: estimating training effect sizes for Chunk 11601 of 12458
Scan 2: estimating training effect sizes for Chunk 11801 of 12458
Scan 2: estimating training effect sizes for Chunk 12001 of 12458
Scan 2: estimating training effect sizes for Chunk 12201 of 12458
Scan 2: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 3.27

Scan 3: estimating training effect sizes for Chunk 1 of 12458
Scan 3: estimating training effect sizes for Chunk 201 of 12458
Scan 3: estimating training effect sizes for Chunk 401 of 12458
Scan 3: estimating training effect sizes for Chunk 601 of 12458
Scan 3: estimating training effect sizes for Chunk 801 of 12458
Scan 3: estimating training effect sizes for Chunk 1001 of 12458
Scan 3: estimating training effect sizes for Chunk 1201 of 12458
Scan 3: estimating training effect sizes for Chunk 1401 of 12458
Scan 3: estimating training effect sizes for Chunk 1601 of 12458
Scan 3: estimating training effect sizes for Chunk 1801 of 12458
Scan 3: estimating training effect sizes for Chunk 2001 of 12458
Scan 3: estimating training effect sizes for Chunk 2201 of 12458
Scan 3: estimating training effect sizes for Chunk 2401 of 12458
Scan 3: estimating training effect sizes for Chunk 2601 of 12458
Scan 3: estimating training effect sizes for Chunk 2801 of 12458
Scan 3: estimating training effect sizes for Chunk 3001 of 12458
Scan 3: estimating training effect sizes for Chunk 3201 of 12458
Scan 3: estimating training effect sizes for Chunk 3401 of 12458
Scan 3: estimating training effect sizes for Chunk 3601 of 12458
Scan 3: estimating training effect sizes for Chunk 3801 of 12458
Scan 3: estimating training effect sizes for Chunk 4001 of 12458
Scan 3: estimating training effect sizes for Chunk 4201 of 12458
Scan 3: estimating training effect sizes for Chunk 4401 of 12458
Scan 3: estimating training effect sizes for Chunk 4601 of 12458
Scan 3: estimating training effect sizes for Chunk 4801 of 12458
Scan 3: estimating training effect sizes for Chunk 5001 of 12458
Scan 3: estimating training effect sizes for Chunk 5201 of 12458
Scan 3: estimating training effect sizes for Chunk 5401 of 12458
Scan 3: estimating training effect sizes for Chunk 5601 of 12458
Scan 3: estimating training effect sizes for Chunk 5801 of 12458
Scan 3: estimating training effect sizes for Chunk 6001 of 12458
Scan 3: estimating training effect sizes for Chunk 6201 of 12458
Scan 3: estimating training effect sizes for Chunk 6401 of 12458
Scan 3: estimating training effect sizes for Chunk 6601 of 12458
Scan 3: estimating training effect sizes for Chunk 6801 of 12458
Scan 3: estimating training effect sizes for Chunk 7001 of 12458
Scan 3: estimating training effect sizes for Chunk 7201 of 12458
Scan 3: estimating training effect sizes for Chunk 7401 of 12458
Scan 3: estimating training effect sizes for Chunk 7601 of 12458
Scan 3: estimating training effect sizes for Chunk 7801 of 12458
Scan 3: estimating training effect sizes for Chunk 8001 of 12458
Scan 3: estimating training effect sizes for Chunk 8201 of 12458
Scan 3: estimating training effect sizes for Chunk 8401 of 12458
Scan 3: estimating training effect sizes for Chunk 8601 of 12458
Scan 3: estimating training effect sizes for Chunk 8801 of 12458
Scan 3: estimating training effect sizes for Chunk 9001 of 12458
Scan 3: estimating training effect sizes for Chunk 9201 of 12458
Scan 3: estimating training effect sizes for Chunk 9401 of 12458
Scan 3: estimating training effect sizes for Chunk 9601 of 12458
Scan 3: estimating training effect sizes for Chunk 9801 of 12458
Scan 3: estimating training effect sizes for Chunk 10001 of 12458
Scan 3: estimating training effect sizes for Chunk 10201 of 12458
Scan 3: estimating training effect sizes for Chunk 10401 of 12458
Scan 3: estimating training effect sizes for Chunk 10601 of 12458
Scan 3: estimating training effect sizes for Chunk 10801 of 12458
Scan 3: estimating training effect sizes for Chunk 11001 of 12458
Scan 3: estimating training effect sizes for Chunk 11201 of 12458
Scan 3: estimating training effect sizes for Chunk 11401 of 12458
Scan 3: estimating training effect sizes for Chunk 11601 of 12458
Scan 3: estimating training effect sizes for Chunk 11801 of 12458
Scan 3: estimating training effect sizes for Chunk 12001 of 12458
Scan 3: estimating training effect sizes for Chunk 12201 of 12458
Scan 3: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.95

Scan 4: estimating training effect sizes for Chunk 1 of 12458
Scan 4: estimating training effect sizes for Chunk 201 of 12458
Scan 4: estimating training effect sizes for Chunk 401 of 12458
Scan 4: estimating training effect sizes for Chunk 601 of 12458
Scan 4: estimating training effect sizes for Chunk 801 of 12458
Scan 4: estimating training effect sizes for Chunk 1001 of 12458
Scan 4: estimating training effect sizes for Chunk 1201 of 12458
Scan 4: estimating training effect sizes for Chunk 1401 of 12458
Scan 4: estimating training effect sizes for Chunk 1601 of 12458
Scan 4: estimating training effect sizes for Chunk 1801 of 12458
Scan 4: estimating training effect sizes for Chunk 2001 of 12458
Scan 4: estimating training effect sizes for Chunk 2201 of 12458
Scan 4: estimating training effect sizes for Chunk 2401 of 12458
Scan 4: estimating training effect sizes for Chunk 2601 of 12458
Scan 4: estimating training effect sizes for Chunk 2801 of 12458
Scan 4: estimating training effect sizes for Chunk 3001 of 12458
Scan 4: estimating training effect sizes for Chunk 3201 of 12458
Scan 4: estimating training effect sizes for Chunk 3401 of 12458
Scan 4: estimating training effect sizes for Chunk 3601 of 12458
Scan 4: estimating training effect sizes for Chunk 3801 of 12458
Scan 4: estimating training effect sizes for Chunk 4001 of 12458
Scan 4: estimating training effect sizes for Chunk 4201 of 12458
Scan 4: estimating training effect sizes for Chunk 4401 of 12458
Scan 4: estimating training effect sizes for Chunk 4601 of 12458
Scan 4: estimating training effect sizes for Chunk 4801 of 12458
Scan 4: estimating training effect sizes for Chunk 5001 of 12458
Scan 4: estimating training effect sizes for Chunk 5201 of 12458
Scan 4: estimating training effect sizes for Chunk 5401 of 12458
Scan 4: estimating training effect sizes for Chunk 5601 of 12458
Scan 4: estimating training effect sizes for Chunk 5801 of 12458
Scan 4: estimating training effect sizes for Chunk 6001 of 12458
Scan 4: estimating training effect sizes for Chunk 6201 of 12458
Scan 4: estimating training effect sizes for Chunk 6401 of 12458
Scan 4: estimating training effect sizes for Chunk 6601 of 12458
Scan 4: estimating training effect sizes for Chunk 6801 of 12458
Scan 4: estimating training effect sizes for Chunk 7001 of 12458
Scan 4: estimating training effect sizes for Chunk 7201 of 12458
Scan 4: estimating training effect sizes for Chunk 7401 of 12458
Scan 4: estimating training effect sizes for Chunk 7601 of 12458
Scan 4: estimating training effect sizes for Chunk 7801 of 12458
Scan 4: estimating training effect sizes for Chunk 8001 of 12458
Scan 4: estimating training effect sizes for Chunk 8201 of 12458
Scan 4: estimating training effect sizes for Chunk 8401 of 12458
Scan 4: estimating training effect sizes for Chunk 8601 of 12458
Scan 4: estimating training effect sizes for Chunk 8801 of 12458
Scan 4: estimating training effect sizes for Chunk 9001 of 12458
Scan 4: estimating training effect sizes for Chunk 9201 of 12458
Scan 4: estimating training effect sizes for Chunk 9401 of 12458
Scan 4: estimating training effect sizes for Chunk 9601 of 12458
Scan 4: estimating training effect sizes for Chunk 9801 of 12458
Scan 4: estimating training effect sizes for Chunk 10001 of 12458
Scan 4: estimating training effect sizes for Chunk 10201 of 12458
Scan 4: estimating training effect sizes for Chunk 10401 of 12458
Scan 4: estimating training effect sizes for Chunk 10601 of 12458
Scan 4: estimating training effect sizes for Chunk 10801 of 12458
Scan 4: estimating training effect sizes for Chunk 11001 of 12458
Scan 4: estimating training effect sizes for Chunk 11201 of 12458
Scan 4: estimating training effect sizes for Chunk 11401 of 12458
Scan 4: estimating training effect sizes for Chunk 11601 of 12458
Scan 4: estimating training effect sizes for Chunk 11801 of 12458
Scan 4: estimating training effect sizes for Chunk 12001 of 12458
Scan 4: estimating training effect sizes for Chunk 12201 of 12458
Scan 4: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.57

Scan 5: estimating training effect sizes for Chunk 1 of 12458
Scan 5: estimating training effect sizes for Chunk 201 of 12458
Scan 5: estimating training effect sizes for Chunk 401 of 12458
Scan 5: estimating training effect sizes for Chunk 601 of 12458
Scan 5: estimating training effect sizes for Chunk 801 of 12458
Scan 5: estimating training effect sizes for Chunk 1001 of 12458
Scan 5: estimating training effect sizes for Chunk 1201 of 12458
Scan 5: estimating training effect sizes for Chunk 1401 of 12458
Scan 5: estimating training effect sizes for Chunk 1601 of 12458
Scan 5: estimating training effect sizes for Chunk 1801 of 12458
Scan 5: estimating training effect sizes for Chunk 2001 of 12458
Scan 5: estimating training effect sizes for Chunk 2201 of 12458
Scan 5: estimating training effect sizes for Chunk 2401 of 12458
Scan 5: estimating training effect sizes for Chunk 2601 of 12458
Scan 5: estimating training effect sizes for Chunk 2801 of 12458
Scan 5: estimating training effect sizes for Chunk 3001 of 12458
Scan 5: estimating training effect sizes for Chunk 3201 of 12458
Scan 5: estimating training effect sizes for Chunk 3401 of 12458
Scan 5: estimating training effect sizes for Chunk 3601 of 12458
Scan 5: estimating training effect sizes for Chunk 3801 of 12458
Scan 5: estimating training effect sizes for Chunk 4001 of 12458
Scan 5: estimating training effect sizes for Chunk 4201 of 12458
Scan 5: estimating training effect sizes for Chunk 4401 of 12458
Scan 5: estimating training effect sizes for Chunk 4601 of 12458
Scan 5: estimating training effect sizes for Chunk 4801 of 12458
Scan 5: estimating training effect sizes for Chunk 5001 of 12458
Scan 5: estimating training effect sizes for Chunk 5201 of 12458
Scan 5: estimating training effect sizes for Chunk 5401 of 12458
Scan 5: estimating training effect sizes for Chunk 5601 of 12458
Scan 5: estimating training effect sizes for Chunk 5801 of 12458
Scan 5: estimating training effect sizes for Chunk 6001 of 12458
Scan 5: estimating training effect sizes for Chunk 6201 of 12458
Scan 5: estimating training effect sizes for Chunk 6401 of 12458
Scan 5: estimating training effect sizes for Chunk 6601 of 12458
Scan 5: estimating training effect sizes for Chunk 6801 of 12458
Scan 5: estimating training effect sizes for Chunk 7001 of 12458
Scan 5: estimating training effect sizes for Chunk 7201 of 12458
Scan 5: estimating training effect sizes for Chunk 7401 of 12458
Scan 5: estimating training effect sizes for Chunk 7601 of 12458
Scan 5: estimating training effect sizes for Chunk 7801 of 12458
Scan 5: estimating training effect sizes for Chunk 8001 of 12458
Scan 5: estimating training effect sizes for Chunk 8201 of 12458
Scan 5: estimating training effect sizes for Chunk 8401 of 12458
Scan 5: estimating training effect sizes for Chunk 8601 of 12458
Scan 5: estimating training effect sizes for Chunk 8801 of 12458
Scan 5: estimating training effect sizes for Chunk 9001 of 12458
Scan 5: estimating training effect sizes for Chunk 9201 of 12458
Scan 5: estimating training effect sizes for Chunk 9401 of 12458
Scan 5: estimating training effect sizes for Chunk 9601 of 12458
Scan 5: estimating training effect sizes for Chunk 9801 of 12458
Scan 5: estimating training effect sizes for Chunk 10001 of 12458
Scan 5: estimating training effect sizes for Chunk 10201 of 12458
Scan 5: estimating training effect sizes for Chunk 10401 of 12458
Scan 5: estimating training effect sizes for Chunk 10601 of 12458
Scan 5: estimating training effect sizes for Chunk 10801 of 12458
Scan 5: estimating training effect sizes for Chunk 11001 of 12458
Scan 5: estimating training effect sizes for Chunk 11201 of 12458
Scan 5: estimating training effect sizes for Chunk 11401 of 12458
Scan 5: estimating training effect sizes for Chunk 11601 of 12458
Scan 5: estimating training effect sizes for Chunk 11801 of 12458
Scan 5: estimating training effect sizes for Chunk 12001 of 12458
Scan 5: estimating training effect sizes for Chunk 12201 of 12458
Scan 5: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.36

Scan 6: estimating training effect sizes for Chunk 1 of 12458
Scan 6: estimating training effect sizes for Chunk 201 of 12458
Scan 6: estimating training effect sizes for Chunk 401 of 12458
Scan 6: estimating training effect sizes for Chunk 601 of 12458
Scan 6: estimating training effect sizes for Chunk 801 of 12458
Scan 6: estimating training effect sizes for Chunk 1001 of 12458
Scan 6: estimating training effect sizes for Chunk 1201 of 12458
Scan 6: estimating training effect sizes for Chunk 1401 of 12458
Scan 6: estimating training effect sizes for Chunk 1601 of 12458
Scan 6: estimating training effect sizes for Chunk 1801 of 12458
Scan 6: estimating training effect sizes for Chunk 2001 of 12458
Scan 6: estimating training effect sizes for Chunk 2201 of 12458
Scan 6: estimating training effect sizes for Chunk 2401 of 12458
Scan 6: estimating training effect sizes for Chunk 2601 of 12458
Scan 6: estimating training effect sizes for Chunk 2801 of 12458
Scan 6: estimating training effect sizes for Chunk 3001 of 12458
Scan 6: estimating training effect sizes for Chunk 3201 of 12458
Scan 6: estimating training effect sizes for Chunk 3401 of 12458
Scan 6: estimating training effect sizes for Chunk 3601 of 12458
Scan 6: estimating training effect sizes for Chunk 3801 of 12458
Scan 6: estimating training effect sizes for Chunk 4001 of 12458
Scan 6: estimating training effect sizes for Chunk 4201 of 12458
Scan 6: estimating training effect sizes for Chunk 4401 of 12458
Scan 6: estimating training effect sizes for Chunk 4601 of 12458
Scan 6: estimating training effect sizes for Chunk 4801 of 12458
Scan 6: estimating training effect sizes for Chunk 5001 of 12458
Scan 6: estimating training effect sizes for Chunk 5201 of 12458
Scan 6: estimating training effect sizes for Chunk 5401 of 12458
Scan 6: estimating training effect sizes for Chunk 5601 of 12458
Scan 6: estimating training effect sizes for Chunk 5801 of 12458
Scan 6: estimating training effect sizes for Chunk 6001 of 12458
Scan 6: estimating training effect sizes for Chunk 6201 of 12458
Scan 6: estimating training effect sizes for Chunk 6401 of 12458
Scan 6: estimating training effect sizes for Chunk 6601 of 12458
Scan 6: estimating training effect sizes for Chunk 6801 of 12458
Scan 6: estimating training effect sizes for Chunk 7001 of 12458
Scan 6: estimating training effect sizes for Chunk 7201 of 12458
Scan 6: estimating training effect sizes for Chunk 7401 of 12458
Scan 6: estimating training effect sizes for Chunk 7601 of 12458
Scan 6: estimating training effect sizes for Chunk 7801 of 12458
Scan 6: estimating training effect sizes for Chunk 8001 of 12458
Scan 6: estimating training effect sizes for Chunk 8201 of 12458
Scan 6: estimating training effect sizes for Chunk 8401 of 12458
Scan 6: estimating training effect sizes for Chunk 8601 of 12458
Scan 6: estimating training effect sizes for Chunk 8801 of 12458
Scan 6: estimating training effect sizes for Chunk 9001 of 12458
Scan 6: estimating training effect sizes for Chunk 9201 of 12458
Scan 6: estimating training effect sizes for Chunk 9401 of 12458
Scan 6: estimating training effect sizes for Chunk 9601 of 12458
Scan 6: estimating training effect sizes for Chunk 9801 of 12458
Scan 6: estimating training effect sizes for Chunk 10001 of 12458
Scan 6: estimating training effect sizes for Chunk 10201 of 12458
Scan 6: estimating training effect sizes for Chunk 10401 of 12458
Scan 6: estimating training effect sizes for Chunk 10601 of 12458
Scan 6: estimating training effect sizes for Chunk 10801 of 12458
Scan 6: estimating training effect sizes for Chunk 11001 of 12458
Scan 6: estimating training effect sizes for Chunk 11201 of 12458
Scan 6: estimating training effect sizes for Chunk 11401 of 12458
Scan 6: estimating training effect sizes for Chunk 11601 of 12458
Scan 6: estimating training effect sizes for Chunk 11801 of 12458
Scan 6: estimating training effect sizes for Chunk 12001 of 12458
Scan 6: estimating training effect sizes for Chunk 12201 of 12458
Scan 6: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.27

Scan 7: estimating training effect sizes for Chunk 1 of 12458
Scan 7: estimating training effect sizes for Chunk 201 of 12458
Scan 7: estimating training effect sizes for Chunk 401 of 12458
Scan 7: estimating training effect sizes for Chunk 601 of 12458
Scan 7: estimating training effect sizes for Chunk 801 of 12458
Scan 7: estimating training effect sizes for Chunk 1001 of 12458
Scan 7: estimating training effect sizes for Chunk 1201 of 12458
Scan 7: estimating training effect sizes for Chunk 1401 of 12458
Scan 7: estimating training effect sizes for Chunk 1601 of 12458
Scan 7: estimating training effect sizes for Chunk 1801 of 12458
Scan 7: estimating training effect sizes for Chunk 2001 of 12458
Scan 7: estimating training effect sizes for Chunk 2201 of 12458
Scan 7: estimating training effect sizes for Chunk 2401 of 12458
Scan 7: estimating training effect sizes for Chunk 2601 of 12458
Scan 7: estimating training effect sizes for Chunk 2801 of 12458
Scan 7: estimating training effect sizes for Chunk 3001 of 12458
Scan 7: estimating training effect sizes for Chunk 3201 of 12458
Scan 7: estimating training effect sizes for Chunk 3401 of 12458
Scan 7: estimating training effect sizes for Chunk 3601 of 12458
Scan 7: estimating training effect sizes for Chunk 3801 of 12458
Scan 7: estimating training effect sizes for Chunk 4001 of 12458
Scan 7: estimating training effect sizes for Chunk 4201 of 12458
Scan 7: estimating training effect sizes for Chunk 4401 of 12458
Scan 7: estimating training effect sizes for Chunk 4601 of 12458
Scan 7: estimating training effect sizes for Chunk 4801 of 12458
Scan 7: estimating training effect sizes for Chunk 5001 of 12458
Scan 7: estimating training effect sizes for Chunk 5201 of 12458
Scan 7: estimating training effect sizes for Chunk 5401 of 12458
Scan 7: estimating training effect sizes for Chunk 5601 of 12458
Scan 7: estimating training effect sizes for Chunk 5801 of 12458
Scan 7: estimating training effect sizes for Chunk 6001 of 12458
Scan 7: estimating training effect sizes for Chunk 6201 of 12458
Scan 7: estimating training effect sizes for Chunk 6401 of 12458
Scan 7: estimating training effect sizes for Chunk 6601 of 12458
Scan 7: estimating training effect sizes for Chunk 6801 of 12458
Scan 7: estimating training effect sizes for Chunk 7001 of 12458
Scan 7: estimating training effect sizes for Chunk 7201 of 12458
Scan 7: estimating training effect sizes for Chunk 7401 of 12458
Scan 7: estimating training effect sizes for Chunk 7601 of 12458
Scan 7: estimating training effect sizes for Chunk 7801 of 12458
Scan 7: estimating training effect sizes for Chunk 8001 of 12458
Scan 7: estimating training effect sizes for Chunk 8201 of 12458
Scan 7: estimating training effect sizes for Chunk 8401 of 12458
Scan 7: estimating training effect sizes for Chunk 8601 of 12458
Scan 7: estimating training effect sizes for Chunk 8801 of 12458
Scan 7: estimating training effect sizes for Chunk 9001 of 12458
Scan 7: estimating training effect sizes for Chunk 9201 of 12458
Scan 7: estimating training effect sizes for Chunk 9401 of 12458
Scan 7: estimating training effect sizes for Chunk 9601 of 12458
Scan 7: estimating training effect sizes for Chunk 9801 of 12458
Scan 7: estimating training effect sizes for Chunk 10001 of 12458
Scan 7: estimating training effect sizes for Chunk 10201 of 12458
Scan 7: estimating training effect sizes for Chunk 10401 of 12458
Scan 7: estimating training effect sizes for Chunk 10601 of 12458
Scan 7: estimating training effect sizes for Chunk 10801 of 12458
Scan 7: estimating training effect sizes for Chunk 11001 of 12458
Scan 7: estimating training effect sizes for Chunk 11201 of 12458
Scan 7: estimating training effect sizes for Chunk 11401 of 12458
Scan 7: estimating training effect sizes for Chunk 11601 of 12458
Scan 7: estimating training effect sizes for Chunk 11801 of 12458
Scan 7: estimating training effect sizes for Chunk 12001 of 12458
Scan 7: estimating training effect sizes for Chunk 12201 of 12458
Scan 7: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.19

Scan 8: estimating training effect sizes for Chunk 1 of 12458
Scan 8: estimating training effect sizes for Chunk 201 of 12458
Scan 8: estimating training effect sizes for Chunk 401 of 12458
Scan 8: estimating training effect sizes for Chunk 601 of 12458
Scan 8: estimating training effect sizes for Chunk 801 of 12458
Scan 8: estimating training effect sizes for Chunk 1001 of 12458
Scan 8: estimating training effect sizes for Chunk 1201 of 12458
Scan 8: estimating training effect sizes for Chunk 1401 of 12458
Scan 8: estimating training effect sizes for Chunk 1601 of 12458
Scan 8: estimating training effect sizes for Chunk 1801 of 12458
Scan 8: estimating training effect sizes for Chunk 2001 of 12458
Scan 8: estimating training effect sizes for Chunk 2201 of 12458
Scan 8: estimating training effect sizes for Chunk 2401 of 12458
Scan 8: estimating training effect sizes for Chunk 2601 of 12458
Scan 8: estimating training effect sizes for Chunk 2801 of 12458
Scan 8: estimating training effect sizes for Chunk 3001 of 12458
Scan 8: estimating training effect sizes for Chunk 3201 of 12458
Scan 8: estimating training effect sizes for Chunk 3401 of 12458
Scan 8: estimating training effect sizes for Chunk 3601 of 12458
Scan 8: estimating training effect sizes for Chunk 3801 of 12458
Scan 8: estimating training effect sizes for Chunk 4001 of 12458
Scan 8: estimating training effect sizes for Chunk 4201 of 12458
Scan 8: estimating training effect sizes for Chunk 4401 of 12458
Scan 8: estimating training effect sizes for Chunk 4601 of 12458
Scan 8: estimating training effect sizes for Chunk 4801 of 12458
Scan 8: estimating training effect sizes for Chunk 5001 of 12458
Scan 8: estimating training effect sizes for Chunk 5201 of 12458
Scan 8: estimating training effect sizes for Chunk 5401 of 12458
Scan 8: estimating training effect sizes for Chunk 5601 of 12458
Scan 8: estimating training effect sizes for Chunk 5801 of 12458
Scan 8: estimating training effect sizes for Chunk 6001 of 12458
Scan 8: estimating training effect sizes for Chunk 6201 of 12458
Scan 8: estimating training effect sizes for Chunk 6401 of 12458
Scan 8: estimating training effect sizes for Chunk 6601 of 12458
Scan 8: estimating training effect sizes for Chunk 6801 of 12458
Scan 8: estimating training effect sizes for Chunk 7001 of 12458
Scan 8: estimating training effect sizes for Chunk 7201 of 12458
Scan 8: estimating training effect sizes for Chunk 7401 of 12458
Scan 8: estimating training effect sizes for Chunk 7601 of 12458
Scan 8: estimating training effect sizes for Chunk 7801 of 12458
Scan 8: estimating training effect sizes for Chunk 8001 of 12458
Scan 8: estimating training effect sizes for Chunk 8201 of 12458
Scan 8: estimating training effect sizes for Chunk 8401 of 12458
Scan 8: estimating training effect sizes for Chunk 8601 of 12458
Scan 8: estimating training effect sizes for Chunk 8801 of 12458
Scan 8: estimating training effect sizes for Chunk 9001 of 12458
Scan 8: estimating training effect sizes for Chunk 9201 of 12458
Scan 8: estimating training effect sizes for Chunk 9401 of 12458
Scan 8: estimating training effect sizes for Chunk 9601 of 12458
Scan 8: estimating training effect sizes for Chunk 9801 of 12458
Scan 8: estimating training effect sizes for Chunk 10001 of 12458
Scan 8: estimating training effect sizes for Chunk 10201 of 12458
Scan 8: estimating training effect sizes for Chunk 10401 of 12458
Scan 8: estimating training effect sizes for Chunk 10601 of 12458
Scan 8: estimating training effect sizes for Chunk 10801 of 12458
Scan 8: estimating training effect sizes for Chunk 11001 of 12458
Scan 8: estimating training effect sizes for Chunk 11201 of 12458
Scan 8: estimating training effect sizes for Chunk 11401 of 12458
Scan 8: estimating training effect sizes for Chunk 11601 of 12458
Scan 8: estimating training effect sizes for Chunk 11801 of 12458
Scan 8: estimating training effect sizes for Chunk 12001 of 12458
Scan 8: estimating training effect sizes for Chunk 12201 of 12458
Scan 8: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.15

Scan 9: estimating training effect sizes for Chunk 1 of 12458
Scan 9: estimating training effect sizes for Chunk 201 of 12458
Scan 9: estimating training effect sizes for Chunk 401 of 12458
Scan 9: estimating training effect sizes for Chunk 601 of 12458
Scan 9: estimating training effect sizes for Chunk 801 of 12458
Scan 9: estimating training effect sizes for Chunk 1001 of 12458
Scan 9: estimating training effect sizes for Chunk 1201 of 12458
Scan 9: estimating training effect sizes for Chunk 1401 of 12458
Scan 9: estimating training effect sizes for Chunk 1601 of 12458
Scan 9: estimating training effect sizes for Chunk 1801 of 12458
Scan 9: estimating training effect sizes for Chunk 2001 of 12458
Scan 9: estimating training effect sizes for Chunk 2201 of 12458
Scan 9: estimating training effect sizes for Chunk 2401 of 12458
Scan 9: estimating training effect sizes for Chunk 2601 of 12458
Scan 9: estimating training effect sizes for Chunk 2801 of 12458
Scan 9: estimating training effect sizes for Chunk 3001 of 12458
Scan 9: estimating training effect sizes for Chunk 3201 of 12458
Scan 9: estimating training effect sizes for Chunk 3401 of 12458
Scan 9: estimating training effect sizes for Chunk 3601 of 12458
Scan 9: estimating training effect sizes for Chunk 3801 of 12458
Scan 9: estimating training effect sizes for Chunk 4001 of 12458
Scan 9: estimating training effect sizes for Chunk 4201 of 12458
Scan 9: estimating training effect sizes for Chunk 4401 of 12458
Scan 9: estimating training effect sizes for Chunk 4601 of 12458
Scan 9: estimating training effect sizes for Chunk 4801 of 12458
Scan 9: estimating training effect sizes for Chunk 5001 of 12458
Scan 9: estimating training effect sizes for Chunk 5201 of 12458
Scan 9: estimating training effect sizes for Chunk 5401 of 12458
Scan 9: estimating training effect sizes for Chunk 5601 of 12458
Scan 9: estimating training effect sizes for Chunk 5801 of 12458
Scan 9: estimating training effect sizes for Chunk 6001 of 12458
Scan 9: estimating training effect sizes for Chunk 6201 of 12458
Scan 9: estimating training effect sizes for Chunk 6401 of 12458
Scan 9: estimating training effect sizes for Chunk 6601 of 12458
Scan 9: estimating training effect sizes for Chunk 6801 of 12458
Scan 9: estimating training effect sizes for Chunk 7001 of 12458
Scan 9: estimating training effect sizes for Chunk 7201 of 12458
Scan 9: estimating training effect sizes for Chunk 7401 of 12458
Scan 9: estimating training effect sizes for Chunk 7601 of 12458
Scan 9: estimating training effect sizes for Chunk 7801 of 12458
Scan 9: estimating training effect sizes for Chunk 8001 of 12458
Scan 9: estimating training effect sizes for Chunk 8201 of 12458
Scan 9: estimating training effect sizes for Chunk 8401 of 12458
Scan 9: estimating training effect sizes for Chunk 8601 of 12458
Scan 9: estimating training effect sizes for Chunk 8801 of 12458
Scan 9: estimating training effect sizes for Chunk 9001 of 12458
Scan 9: estimating training effect sizes for Chunk 9201 of 12458
Scan 9: estimating training effect sizes for Chunk 9401 of 12458
Scan 9: estimating training effect sizes for Chunk 9601 of 12458
Scan 9: estimating training effect sizes for Chunk 9801 of 12458
Scan 9: estimating training effect sizes for Chunk 10001 of 12458
Scan 9: estimating training effect sizes for Chunk 10201 of 12458
Scan 9: estimating training effect sizes for Chunk 10401 of 12458
Scan 9: estimating training effect sizes for Chunk 10601 of 12458
Scan 9: estimating training effect sizes for Chunk 10801 of 12458
Scan 9: estimating training effect sizes for Chunk 11001 of 12458
Scan 9: estimating training effect sizes for Chunk 11201 of 12458
Scan 9: estimating training effect sizes for Chunk 11401 of 12458
Scan 9: estimating training effect sizes for Chunk 11601 of 12458
Scan 9: estimating training effect sizes for Chunk 11801 of 12458
Scan 9: estimating training effect sizes for Chunk 12001 of 12458
Scan 9: estimating training effect sizes for Chunk 12201 of 12458
Scan 9: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.14

Scan 10: estimating training effect sizes for Chunk 1 of 12458
Scan 10: estimating training effect sizes for Chunk 201 of 12458
Scan 10: estimating training effect sizes for Chunk 401 of 12458
Scan 10: estimating training effect sizes for Chunk 601 of 12458
Scan 10: estimating training effect sizes for Chunk 801 of 12458
Scan 10: estimating training effect sizes for Chunk 1001 of 12458
Scan 10: estimating training effect sizes for Chunk 1201 of 12458
Scan 10: estimating training effect sizes for Chunk 1401 of 12458
Scan 10: estimating training effect sizes for Chunk 1601 of 12458
Scan 10: estimating training effect sizes for Chunk 1801 of 12458
Scan 10: estimating training effect sizes for Chunk 2001 of 12458
Scan 10: estimating training effect sizes for Chunk 2201 of 12458
Scan 10: estimating training effect sizes for Chunk 2401 of 12458
Scan 10: estimating training effect sizes for Chunk 2601 of 12458
Scan 10: estimating training effect sizes for Chunk 2801 of 12458
Scan 10: estimating training effect sizes for Chunk 3001 of 12458
Scan 10: estimating training effect sizes for Chunk 3201 of 12458
Scan 10: estimating training effect sizes for Chunk 3401 of 12458
Scan 10: estimating training effect sizes for Chunk 3601 of 12458
Scan 10: estimating training effect sizes for Chunk 3801 of 12458
Scan 10: estimating training effect sizes for Chunk 4001 of 12458
Scan 10: estimating training effect sizes for Chunk 4201 of 12458
Scan 10: estimating training effect sizes for Chunk 4401 of 12458
Scan 10: estimating training effect sizes for Chunk 4601 of 12458
Scan 10: estimating training effect sizes for Chunk 4801 of 12458
Scan 10: estimating training effect sizes for Chunk 5001 of 12458
Scan 10: estimating training effect sizes for Chunk 5201 of 12458
Scan 10: estimating training effect sizes for Chunk 5401 of 12458
Scan 10: estimating training effect sizes for Chunk 5601 of 12458
Scan 10: estimating training effect sizes for Chunk 5801 of 12458
Scan 10: estimating training effect sizes for Chunk 6001 of 12458
Scan 10: estimating training effect sizes for Chunk 6201 of 12458
Scan 10: estimating training effect sizes for Chunk 6401 of 12458
Scan 10: estimating training effect sizes for Chunk 6601 of 12458
Scan 10: estimating training effect sizes for Chunk 6801 of 12458
Scan 10: estimating training effect sizes for Chunk 7001 of 12458
Scan 10: estimating training effect sizes for Chunk 7201 of 12458
Scan 10: estimating training effect sizes for Chunk 7401 of 12458
Scan 10: estimating training effect sizes for Chunk 7601 of 12458
Scan 10: estimating training effect sizes for Chunk 7801 of 12458
Scan 10: estimating training effect sizes for Chunk 8001 of 12458
Scan 10: estimating training effect sizes for Chunk 8201 of 12458
Scan 10: estimating training effect sizes for Chunk 8401 of 12458
Scan 10: estimating training effect sizes for Chunk 8601 of 12458
Scan 10: estimating training effect sizes for Chunk 8801 of 12458
Scan 10: estimating training effect sizes for Chunk 9001 of 12458
Scan 10: estimating training effect sizes for Chunk 9201 of 12458
Scan 10: estimating training effect sizes for Chunk 9401 of 12458
Scan 10: estimating training effect sizes for Chunk 9601 of 12458
Scan 10: estimating training effect sizes for Chunk 9801 of 12458
Scan 10: estimating training effect sizes for Chunk 10001 of 12458
Scan 10: estimating training effect sizes for Chunk 10201 of 12458
Scan 10: estimating training effect sizes for Chunk 10401 of 12458
Scan 10: estimating training effect sizes for Chunk 10601 of 12458
Scan 10: estimating training effect sizes for Chunk 10801 of 12458
Scan 10: estimating training effect sizes for Chunk 11001 of 12458
Scan 10: estimating training effect sizes for Chunk 11201 of 12458
Scan 10: estimating training effect sizes for Chunk 11401 of 12458
Scan 10: estimating training effect sizes for Chunk 11601 of 12458
Scan 10: estimating training effect sizes for Chunk 11801 of 12458
Scan 10: estimating training effect sizes for Chunk 12001 of 12458
Scan 10: estimating training effect sizes for Chunk 12201 of 12458
Scan 10: estimating training effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.12

Warning, Variational Bayes did not converge after 10 scans (this is not normally a problem)

The revised estimate of heritability is 0.8000

Measuring accuracy of each model
Model 1: heritability 0.8000, p 0.0000, f2 1.0000, mean squared error 0.9985
Model 2: heritability 0.8000, p 0.5000, f2 0.5000, mean squared error 0.9984
Model 3: heritability 0.8000, p 0.5000, f2 0.3000, mean squared error 1.0012
Model 4: heritability 0.8000, p 0.5000, f2 0.1000, mean squared error 1.0064
Model 5: heritability 0.8000, p 0.1000, f2 0.5000, mean squared error 0.9998
Model 6: heritability 0.8000, p 0.1000, f2 0.3000, mean squared error 1.0012
Model 7: heritability 0.8000, p 0.1000, f2 0.1000, mean squared error 1.0086
Model 8: heritability 0.8000, p 0.0100, f2 0.5000, mean squared error 0.9987
Model 9: heritability 0.8000, p 0.0100, f2 0.3000, mean squared error 0.9989
Model 10: heritability 0.8000, p 0.0100, f2 0.1000, mean squared error 0.9991

Time check: have so far spent 0.18 hours

Constructing final PRS (heritability 0.8000, p 0.5000, f2 0.5000) using all samples

Scan 1: estimating final effect sizes for Chunk 1 of 12458
Scan 1: estimating final effect sizes for Chunk 201 of 12458
Scan 1: estimating final effect sizes for Chunk 401 of 12458
Scan 1: estimating final effect sizes for Chunk 601 of 12458
Scan 1: estimating final effect sizes for Chunk 801 of 12458
Scan 1: estimating final effect sizes for Chunk 1001 of 12458
Scan 1: estimating final effect sizes for Chunk 1201 of 12458
Scan 1: estimating final effect sizes for Chunk 1401 of 12458
Scan 1: estimating final effect sizes for Chunk 1601 of 12458
Scan 1: estimating final effect sizes for Chunk 1801 of 12458
Scan 1: estimating final effect sizes for Chunk 2001 of 12458
Scan 1: estimating final effect sizes for Chunk 2201 of 12458
Scan 1: estimating final effect sizes for Chunk 2401 of 12458
Scan 1: estimating final effect sizes for Chunk 2601 of 12458
Scan 1: estimating final effect sizes for Chunk 2801 of 12458
Scan 1: estimating final effect sizes for Chunk 3001 of 12458
Scan 1: estimating final effect sizes for Chunk 3201 of 12458
Scan 1: estimating final effect sizes for Chunk 3401 of 12458
Scan 1: estimating final effect sizes for Chunk 3601 of 12458
Scan 1: estimating final effect sizes for Chunk 3801 of 12458
Scan 1: estimating final effect sizes for Chunk 4001 of 12458
Scan 1: estimating final effect sizes for Chunk 4201 of 12458
Scan 1: estimating final effect sizes for Chunk 4401 of 12458
Scan 1: estimating final effect sizes for Chunk 4601 of 12458
Scan 1: estimating final effect sizes for Chunk 4801 of 12458
Scan 1: estimating final effect sizes for Chunk 5001 of 12458
Scan 1: estimating final effect sizes for Chunk 5201 of 12458
Scan 1: estimating final effect sizes for Chunk 5401 of 12458
Scan 1: estimating final effect sizes for Chunk 5601 of 12458
Scan 1: estimating final effect sizes for Chunk 5801 of 12458
Scan 1: estimating final effect sizes for Chunk 6001 of 12458
Scan 1: estimating final effect sizes for Chunk 6201 of 12458
Scan 1: estimating final effect sizes for Chunk 6401 of 12458
Scan 1: estimating final effect sizes for Chunk 6601 of 12458
Scan 1: estimating final effect sizes for Chunk 6801 of 12458
Scan 1: estimating final effect sizes for Chunk 7001 of 12458
Scan 1: estimating final effect sizes for Chunk 7201 of 12458
Scan 1: estimating final effect sizes for Chunk 7401 of 12458
Scan 1: estimating final effect sizes for Chunk 7601 of 12458
Scan 1: estimating final effect sizes for Chunk 7801 of 12458
Scan 1: estimating final effect sizes for Chunk 8001 of 12458
Scan 1: estimating final effect sizes for Chunk 8201 of 12458
Scan 1: estimating final effect sizes for Chunk 8401 of 12458
Scan 1: estimating final effect sizes for Chunk 8601 of 12458
Scan 1: estimating final effect sizes for Chunk 8801 of 12458
Scan 1: estimating final effect sizes for Chunk 9001 of 12458
Scan 1: estimating final effect sizes for Chunk 9201 of 12458
Scan 1: estimating final effect sizes for Chunk 9401 of 12458
Scan 1: estimating final effect sizes for Chunk 9601 of 12458
Scan 1: estimating final effect sizes for Chunk 9801 of 12458
Scan 1: estimating final effect sizes for Chunk 10001 of 12458
Scan 1: estimating final effect sizes for Chunk 10201 of 12458
Scan 1: estimating final effect sizes for Chunk 10401 of 12458
Scan 1: estimating final effect sizes for Chunk 10601 of 12458
Scan 1: estimating final effect sizes for Chunk 10801 of 12458
Scan 1: estimating final effect sizes for Chunk 11001 of 12458
Scan 1: estimating final effect sizes for Chunk 11201 of 12458
Scan 1: estimating final effect sizes for Chunk 11401 of 12458
Scan 1: estimating final effect sizes for Chunk 11601 of 12458
Scan 1: estimating final effect sizes for Chunk 11801 of 12458
Scan 1: estimating final effect sizes for Chunk 12001 of 12458
Scan 1: estimating final effect sizes for Chunk 12201 of 12458
Scan 1: estimating final effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.63

Scan 2: estimating final effect sizes for Chunk 1 of 12458
Scan 2: estimating final effect sizes for Chunk 201 of 12458
Scan 2: estimating final effect sizes for Chunk 401 of 12458
Scan 2: estimating final effect sizes for Chunk 601 of 12458
Scan 2: estimating final effect sizes for Chunk 801 of 12458
Scan 2: estimating final effect sizes for Chunk 1001 of 12458
Scan 2: estimating final effect sizes for Chunk 1201 of 12458
Scan 2: estimating final effect sizes for Chunk 1401 of 12458
Scan 2: estimating final effect sizes for Chunk 1601 of 12458
Scan 2: estimating final effect sizes for Chunk 1801 of 12458
Scan 2: estimating final effect sizes for Chunk 2001 of 12458
Scan 2: estimating final effect sizes for Chunk 2201 of 12458
Scan 2: estimating final effect sizes for Chunk 2401 of 12458
Scan 2: estimating final effect sizes for Chunk 2601 of 12458
Scan 2: estimating final effect sizes for Chunk 2801 of 12458
Scan 2: estimating final effect sizes for Chunk 3001 of 12458
Scan 2: estimating final effect sizes for Chunk 3201 of 12458
Scan 2: estimating final effect sizes for Chunk 3401 of 12458
Scan 2: estimating final effect sizes for Chunk 3601 of 12458
Scan 2: estimating final effect sizes for Chunk 3801 of 12458
Scan 2: estimating final effect sizes for Chunk 4001 of 12458
Scan 2: estimating final effect sizes for Chunk 4201 of 12458
Scan 2: estimating final effect sizes for Chunk 4401 of 12458
Scan 2: estimating final effect sizes for Chunk 4601 of 12458
Scan 2: estimating final effect sizes for Chunk 4801 of 12458
Scan 2: estimating final effect sizes for Chunk 5001 of 12458
Scan 2: estimating final effect sizes for Chunk 5201 of 12458
Scan 2: estimating final effect sizes for Chunk 5401 of 12458
Scan 2: estimating final effect sizes for Chunk 5601 of 12458
Scan 2: estimating final effect sizes for Chunk 5801 of 12458
Scan 2: estimating final effect sizes for Chunk 6001 of 12458
Scan 2: estimating final effect sizes for Chunk 6201 of 12458
Scan 2: estimating final effect sizes for Chunk 6401 of 12458
Scan 2: estimating final effect sizes for Chunk 6601 of 12458
Scan 2: estimating final effect sizes for Chunk 6801 of 12458
Scan 2: estimating final effect sizes for Chunk 7001 of 12458
Scan 2: estimating final effect sizes for Chunk 7201 of 12458
Scan 2: estimating final effect sizes for Chunk 7401 of 12458
Scan 2: estimating final effect sizes for Chunk 7601 of 12458
Scan 2: estimating final effect sizes for Chunk 7801 of 12458
Scan 2: estimating final effect sizes for Chunk 8001 of 12458
Scan 2: estimating final effect sizes for Chunk 8201 of 12458
Scan 2: estimating final effect sizes for Chunk 8401 of 12458
Scan 2: estimating final effect sizes for Chunk 8601 of 12458
Scan 2: estimating final effect sizes for Chunk 8801 of 12458
Scan 2: estimating final effect sizes for Chunk 9001 of 12458
Scan 2: estimating final effect sizes for Chunk 9201 of 12458
Scan 2: estimating final effect sizes for Chunk 9401 of 12458
Scan 2: estimating final effect sizes for Chunk 9601 of 12458
Scan 2: estimating final effect sizes for Chunk 9801 of 12458
Scan 2: estimating final effect sizes for Chunk 10001 of 12458
Scan 2: estimating final effect sizes for Chunk 10201 of 12458
Scan 2: estimating final effect sizes for Chunk 10401 of 12458
Scan 2: estimating final effect sizes for Chunk 10601 of 12458
Scan 2: estimating final effect sizes for Chunk 10801 of 12458
Scan 2: estimating final effect sizes for Chunk 11001 of 12458
Scan 2: estimating final effect sizes for Chunk 11201 of 12458
Scan 2: estimating final effect sizes for Chunk 11401 of 12458
Scan 2: estimating final effect sizes for Chunk 11601 of 12458
Scan 2: estimating final effect sizes for Chunk 11801 of 12458
Scan 2: estimating final effect sizes for Chunk 12001 of 12458
Scan 2: estimating final effect sizes for Chunk 12201 of 12458
Scan 2: estimating final effect sizes for Chunk 12401 of 12458
Average number of iterations per chunk: 2.01

Scan 3: estimating final effect sizes for Chunk 1 of 8226
Scan 3: estimating final effect sizes for Chunk 201 of 8226
Scan 3: estimating final effect sizes for Chunk 401 of 8226
Scan 3: estimating final effect sizes for Chunk 601 of 8226
Scan 3: estimating final effect sizes for Chunk 801 of 8226
Scan 3: estimating final effect sizes for Chunk 1001 of 8226
Scan 3: estimating final effect sizes for Chunk 1201 of 8226
Scan 3: estimating final effect sizes for Chunk 1401 of 8226
Scan 3: estimating final effect sizes for Chunk 1601 of 8226
Scan 3: estimating final effect sizes for Chunk 1801 of 8226
Scan 3: estimating final effect sizes for Chunk 2001 of 8226
Scan 3: estimating final effect sizes for Chunk 2201 of 8226
Scan 3: estimating final effect sizes for Chunk 2401 of 8226
Scan 3: estimating final effect sizes for Chunk 2601 of 8226
Scan 3: estimating final effect sizes for Chunk 2801 of 8226
Scan 3: estimating final effect sizes for Chunk 3001 of 8226
Scan 3: estimating final effect sizes for Chunk 3201 of 8226
Scan 3: estimating final effect sizes for Chunk 3401 of 8226
Scan 3: estimating final effect sizes for Chunk 3601 of 8226
Scan 3: estimating final effect sizes for Chunk 3801 of 8226
Scan 3: estimating final effect sizes for Chunk 4001 of 8226
Scan 3: estimating final effect sizes for Chunk 4201 of 8226
Scan 3: estimating final effect sizes for Chunk 4401 of 8226
Scan 3: estimating final effect sizes for Chunk 4601 of 8226
Scan 3: estimating final effect sizes for Chunk 4801 of 8226
Scan 3: estimating final effect sizes for Chunk 5001 of 8226
Scan 3: estimating final effect sizes for Chunk 5201 of 8226
Scan 3: estimating final effect sizes for Chunk 5401 of 8226
Scan 3: estimating final effect sizes for Chunk 5601 of 8226
Scan 3: estimating final effect sizes for Chunk 5801 of 8226
Scan 3: estimating final effect sizes for Chunk 6001 of 8226
Scan 3: estimating final effect sizes for Chunk 6201 of 8226
Scan 3: estimating final effect sizes for Chunk 6401 of 8226
Scan 3: estimating final effect sizes for Chunk 6601 of 8226
Scan 3: estimating final effect sizes for Chunk 6801 of 8226
Scan 3: estimating final effect sizes for Chunk 7001 of 8226
Scan 3: estimating final effect sizes for Chunk 7201 of 8226
Scan 3: estimating final effect sizes for Chunk 7401 of 8226
Scan 3: estimating final effect sizes for Chunk 7601 of 8226
Scan 3: estimating final effect sizes for Chunk 7801 of 8226
Scan 3: estimating final effect sizes for Chunk 8001 of 8226
Scan 3: estimating final effect sizes for Chunk 8201 of 8226
Average number of iterations per chunk: 2.01

Scan 4: estimating final effect sizes for Chunk 1 of 5017
Scan 4: estimating final effect sizes for Chunk 201 of 5017
Scan 4: estimating final effect sizes for Chunk 401 of 5017
Scan 4: estimating final effect sizes for Chunk 601 of 5017
Scan 4: estimating final effect sizes for Chunk 801 of 5017
Scan 4: estimating final effect sizes for Chunk 1001 of 5017
Scan 4: estimating final effect sizes for Chunk 1201 of 5017
Scan 4: estimating final effect sizes for Chunk 1401 of 5017
Scan 4: estimating final effect sizes for Chunk 1601 of 5017
Scan 4: estimating final effect sizes for Chunk 1801 of 5017
Scan 4: estimating final effect sizes for Chunk 2001 of 5017
Scan 4: estimating final effect sizes for Chunk 2201 of 5017
Scan 4: estimating final effect sizes for Chunk 2401 of 5017
Scan 4: estimating final effect sizes for Chunk 2601 of 5017
Scan 4: estimating final effect sizes for Chunk 2801 of 5017
Scan 4: estimating final effect sizes for Chunk 3001 of 5017
Scan 4: estimating final effect sizes for Chunk 3201 of 5017
Scan 4: estimating final effect sizes for Chunk 3401 of 5017
Scan 4: estimating final effect sizes for Chunk 3601 of 5017
Scan 4: estimating final effect sizes for Chunk 3801 of 5017
Scan 4: estimating final effect sizes for Chunk 4001 of 5017
Scan 4: estimating final effect sizes for Chunk 4201 of 5017
Scan 4: estimating final effect sizes for Chunk 4401 of 5017
Scan 4: estimating final effect sizes for Chunk 4601 of 5017
Scan 4: estimating final effect sizes for Chunk 4801 of 5017
Scan 4: estimating final effect sizes for Chunk 5001 of 5017
Average number of iterations per chunk: 2.00

Scan 5: estimating final effect sizes for Chunk 1 of 2277
Scan 5: estimating final effect sizes for Chunk 201 of 2277
Scan 5: estimating final effect sizes for Chunk 401 of 2277
Scan 5: estimating final effect sizes for Chunk 601 of 2277
Scan 5: estimating final effect sizes for Chunk 801 of 2277
Scan 5: estimating final effect sizes for Chunk 1001 of 2277
Scan 5: estimating final effect sizes for Chunk 1201 of 2277
Scan 5: estimating final effect sizes for Chunk 1401 of 2277
Scan 5: estimating final effect sizes for Chunk 1601 of 2277
Scan 5: estimating final effect sizes for Chunk 1801 of 2277
Scan 5: estimating final effect sizes for Chunk 2001 of 2277
Scan 5: estimating final effect sizes for Chunk 2201 of 2277
Average number of iterations per chunk: 2.74

Scan 6: estimating final effect sizes for Chunk 1 of 1928
Scan 6: estimating final effect sizes for Chunk 201 of 1928
Scan 6: estimating final effect sizes for Chunk 401 of 1928
Scan 6: estimating final effect sizes for Chunk 601 of 1928
Scan 6: estimating final effect sizes for Chunk 801 of 1928
Scan 6: estimating final effect sizes for Chunk 1001 of 1928
Scan 6: estimating final effect sizes for Chunk 1201 of 1928
Scan 6: estimating final effect sizes for Chunk 1401 of 1928
Scan 6: estimating final effect sizes for Chunk 1601 of 1928
Scan 6: estimating final effect sizes for Chunk 1801 of 1928
Average number of iterations per chunk: 2.65

Scan 7: estimating final effect sizes for Chunk 1 of 1754
Scan 7: estimating final effect sizes for Chunk 201 of 1754
Scan 7: estimating final effect sizes for Chunk 401 of 1754
Scan 7: estimating final effect sizes for Chunk 601 of 1754
Scan 7: estimating final effect sizes for Chunk 801 of 1754
Scan 7: estimating final effect sizes for Chunk 1001 of 1754
Scan 7: estimating final effect sizes for Chunk 1201 of 1754
Scan 7: estimating final effect sizes for Chunk 1401 of 1754
Scan 7: estimating final effect sizes for Chunk 1601 of 1754
Average number of iterations per chunk: 2.29

Scan 8: estimating final effect sizes for Chunk 1 of 1509
Scan 8: estimating final effect sizes for Chunk 201 of 1509
Scan 8: estimating final effect sizes for Chunk 401 of 1509
Scan 8: estimating final effect sizes for Chunk 601 of 1509
Scan 8: estimating final effect sizes for Chunk 801 of 1509
Scan 8: estimating final effect sizes for Chunk 1001 of 1509
Scan 8: estimating final effect sizes for Chunk 1201 of 1509
Scan 8: estimating final effect sizes for Chunk 1401 of 1509
Average number of iterations per chunk: 2.09

Scan 9: estimating final effect sizes for Chunk 1 of 1141
Scan 9: estimating final effect sizes for Chunk 201 of 1141
Scan 9: estimating final effect sizes for Chunk 401 of 1141
Scan 9: estimating final effect sizes for Chunk 601 of 1141
Scan 9: estimating final effect sizes for Chunk 801 of 1141
Scan 9: estimating final effect sizes for Chunk 1001 of 1141
Average number of iterations per chunk: 2.10

Scan 10: estimating final effect sizes for Chunk 1 of 893
Scan 10: estimating final effect sizes for Chunk 201 of 893
Scan 10: estimating final effect sizes for Chunk 401 of 893
Scan 10: estimating final effect sizes for Chunk 601 of 893
Scan 10: estimating final effect sizes for Chunk 801 of 893
Average number of iterations per chunk: 2.25

Warning, Variational Bayes did not converge after 10 scans (this is not normally a problem)

Best-fitting model saved in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic.effects, with posterior probabilities in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic.probs

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:46:39 2025 and ended at Mon May 12 20:57:42 2025
The elapsed time was 0.18 hours
Given the command used one thread, this means the CPU time was also 0.18 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic.effects
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for one profile

Please note that ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic.effects is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 3189219 predictors from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic.effects

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 3200
Calculating scores for Chunk 51 of 3200
Calculating scores for Chunk 101 of 3200
Calculating scores for Chunk 151 of 3200
Calculating scores for Chunk 201 of 3200
Calculating scores for Chunk 251 of 3200
Calculating scores for Chunk 301 of 3200
Calculating scores for Chunk 351 of 3200
Calculating scores for Chunk 401 of 3200
Calculating scores for Chunk 451 of 3200
Calculating scores for Chunk 501 of 3200
Calculating scores for Chunk 551 of 3200
Calculating scores for Chunk 601 of 3200
Calculating scores for Chunk 651 of 3200
Calculating scores for Chunk 701 of 3200
Calculating scores for Chunk 751 of 3200
Calculating scores for Chunk 801 of 3200
Calculating scores for Chunk 851 of 3200
Calculating scores for Chunk 901 of 3200
Calculating scores for Chunk 951 of 3200
Calculating scores for Chunk 1001 of 3200
Calculating scores for Chunk 1051 of 3200
Calculating scores for Chunk 1101 of 3200
Calculating scores for Chunk 1151 of 3200
Calculating scores for Chunk 1201 of 3200
Calculating scores for Chunk 1251 of 3200
Calculating scores for Chunk 1301 of 3200
Calculating scores for Chunk 1351 of 3200
Calculating scores for Chunk 1401 of 3200
Calculating scores for Chunk 1451 of 3200
Calculating scores for Chunk 1501 of 3200
Calculating scores for Chunk 1551 of 3200
Calculating scores for Chunk 1601 of 3200
Calculating scores for Chunk 1651 of 3200
Calculating scores for Chunk 1701 of 3200
Calculating scores for Chunk 1751 of 3200
Calculating scores for Chunk 1801 of 3200
Calculating scores for Chunk 1851 of 3200
Calculating scores for Chunk 1901 of 3200
Calculating scores for Chunk 1951 of 3200
Calculating scores for Chunk 2001 of 3200
Calculating scores for Chunk 2051 of 3200
Calculating scores for Chunk 2101 of 3200
Calculating scores for Chunk 2151 of 3200
Calculating scores for Chunk 2201 of 3200
Calculating scores for Chunk 2251 of 3200
Calculating scores for Chunk 2301 of 3200
Calculating scores for Chunk 2351 of 3200
Calculating scores for Chunk 2401 of 3200
Calculating scores for Chunk 2451 of 3200
Calculating scores for Chunk 2501 of 3200
Calculating scores for Chunk 2551 of 3200
Calculating scores for Chunk 2601 of 3200
Calculating scores for Chunk 2651 of 3200
Calculating scores for Chunk 2701 of 3200
Calculating scores for Chunk 2751 of 3200
Calculating scores for Chunk 2801 of 3200
Calculating scores for Chunk 2851 of 3200
Calculating scores for Chunk 2901 of 3200
Calculating scores for Chunk 2951 of 3200
Calculating scores for Chunk 3001 of 3200
Calculating scores for Chunk 3051 of 3200
Calculating scores for Chunk 3101 of 3200
Calculating scores for Chunk 3151 of 3200

Profile saved in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic_prs_calc_with_pheno.profile

Correlation between score and phenotype is 0.8782, saved in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:57:42 2025 and ended at Mon May 12 20:58:00 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic.effects
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for one profile

Please note that ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic.effects is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 3189219 predictors from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic.effects

Calculating scores for Chunk 1 of 3200
Calculating scores for Chunk 51 of 3200
Calculating scores for Chunk 101 of 3200
Calculating scores for Chunk 151 of 3200
Calculating scores for Chunk 201 of 3200
Calculating scores for Chunk 251 of 3200
Calculating scores for Chunk 301 of 3200
Calculating scores for Chunk 351 of 3200
Calculating scores for Chunk 401 of 3200
Calculating scores for Chunk 451 of 3200
Calculating scores for Chunk 501 of 3200
Calculating scores for Chunk 551 of 3200
Calculating scores for Chunk 601 of 3200
Calculating scores for Chunk 651 of 3200
Calculating scores for Chunk 701 of 3200
Calculating scores for Chunk 751 of 3200
Calculating scores for Chunk 801 of 3200
Calculating scores for Chunk 851 of 3200
Calculating scores for Chunk 901 of 3200
Calculating scores for Chunk 951 of 3200
Calculating scores for Chunk 1001 of 3200
Calculating scores for Chunk 1051 of 3200
Calculating scores for Chunk 1101 of 3200
Calculating scores for Chunk 1151 of 3200
Calculating scores for Chunk 1201 of 3200
Calculating scores for Chunk 1251 of 3200
Calculating scores for Chunk 1301 of 3200
Calculating scores for Chunk 1351 of 3200
Calculating scores for Chunk 1401 of 3200
Calculating scores for Chunk 1451 of 3200
Calculating scores for Chunk 1501 of 3200
Calculating scores for Chunk 1551 of 3200
Calculating scores for Chunk 1601 of 3200
Calculating scores for Chunk 1651 of 3200
Calculating scores for Chunk 1701 of 3200
Calculating scores for Chunk 1751 of 3200
Calculating scores for Chunk 1801 of 3200
Calculating scores for Chunk 1851 of 3200
Calculating scores for Chunk 1901 of 3200
Calculating scores for Chunk 1951 of 3200
Calculating scores for Chunk 2001 of 3200
Calculating scores for Chunk 2051 of 3200
Calculating scores for Chunk 2101 of 3200
Calculating scores for Chunk 2151 of 3200
Calculating scores for Chunk 2201 of 3200
Calculating scores for Chunk 2251 of 3200
Calculating scores for Chunk 2301 of 3200
Calculating scores for Chunk 2351 of 3200
Calculating scores for Chunk 2401 of 3200
Calculating scores for Chunk 2451 of 3200
Calculating scores for Chunk 2501 of 3200
Calculating scores for Chunk 2551 of 3200
Calculating scores for Chunk 2601 of 3200
Calculating scores for Chunk 2651 of 3200
Calculating scores for Chunk 2701 of 3200
Calculating scores for Chunk 2751 of 3200
Calculating scores for Chunk 2801 of 3200
Calculating scores for Chunk 2851 of 3200
Calculating scores for Chunk 2901 of 3200
Calculating scores for Chunk 2951 of 3200
Calculating scores for Chunk 3001 of 3200
Calculating scores for Chunk 3051 of 3200
Calculating scores for Chunk 3101 of 3200
Calculating scores for Chunk 3151 of 3200

Profile saved in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_elastic_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:58:00 2025 and ended at Mon May 12 20:58:19 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## check the PRS is the same ##


###### calculate a PRS using the classical approach ######

## calculate linear association between SNPs and the phenotype ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--linear ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 6 covariates for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.combined")

Performing linear regression for Chunk 1 of 3200
Performing linear regression for Chunk 11 of 3200
Performing linear regression for Chunk 21 of 3200
Performing linear regression for Chunk 31 of 3200
Performing linear regression for Chunk 41 of 3200
Performing linear regression for Chunk 51 of 3200
Performing linear regression for Chunk 61 of 3200
Performing linear regression for Chunk 71 of 3200
Performing linear regression for Chunk 81 of 3200
Performing linear regression for Chunk 91 of 3200
Performing linear regression for Chunk 101 of 3200
Performing linear regression for Chunk 111 of 3200
Performing linear regression for Chunk 121 of 3200
Performing linear regression for Chunk 131 of 3200
Performing linear regression for Chunk 141 of 3200
Performing linear regression for Chunk 151 of 3200
Performing linear regression for Chunk 161 of 3200
Performing linear regression for Chunk 171 of 3200
Performing linear regression for Chunk 181 of 3200
Performing linear regression for Chunk 191 of 3200
Performing linear regression for Chunk 201 of 3200
Performing linear regression for Chunk 211 of 3200
Performing linear regression for Chunk 221 of 3200
Performing linear regression for Chunk 231 of 3200
Performing linear regression for Chunk 241 of 3200
Performing linear regression for Chunk 251 of 3200
Performing linear regression for Chunk 261 of 3200
Performing linear regression for Chunk 271 of 3200
Performing linear regression for Chunk 281 of 3200
Performing linear regression for Chunk 291 of 3200
Performing linear regression for Chunk 301 of 3200
Performing linear regression for Chunk 311 of 3200
Performing linear regression for Chunk 321 of 3200
Performing linear regression for Chunk 331 of 3200
Performing linear regression for Chunk 341 of 3200
Performing linear regression for Chunk 351 of 3200
Performing linear regression for Chunk 361 of 3200
Performing linear regression for Chunk 371 of 3200
Performing linear regression for Chunk 381 of 3200
Performing linear regression for Chunk 391 of 3200
Performing linear regression for Chunk 401 of 3200
Performing linear regression for Chunk 411 of 3200
Performing linear regression for Chunk 421 of 3200
Performing linear regression for Chunk 431 of 3200
Performing linear regression for Chunk 441 of 3200
Performing linear regression for Chunk 451 of 3200
Performing linear regression for Chunk 461 of 3200
Performing linear regression for Chunk 471 of 3200
Performing linear regression for Chunk 481 of 3200
Performing linear regression for Chunk 491 of 3200
Performing linear regression for Chunk 501 of 3200
Performing linear regression for Chunk 511 of 3200
Performing linear regression for Chunk 521 of 3200
Performing linear regression for Chunk 531 of 3200
Performing linear regression for Chunk 541 of 3200
Performing linear regression for Chunk 551 of 3200
Performing linear regression for Chunk 561 of 3200
Performing linear regression for Chunk 571 of 3200
Performing linear regression for Chunk 581 of 3200
Performing linear regression for Chunk 591 of 3200
Performing linear regression for Chunk 601 of 3200
Performing linear regression for Chunk 611 of 3200
Performing linear regression for Chunk 621 of 3200
Performing linear regression for Chunk 631 of 3200
Performing linear regression for Chunk 641 of 3200
Performing linear regression for Chunk 651 of 3200
Performing linear regression for Chunk 661 of 3200
Performing linear regression for Chunk 671 of 3200
Performing linear regression for Chunk 681 of 3200
Performing linear regression for Chunk 691 of 3200
Performing linear regression for Chunk 701 of 3200
Performing linear regression for Chunk 711 of 3200
Performing linear regression for Chunk 721 of 3200
Performing linear regression for Chunk 731 of 3200
Performing linear regression for Chunk 741 of 3200
Performing linear regression for Chunk 751 of 3200
Performing linear regression for Chunk 761 of 3200
Performing linear regression for Chunk 771 of 3200
Performing linear regression for Chunk 781 of 3200
Performing linear regression for Chunk 791 of 3200
Performing linear regression for Chunk 801 of 3200
Performing linear regression for Chunk 811 of 3200
Performing linear regression for Chunk 821 of 3200
Performing linear regression for Chunk 831 of 3200
Performing linear regression for Chunk 841 of 3200
Performing linear regression for Chunk 851 of 3200
Performing linear regression for Chunk 861 of 3200
Performing linear regression for Chunk 871 of 3200
Performing linear regression for Chunk 881 of 3200
Performing linear regression for Chunk 891 of 3200
Performing linear regression for Chunk 901 of 3200
Performing linear regression for Chunk 911 of 3200
Performing linear regression for Chunk 921 of 3200
Performing linear regression for Chunk 931 of 3200
Performing linear regression for Chunk 941 of 3200
Performing linear regression for Chunk 951 of 3200
Performing linear regression for Chunk 961 of 3200
Performing linear regression for Chunk 971 of 3200
Performing linear regression for Chunk 981 of 3200
Performing linear regression for Chunk 991 of 3200
Performing linear regression for Chunk 1001 of 3200
Performing linear regression for Chunk 1011 of 3200
Performing linear regression for Chunk 1021 of 3200
Performing linear regression for Chunk 1031 of 3200
Performing linear regression for Chunk 1041 of 3200
Performing linear regression for Chunk 1051 of 3200
Performing linear regression for Chunk 1061 of 3200
Performing linear regression for Chunk 1071 of 3200
Performing linear regression for Chunk 1081 of 3200
Performing linear regression for Chunk 1091 of 3200
Performing linear regression for Chunk 1101 of 3200
Performing linear regression for Chunk 1111 of 3200
Performing linear regression for Chunk 1121 of 3200
Performing linear regression for Chunk 1131 of 3200
Performing linear regression for Chunk 1141 of 3200
Performing linear regression for Chunk 1151 of 3200
Performing linear regression for Chunk 1161 of 3200
Performing linear regression for Chunk 1171 of 3200
Performing linear regression for Chunk 1181 of 3200
Performing linear regression for Chunk 1191 of 3200
Performing linear regression for Chunk 1201 of 3200
Performing linear regression for Chunk 1211 of 3200
Performing linear regression for Chunk 1221 of 3200
Performing linear regression for Chunk 1231 of 3200
Performing linear regression for Chunk 1241 of 3200
Performing linear regression for Chunk 1251 of 3200
Performing linear regression for Chunk 1261 of 3200
Performing linear regression for Chunk 1271 of 3200
Performing linear regression for Chunk 1281 of 3200
Performing linear regression for Chunk 1291 of 3200
Performing linear regression for Chunk 1301 of 3200
Performing linear regression for Chunk 1311 of 3200
Performing linear regression for Chunk 1321 of 3200
Performing linear regression for Chunk 1331 of 3200
Performing linear regression for Chunk 1341 of 3200
Performing linear regression for Chunk 1351 of 3200
Performing linear regression for Chunk 1361 of 3200
Performing linear regression for Chunk 1371 of 3200
Performing linear regression for Chunk 1381 of 3200
Performing linear regression for Chunk 1391 of 3200
Performing linear regression for Chunk 1401 of 3200
Performing linear regression for Chunk 1411 of 3200
Performing linear regression for Chunk 1421 of 3200
Performing linear regression for Chunk 1431 of 3200
Performing linear regression for Chunk 1441 of 3200
Performing linear regression for Chunk 1451 of 3200
Performing linear regression for Chunk 1461 of 3200
Performing linear regression for Chunk 1471 of 3200
Performing linear regression for Chunk 1481 of 3200
Performing linear regression for Chunk 1491 of 3200
Performing linear regression for Chunk 1501 of 3200
Performing linear regression for Chunk 1511 of 3200
Performing linear regression for Chunk 1521 of 3200
Performing linear regression for Chunk 1531 of 3200
Performing linear regression for Chunk 1541 of 3200
Performing linear regression for Chunk 1551 of 3200
Performing linear regression for Chunk 1561 of 3200
Performing linear regression for Chunk 1571 of 3200
Performing linear regression for Chunk 1581 of 3200
Performing linear regression for Chunk 1591 of 3200
Performing linear regression for Chunk 1601 of 3200
Performing linear regression for Chunk 1611 of 3200
Performing linear regression for Chunk 1621 of 3200
Performing linear regression for Chunk 1631 of 3200
Performing linear regression for Chunk 1641 of 3200
Performing linear regression for Chunk 1651 of 3200
Performing linear regression for Chunk 1661 of 3200
Performing linear regression for Chunk 1671 of 3200
Performing linear regression for Chunk 1681 of 3200
Performing linear regression for Chunk 1691 of 3200
Performing linear regression for Chunk 1701 of 3200
Performing linear regression for Chunk 1711 of 3200
Performing linear regression for Chunk 1721 of 3200
Performing linear regression for Chunk 1731 of 3200
Performing linear regression for Chunk 1741 of 3200
Performing linear regression for Chunk 1751 of 3200
Performing linear regression for Chunk 1761 of 3200
Performing linear regression for Chunk 1771 of 3200
Performing linear regression for Chunk 1781 of 3200
Performing linear regression for Chunk 1791 of 3200
Performing linear regression for Chunk 1801 of 3200
Performing linear regression for Chunk 1811 of 3200
Performing linear regression for Chunk 1821 of 3200
Performing linear regression for Chunk 1831 of 3200
Performing linear regression for Chunk 1841 of 3200
Performing linear regression for Chunk 1851 of 3200
Performing linear regression for Chunk 1861 of 3200
Performing linear regression for Chunk 1871 of 3200
Performing linear regression for Chunk 1881 of 3200
Performing linear regression for Chunk 1891 of 3200
Performing linear regression for Chunk 1901 of 3200
Performing linear regression for Chunk 1911 of 3200
Performing linear regression for Chunk 1921 of 3200
Performing linear regression for Chunk 1931 of 3200
Performing linear regression for Chunk 1941 of 3200
Performing linear regression for Chunk 1951 of 3200
Performing linear regression for Chunk 1961 of 3200
Performing linear regression for Chunk 1971 of 3200
Performing linear regression for Chunk 1981 of 3200
Performing linear regression for Chunk 1991 of 3200
Performing linear regression for Chunk 2001 of 3200
Performing linear regression for Chunk 2011 of 3200
Performing linear regression for Chunk 2021 of 3200
Performing linear regression for Chunk 2031 of 3200
Performing linear regression for Chunk 2041 of 3200
Performing linear regression for Chunk 2051 of 3200
Performing linear regression for Chunk 2061 of 3200
Performing linear regression for Chunk 2071 of 3200
Performing linear regression for Chunk 2081 of 3200
Performing linear regression for Chunk 2091 of 3200
Performing linear regression for Chunk 2101 of 3200
Performing linear regression for Chunk 2111 of 3200
Performing linear regression for Chunk 2121 of 3200
Performing linear regression for Chunk 2131 of 3200
Performing linear regression for Chunk 2141 of 3200
Performing linear regression for Chunk 2151 of 3200
Performing linear regression for Chunk 2161 of 3200
Performing linear regression for Chunk 2171 of 3200
Performing linear regression for Chunk 2181 of 3200
Performing linear regression for Chunk 2191 of 3200
Performing linear regression for Chunk 2201 of 3200
Performing linear regression for Chunk 2211 of 3200
Performing linear regression for Chunk 2221 of 3200
Performing linear regression for Chunk 2231 of 3200
Performing linear regression for Chunk 2241 of 3200
Performing linear regression for Chunk 2251 of 3200
Performing linear regression for Chunk 2261 of 3200
Performing linear regression for Chunk 2271 of 3200
Performing linear regression for Chunk 2281 of 3200
Performing linear regression for Chunk 2291 of 3200
Performing linear regression for Chunk 2301 of 3200
Performing linear regression for Chunk 2311 of 3200
Performing linear regression for Chunk 2321 of 3200
Performing linear regression for Chunk 2331 of 3200
Performing linear regression for Chunk 2341 of 3200
Performing linear regression for Chunk 2351 of 3200
Performing linear regression for Chunk 2361 of 3200
Performing linear regression for Chunk 2371 of 3200
Performing linear regression for Chunk 2381 of 3200
Performing linear regression for Chunk 2391 of 3200
Performing linear regression for Chunk 2401 of 3200
Performing linear regression for Chunk 2411 of 3200
Performing linear regression for Chunk 2421 of 3200
Performing linear regression for Chunk 2431 of 3200
Performing linear regression for Chunk 2441 of 3200
Performing linear regression for Chunk 2451 of 3200
Performing linear regression for Chunk 2461 of 3200
Performing linear regression for Chunk 2471 of 3200
Performing linear regression for Chunk 2481 of 3200
Performing linear regression for Chunk 2491 of 3200
Performing linear regression for Chunk 2501 of 3200
Performing linear regression for Chunk 2511 of 3200
Performing linear regression for Chunk 2521 of 3200
Performing linear regression for Chunk 2531 of 3200
Performing linear regression for Chunk 2541 of 3200
Performing linear regression for Chunk 2551 of 3200
Performing linear regression for Chunk 2561 of 3200
Performing linear regression for Chunk 2571 of 3200
Performing linear regression for Chunk 2581 of 3200
Performing linear regression for Chunk 2591 of 3200
Performing linear regression for Chunk 2601 of 3200
Performing linear regression for Chunk 2611 of 3200
Performing linear regression for Chunk 2621 of 3200
Performing linear regression for Chunk 2631 of 3200
Performing linear regression for Chunk 2641 of 3200
Performing linear regression for Chunk 2651 of 3200
Performing linear regression for Chunk 2661 of 3200
Performing linear regression for Chunk 2671 of 3200
Performing linear regression for Chunk 2681 of 3200
Performing linear regression for Chunk 2691 of 3200
Performing linear regression for Chunk 2701 of 3200
Performing linear regression for Chunk 2711 of 3200
Performing linear regression for Chunk 2721 of 3200
Performing linear regression for Chunk 2731 of 3200
Performing linear regression for Chunk 2741 of 3200
Performing linear regression for Chunk 2751 of 3200
Performing linear regression for Chunk 2761 of 3200
Performing linear regression for Chunk 2771 of 3200
Performing linear regression for Chunk 2781 of 3200
Performing linear regression for Chunk 2791 of 3200
Performing linear regression for Chunk 2801 of 3200
Performing linear regression for Chunk 2811 of 3200
Performing linear regression for Chunk 2821 of 3200
Performing linear regression for Chunk 2831 of 3200
Performing linear regression for Chunk 2841 of 3200
Performing linear regression for Chunk 2851 of 3200
Performing linear regression for Chunk 2861 of 3200
Performing linear regression for Chunk 2871 of 3200
Performing linear regression for Chunk 2881 of 3200
Performing linear regression for Chunk 2891 of 3200
Performing linear regression for Chunk 2901 of 3200
Performing linear regression for Chunk 2911 of 3200
Performing linear regression for Chunk 2921 of 3200
Performing linear regression for Chunk 2931 of 3200
Performing linear regression for Chunk 2941 of 3200
Performing linear regression for Chunk 2951 of 3200
Performing linear regression for Chunk 2961 of 3200
Performing linear regression for Chunk 2971 of 3200
Performing linear regression for Chunk 2981 of 3200
Performing linear regression for Chunk 2991 of 3200
Performing linear regression for Chunk 3001 of 3200
Performing linear regression for Chunk 3011 of 3200
Performing linear regression for Chunk 3021 of 3200
Performing linear regression for Chunk 3031 of 3200
Performing linear regression for Chunk 3041 of 3200
Performing linear regression for Chunk 3051 of 3200
Performing linear regression for Chunk 3061 of 3200
Performing linear regression for Chunk 3071 of 3200
Performing linear regression for Chunk 3081 of 3200
Performing linear regression for Chunk 3091 of 3200
Performing linear regression for Chunk 3101 of 3200
Performing linear regression for Chunk 3111 of 3200
Performing linear regression for Chunk 3121 of 3200
Performing linear regression for Chunk 3131 of 3200
Performing linear regression for Chunk 3141 of 3200
Performing linear regression for Chunk 3151 of 3200
Performing linear regression for Chunk 3161 of 3200
Performing linear regression for Chunk 3171 of 3200
Performing linear regression for Chunk 3181 of 3200
Performing linear regression for Chunk 3191 of 3200

Main results saved in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.assoc, with a summary version in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.summaries, p-values in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues and score file in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:58:19 2025 and ended at Mon May 12 20:58:35 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## define p-values cut-offs for thresholding ##

# load the p-values #

# get the minimum p-value #

# make a list of thresholds that are above the minimum p-value #

## obtain scores in a reduced set of SNPs after thresholding and cumpling ##

# perform thresholding considering the threshold 1 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_small_set_predictors_set_linear_clump_thresholding_1_predictors
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 1

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e+00, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

3189219 of the 3189219 predictors in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e+00
All of these are in the data

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading p-values from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr1_858952_G_A 7.7391e-01 | chr1_905373_T_C 8.4775e-01 | chr1_911428_C_T 2.4979e-01

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 25 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 127569; stay tuned for updates ;)
Pass 1: Thinning for Chunk 2001 of 127569; kept 5329 out of 50000 predictors (10.66%)
Pass 1: Thinning for Chunk 4001 of 127569; kept 10152 out of 100000 predictors (10.15%)
Pass 1: Thinning for Chunk 6001 of 127569; kept 15048 out of 150000 predictors (10.03%)
Pass 1: Thinning for Chunk 8001 of 127569; kept 19816 out of 200000 predictors (9.91%)
Pass 1: Thinning for Chunk 10001 of 127569; kept 24341 out of 250000 predictors (9.74%)
Pass 1: Thinning for Chunk 12001 of 127569; kept 28865 out of 300000 predictors (9.62%)
Pass 1: Thinning for Chunk 14001 of 127569; kept 33423 out of 350000 predictors (9.55%)
Pass 1: Thinning for Chunk 16001 of 127569; kept 38004 out of 400000 predictors (9.50%)
Pass 1: Thinning for Chunk 18001 of 127569; kept 42803 out of 450000 predictors (9.51%)
Pass 1: Thinning for Chunk 20001 of 127569; kept 48195 out of 500000 predictors (9.64%)
Pass 1: Thinning for Chunk 22001 of 127569; kept 52708 out of 550000 predictors (9.58%)
Pass 1: Thinning for Chunk 24001 of 127569; kept 57463 out of 600000 predictors (9.58%)
Pass 1: Thinning for Chunk 26001 of 127569; kept 61905 out of 650000 predictors (9.52%)
Pass 1: Thinning for Chunk 28001 of 127569; kept 66677 out of 700000 predictors (9.53%)
Pass 1: Thinning for Chunk 30001 of 127569; kept 71402 out of 750000 predictors (9.52%)
Pass 1: Thinning for Chunk 32001 of 127569; kept 75997 out of 800000 predictors (9.50%)
Pass 1: Thinning for Chunk 34001 of 127569; kept 80163 out of 850000 predictors (9.43%)
Pass 1: Thinning for Chunk 36001 of 127569; kept 84499 out of 900000 predictors (9.39%)
Pass 1: Thinning for Chunk 38001 of 127569; kept 88950 out of 950000 predictors (9.36%)
Pass 1: Thinning for Chunk 40001 of 127569; kept 93254 out of 1000000 predictors (9.33%)
Pass 1: Thinning for Chunk 42001 of 127569; kept 97904 out of 1050000 predictors (9.32%)
Pass 1: Thinning for Chunk 44001 of 127569; kept 102831 out of 1100000 predictors (9.35%)
Pass 1: Thinning for Chunk 46001 of 127569; kept 107193 out of 1150000 predictors (9.32%)
Pass 1: Thinning for Chunk 48001 of 127569; kept 112251 out of 1200000 predictors (9.35%)
Pass 1: Thinning for Chunk 50001 of 127569; kept 116767 out of 1250000 predictors (9.34%)
Pass 1: Thinning for Chunk 52001 of 127569; kept 120556 out of 1300000 predictors (9.27%)
Pass 1: Thinning for Chunk 54001 of 127569; kept 124880 out of 1350000 predictors (9.25%)
Pass 1: Thinning for Chunk 56001 of 127569; kept 129794 out of 1400000 predictors (9.27%)
Pass 1: Thinning for Chunk 58001 of 127569; kept 134072 out of 1450000 predictors (9.25%)
Pass 1: Thinning for Chunk 60001 of 127569; kept 138239 out of 1500000 predictors (9.22%)
Pass 1: Thinning for Chunk 62001 of 127569; kept 142279 out of 1550000 predictors (9.18%)
Pass 1: Thinning for Chunk 64001 of 127569; kept 147191 out of 1600000 predictors (9.20%)
Pass 1: Thinning for Chunk 66001 of 127569; kept 151657 out of 1650000 predictors (9.19%)
Pass 1: Thinning for Chunk 68001 of 127569; kept 155448 out of 1700000 predictors (9.14%)
Pass 1: Thinning for Chunk 70001 of 127569; kept 159945 out of 1750000 predictors (9.14%)
Pass 1: Thinning for Chunk 72001 of 127569; kept 164621 out of 1800000 predictors (9.15%)
Pass 1: Thinning for Chunk 74001 of 127569; kept 169033 out of 1850000 predictors (9.14%)
Pass 1: Thinning for Chunk 76001 of 127569; kept 173824 out of 1900000 predictors (9.15%)
Pass 1: Thinning for Chunk 78001 of 127569; kept 178707 out of 1950000 predictors (9.16%)
Pass 1: Thinning for Chunk 80001 of 127569; kept 183371 out of 2000000 predictors (9.17%)
Pass 1: Thinning for Chunk 82001 of 127569; kept 187459 out of 2050000 predictors (9.14%)
Pass 1: Thinning for Chunk 84001 of 127569; kept 192248 out of 2100000 predictors (9.15%)
Pass 1: Thinning for Chunk 86001 of 127569; kept 196651 out of 2150000 predictors (9.15%)
Pass 1: Thinning for Chunk 88001 of 127569; kept 201139 out of 2200000 predictors (9.14%)
Pass 1: Thinning for Chunk 90001 of 127569; kept 205913 out of 2250000 predictors (9.15%)
Pass 1: Thinning for Chunk 92001 of 127569; kept 210582 out of 2300000 predictors (9.16%)
Pass 1: Thinning for Chunk 94001 of 127569; kept 214928 out of 2350000 predictors (9.15%)
Pass 1: Thinning for Chunk 96001 of 127569; kept 219570 out of 2400000 predictors (9.15%)
Pass 1: Thinning for Chunk 98001 of 127569; kept 224687 out of 2450000 predictors (9.17%)
Pass 1: Thinning for Chunk 100001 of 127569; kept 229534 out of 2500000 predictors (9.18%)
Pass 1: Thinning for Chunk 102001 of 127569; kept 233920 out of 2550000 predictors (9.17%)
Pass 1: Thinning for Chunk 104001 of 127569; kept 238704 out of 2600000 predictors (9.18%)
Pass 1: Thinning for Chunk 106001 of 127569; kept 243046 out of 2650000 predictors (9.17%)
Pass 1: Thinning for Chunk 108001 of 127569; kept 248116 out of 2700000 predictors (9.19%)
Pass 1: Thinning for Chunk 110001 of 127569; kept 253370 out of 2750000 predictors (9.21%)
Pass 1: Thinning for Chunk 112001 of 127569; kept 258167 out of 2800000 predictors (9.22%)
Pass 1: Thinning for Chunk 114001 of 127569; kept 262976 out of 2850000 predictors (9.23%)
Pass 1: Thinning for Chunk 116001 of 127569; kept 268441 out of 2900000 predictors (9.26%)
Pass 1: Thinning for Chunk 118001 of 127569; kept 274074 out of 2950000 predictors (9.29%)
Pass 1: Thinning for Chunk 120001 of 127569; kept 278846 out of 3000000 predictors (9.29%)
Pass 1: Thinning for Chunk 122001 of 127569; kept 284289 out of 3050000 predictors (9.32%)
Pass 1: Thinning for Chunk 124001 of 127569; kept 289439 out of 3100000 predictors (9.34%)
Pass 1: Thinning for Chunk 126001 of 127569; kept 294504 out of 3150000 predictors (9.35%)

The bit-size has now been set to 124

Pass 2: Thinning for Chunk 1 of 2409; stay tuned for updates ;)
Pass 2: Thinning for Chunk 401 of 2409; kept 15565 out of 49600 predictors (31.38%)
Pass 2: Thinning for Chunk 801 of 2409; kept 31673 out of 99200 predictors (31.93%)
Pass 2: Thinning for Chunk 1201 of 2409; kept 46706 out of 148800 predictors (31.39%)
Pass 2: Thinning for Chunk 1601 of 2409; kept 64356 out of 198400 predictors (32.44%)
Pass 2: Thinning for Chunk 2001 of 2409; kept 80791 out of 248000 predictors (32.58%)
Pass 2: Thinning for Chunk 2401 of 2409; kept 101801 out of 297600 predictors (34.21%)

Thinning complete: 102387 predictors kept (saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_small_set_predictors_set_linear_clump_thresholding_1_predictors.in), 3086832 lost (./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_small_set_predictors_set_linear_clump_thresholding_1_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_small_set_predictors_set_linear_clump_thresholding_1_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:58:36 2025 and ended at Mon May 12 20:59:27 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_small_set_predictors_set_linear_clump_thresholding_1_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 102387 predictors to extract from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_small_set_predictors_set_linear_clump_thresholding_1_predictors.in

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 102387)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 6 covariates for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1.combined")

Performing linear regression for Chunk 1 of 115
Performing linear regression for Chunk 11 of 115
Performing linear regression for Chunk 21 of 115
Performing linear regression for Chunk 31 of 115
Performing linear regression for Chunk 41 of 115
Performing linear regression for Chunk 51 of 115
Performing linear regression for Chunk 61 of 115
Performing linear regression for Chunk 71 of 115
Performing linear regression for Chunk 81 of 115
Performing linear regression for Chunk 91 of 115
Performing linear regression for Chunk 101 of 115
Performing linear regression for Chunk 111 of 115

Main results saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1.assoc, with a summary version in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1.summaries, p-values in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1.pvalues and score file in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:59:27 2025 and ended at Mon May 12 20:59:31 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1.score
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 102387 predictors from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1.score

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 115
Calculating scores for Chunk 51 of 115
Calculating scores for Chunk 101 of 115

Profiles saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.2557 to 0.8757, saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:59:31 2025 and ended at Mon May 12 20:59:36 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1.score
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 102387 predictors from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1.score

Calculating scores for Chunk 1 of 115
Calculating scores for Chunk 51 of 115
Calculating scores for Chunk 101 of 115

Profiles saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1/vo2_change_linear_clump_thresholding_1_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:59:36 2025 and ended at Mon May 12 20:59:41 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


# perform thresholding considering the threshold 0.1 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 0.1

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e-01, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

329152 of the 3189219 predictors in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e-01
All of these are in the data

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 329152)

Reading p-values from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr1_962184_T_C 3.8165e-02 | chr1_984039_T_C 5.5111e-02 | chr1_1094994_G_A 3.1236e-02

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 20 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 16458; stay tuned for updates ;)
Pass 1: Thinning for Chunk 2501 of 16458; kept 8202 out of 50000 predictors (16.40%)
Pass 1: Thinning for Chunk 5001 of 16458; kept 15891 out of 100000 predictors (15.89%)
Pass 1: Thinning for Chunk 7501 of 16458; kept 24244 out of 150000 predictors (16.16%)
Pass 1: Thinning for Chunk 10001 of 16458; kept 32256 out of 200000 predictors (16.13%)
Pass 1: Thinning for Chunk 12501 of 16458; kept 39787 out of 250000 predictors (15.91%)
Pass 1: Thinning for Chunk 15001 of 16458; kept 48417 out of 300000 predictors (16.14%)

The bit-size has now been set to 25

Pass 2: Thinning for Chunk 1 of 2140; stay tuned for updates ;)
Pass 2: Thinning for Chunk 2001 of 2140; kept 22809 out of 50000 predictors (45.62%)

Thinning complete: 24761 predictors kept (saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors.in), 304391 lost (./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 20:59:41 2025 and ended at Mon May 12 21:00:06 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 24761 predictors to extract from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.1_predictors.in

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 24761)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 6 covariates for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1.combined")

Performing linear regression for Chunk 1 of 35
Performing linear regression for Chunk 11 of 35
Performing linear regression for Chunk 21 of 35
Performing linear regression for Chunk 31 of 35

Main results saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1.assoc, with a summary version in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1.summaries, p-values in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1.pvalues and score file in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 21:00:06 2025 and ended at Mon May 12 21:00:10 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1.score
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 24761 predictors from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1.score

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 35

Profiles saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.2557 to 0.8744, saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 21:00:10 2025 and ended at Mon May 12 21:00:14 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1.score
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 24761 predictors from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1.score

Calculating scores for Chunk 1 of 35

Profiles saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.1/vo2_change_linear_clump_thresholding_0.1_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 21:00:14 2025 and ended at Mon May 12 21:00:18 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


# perform thresholding considering the threshold 0.01 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 0.01

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e-02, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

33631 of the 3189219 predictors in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e-02
All of these are in the data

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 33631)

Reading p-values from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr1_3972702_C_T 7.7385e-03 | chr1_4061687_C_A 8.3819e-05 | chr1_4163015_G_A 7.7134e-03

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 20 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 1682; stay tuned for updates ;)

The bit-size has now been set to 20

Pass 2: Thinning for Chunk 1 of 341; stay tuned for updates ;)

Thinning complete: 3774 predictors kept (saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors.in), 29857 lost (./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 21:00:18 2025 and ended at Mon May 12 21:00:41 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 3774 predictors to extract from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.01_predictors.in

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3774)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 6 covariates for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01.combined")

Performing linear regression for Chunk 1 of 22
Performing linear regression for Chunk 11 of 22
Performing linear regression for Chunk 21 of 22

Main results saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01.assoc, with a summary version in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01.summaries, p-values in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01.pvalues and score file in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 21:00:41 2025 and ended at Mon May 12 21:00:45 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01.score
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 3774 predictors from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01.score

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 22

Profiles saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.2557 to 0.8668, saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 21:00:45 2025 and ended at Mon May 12 21:00:48 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01.score
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 3774 predictors from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01.score

Calculating scores for Chunk 1 of 22

Profiles saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.01/vo2_change_linear_clump_thresholding_0.01_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 21:00:48 2025 and ended at Mon May 12 21:00:52 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


# perform thresholding considering the threshold 0.001 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 0.001

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e-03, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

3461 of the 3189219 predictors in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e-03
All of these are in the data

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3461)

Reading p-values from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr1_4061687_C_A 8.3819e-05 | chr1_7770202_T_C 3.4849e-04 | chr1_7776738_A_G 5.1147e-04

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 20 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 174; stay tuned for updates ;)

The bit-size has now been set to 20

Pass 2: Thinning for Chunk 1 of 40; stay tuned for updates ;)

Thinning complete: 492 predictors kept (saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors.in), 2969 lost (./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 21:00:52 2025 and ended at Mon May 12 21:01:14 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 492 predictors to extract from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.001_predictors.in

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 492)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 6 covariates for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001.combined")

Performing linear regression for Chunk 1 of 22
Performing linear regression for Chunk 11 of 22
Performing linear regression for Chunk 21 of 22

Main results saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001.assoc, with a summary version in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001.summaries, p-values in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001.pvalues and score file in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 21:01:14 2025 and ended at Mon May 12 21:01:18 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001.score
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 492 predictors from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001.score

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 22

Profiles saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.2557 to 0.8182, saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 21:01:18 2025 and ended at Mon May 12 21:01:22 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001.score
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 492 predictors from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001.score

Calculating scores for Chunk 1 of 22

Profiles saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.001/vo2_change_linear_clump_thresholding_0.001_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 21:01:22 2025 and ended at Mon May 12 21:01:26 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


# perform thresholding considering the threshold 0.0001 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 0.0001

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e-04, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

183 of the 3189219 predictors in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e-04
All of these are in the data

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 183)

Reading p-values from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr1_4061687_C_A 8.3819e-05 | chr1_22351153_G_C 9.6411e-05 | chr1_22352632_C_T 7.4123e-05

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 20 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 10; stay tuned for updates ;)

The bit-size has now been set to 20

Pass 2: Thinning for Chunk 1 of 4; stay tuned for updates ;)

Thinning complete: 47 predictors kept (saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors.in), 136 lost (./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 21:01:26 2025 and ended at Mon May 12 21:01:47 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 47 predictors to extract from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_small_set_predictors_set_linear_clump_thresholding_0.0001_predictors.in

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 47)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 6 covariates for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001.combined")

Performing linear regression for Chunk 1 of 16
Performing linear regression for Chunk 11 of 16

Main results saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001.assoc, with a summary version in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001.summaries, p-values in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001.pvalues and score file in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 21:01:47 2025 and ended at Mon May 12 21:01:51 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001.score
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 47 predictors from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001.score

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 16

Profiles saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.2557 to 0.5959, saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 21:01:51 2025 and ended at Mon May 12 21:01:55 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001.score
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 47 predictors from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001.score

Calculating scores for Chunk 1 of 16

Profiles saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_0.0001/vo2_change_linear_clump_thresholding_0.0001_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 21:01:55 2025 and ended at Mon May 12 21:01:59 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


# perform thresholding considering the threshold 1e-05 and then perform clumping #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 6 pairs of arguments:
--thin-tops ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--window-prune 0.2
--window-kb 1000
--pvalues ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues
--cutoff 1e-05

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Will identify predictors with p-values less than 1.00e-05, then prune so that no pair within 1000.00kb remains with correlation squared greater than 0.2000 (predictors with higher p-values will be excluded first)

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

27 of the 3189219 predictors in ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues have P <= 1.00e-05
All of these are in the data

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 27)

Reading p-values from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_linear_raw.pvalues
First few p-values are: chr3_187336176_T_C 6.5203e-06 | chr3_187336418_T_C 5.9398e-06 | chr3_187336496_T_C 8.4797e-06

Will prune in two passes; first with windows of size 10kb, then 1000.00kb

The bit-size will be set to 20 (you can change this using "--bit-size")

Pass 1: Thinning for Chunk 1 of 2; stay tuned for updates ;)

The bit-size has now been set to 20

Pass 2: Thinning for Chunk 1 of 1; stay tuned for updates ;)

Thinning complete: 4 predictors kept (saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors.in), 23 lost (./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors.out) and 0 trivial (./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors.trivial)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 21:01:59 2025 and ended at Mon May 12 21:02:20 2025
The elapsed time was 0.01 hours
Given the command used one thread, this means the CPU time was also 0.01 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# run again linear on the reduced set of SNPs #
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 7 pairs of arguments:
--linear ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv
--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
--extract ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors.in
--permute NO

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Performing linear regression for one phenotype

Will adjust all predictors for covariates (use "--adjust-predictors NO" to not adjust predictors, or use "--adjust-predictors PARTIAL" to adjust only the most significant predictors)

To perform weighted linear regression, use "--sample-weights"

Will compute standard test statistics; use "--spa-test YES" to switch to a saddlepoint approximation, 

You can use "--top-preds" to include (strongly-associated) predictors as extra covariates

To perform quality control of predictors use "--min-maf", "--max-maf", "--min-var" and/or "--min-obs"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Checking responses for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Reading list of 4 predictors to extract from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_small_set_predictors_set_linear_clump_thresholding_1e-05_predictors.in

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 4)

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Examining 1 factors for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv
Factor 1 has 2 distinct values
The factors will be converted to 1 indicator variables

Reading 6 covariates for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv

Note that a combined covariate file has been saved to ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05.combined (this contains both the quantitative covariates and the indicator variables corresponding to the factors; it would be equivalent to repeat this analysis replacing "--covar ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_cont.tsv" and "--factors ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_covars_factors.tsv" with "--covar ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05.combined")

Performing linear regression for Chunk 1 of 4

Main results saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05.assoc, with a summary version in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05.summaries, p-values in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05.pvalues and score file in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05.score

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 21:02:20 2025 and ended at Mon May 12 21:02:23 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## do the calculation of the PRS in the same set of samples ##

## first using the phenotype as input ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 5 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05_prs_calc_with_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05.score
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--pheno ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 4 predictors from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05.score

Reading phenotypes for 1018 samples from ./results/final_results/analysis_full_data/vo2_change/vo2_change_small_set_predictors_set_transform_subset_response.tsv

Calculating scores for Chunk 1 of 4

Profiles saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05_prs_calc_with_pheno.profile

Correlations between 7 scores and phenotype range from 0.2557 to 0.2557, saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05_prs_calc_with_pheno.cors

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 21:02:23 2025 and ended at Mon May 12 21:02:27 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



## then without phenotype, so we can check the PRS calculation of input reposne and covariates and hence we do not need to add covariates ##
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
LDAK - Software for obtaining Linkage Disequilibrium Adjusted Kinships and Loads More
Version 6 - Help pages at www.dougspeed.com
-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

There are 4 pairs of arguments:
--calc-scores ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05_prs_calc_without_pheno
--scorefile ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05.score
--bfile ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing
--power 0

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Calculating scores for 7 profiles

Please note that ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05.score is assumed to contains raw effect sizes (e.g., those generated by "--calc-blups", "--linear", "--ridge", "--bolt", "--bayesr", "--elastic", "--mega-prs" or "--calc-pca-loads"); if it instead contains standardized effect sizes (e.g., those from "--calc-posts"), you should use "--power -1"

If you add "--pheno" (or "--summary"), LDAK will compute the correlation between scores and the phenotype

If you require counts (how many predictors contribute to each profile) add "--save-counts"

To run the parallel version of LDAK, use "--max-threads" (this will only reduce runtime for some commands)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

Reading IDs for 1018 samples from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.fam

Reading details for 3189219 predictors from ./data/plink_filesets/small_set_predictors/vo2_change_filesets/vo2_change_subset_missing_clean_maf_hwe_sample_snp_missing.bim

Data contain 1018 samples and 3189219 predictors (will be using 1018 and 3189219)

Reading details for 4 predictors from ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05.score

Calculating scores for Chunk 1 of 4

Profiles saved in ./results/final_results/analysis_full_data/vo2_change/clump_thresholding_1e-05/vo2_change_linear_clump_thresholding_1e-05_prs_calc_without_pheno.profile

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --

This command started at Mon May 12 21:02:27 2025 and ended at Mon May 12 21:02:31 2025
The elapsed time was 0.00 hours
Given the command used one thread, this means the CPU time was also 0.00 hours
Mission completed. All your basepair are belong to us :)

-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --



# check the PRS is the same #


###### check we have used the correct samples in both analyses ######

## load the FAM files used for training and test ##

## samples in files generated by elastic net ##

## samples in files generated by linear ##

###### plot quantiles of PRS against phenotype ######

## prepare folders ##


## load the phenotype data before transformation ##

## process it ##

# get only the samples finally included in modelling #

# split the ID into FID and IID #

# check that the new variables has been correctly create #

## open the plot ##

# Create a figure with 8 subplots arranged in a grid (e.g., 2 rows x 4 columns) #

## Flatten the axes array for easier iteration ##

## create a list of models ##

## iterate across models ##

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          8             -0.750              2.0              5.9250
1          7             -2.785              1.0              3.1775
2          4             -3.500              0.3              3.2000
3          1             -9.875             -3.5              1.3000
4         11              0.375              3.4              5.4000
5          3             -3.950              0.6              3.1250
6         15              2.375              4.2              6.3000
7         13              1.825              3.6              7.3500
8         10              0.075              2.7              5.6250
9          5             -2.675              0.4              3.5250
10        12              0.850              3.6              5.5500
11        17              3.225              5.3              8.0500
12        18              3.550              6.3              8.7500
13         6             -1.600              1.3              4.0250
14        20              5.400              9.0             16.5500
15         2             -4.400             -0.3              2.2250
16        14              0.880              3.8              6.2325
17        16              1.350              4.2              6.3750
18        19              3.350              6.9              9.1000
19         9              0.300              2.2              4.4250

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          8             -1.550             1.90               4.500
1          7             -2.740             1.35               3.810
2          3             -4.325             0.60               2.525
3          1             -9.875            -3.40               1.300
4         12              0.850             3.50               5.250
5          4             -1.300             0.60               3.300
6         14              1.790             3.80               7.540
7         13              0.750             3.60               5.400
8          9              0.300             2.50               5.750
9          5             -3.600             0.60               3.525
10        17              3.225             5.10               7.750
11        18              4.050             6.80               9.200
12        10              0.075             2.20               5.400
13         6             -1.675             1.10               4.125
14        20              5.550             9.00              16.550
15        16              1.350             4.20               6.225
16        11              0.600             3.40               5.625
17        15              2.300             4.10               6.600
18        19              3.125             6.70               8.800
19         2             -4.250             0.00               2.300

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          8            -1.0500             1.90              4.1250
1          7            -2.7850             1.55              4.1325
2          3            -4.3250             0.30              2.5250
3          1            -9.8750            -3.40              1.3000
4         11             0.0750             3.40              5.4000
5          5            -3.6000             0.60              3.3500
6         15             2.1500             4.10              6.2250
7         12             1.3750             3.40              5.7000
8         10             0.3500             2.70              5.6250
9          6            -1.6000             1.00              3.5250
10        13             0.8250             3.60              5.6250
11        17             3.3500             5.40              8.5000
12        18             3.3500             6.30              8.6000
13         4            -1.3750             0.90              3.2000
14        19             3.5500             6.90              9.1000
15        20             5.5500             9.00             16.5500
16         2            -4.2500             0.00              2.3000
17         9             0.0000             2.20              5.9250
18        16             1.3500             4.10              7.0750
19        14             1.8125             3.75              6.3000

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          8             -0.725             2.10              4.8750
1          6             -2.600             0.90              3.6000
2          3             -3.725             0.00              2.6000
3          1             -9.875            -3.40              1.3500
4         13              0.425             3.60              5.9250
5         14              1.600             3.80              6.4750
6         10              0.000             2.50              5.7000
7          5             -2.900             0.60              3.3750
8         12              0.775             3.50              5.3750
9         17              3.200             5.30              7.4500
10        18              3.125             6.30              8.8500
11        11              0.375             3.40              5.4000
12         7             -1.455             1.45              4.4325
13        19              3.550             6.90              9.5000
14         4             -4.325             0.60              3.3750
15        16              1.350             4.40              7.3000
16         9             -0.075             2.10              5.9750
17        20              4.450             8.80             16.5500
18         2             -5.400            -0.30              2.2250
19        15              2.300             3.80              6.2250

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          9            -1.0000              2.1              5.3000
1          8            -3.7750              2.0              5.8250
2          2            -5.0750             -0.3              2.7500
3          1            -9.8750             -3.3              1.8500
4         14             0.9475              3.7              6.6325
5          3            -3.7750              0.6              3.2750
6         12             0.2000              3.3              5.7000
7         15             1.3750              3.5              6.9750
8          5            -4.4000              0.7              4.1250
9         18             3.1250              5.7              8.8000
10        13             1.5500              3.8              5.8500
11         7            -1.5550              1.5              4.2775
12        19             3.1250              6.3             12.1000
13         6            -1.6500              1.7              4.0250
14        16             1.7000              5.0              7.5000
15        17             2.3750              5.1              9.0500
16        20             4.1250              8.2             16.5500
17        11             0.3000              2.1              6.0500
18         4            -3.2500              0.6              3.9500
19        10            -1.2250              3.1              4.8500

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0         14            -2.5600             3.70              7.3875
1          9            -4.0500             2.60              8.2750
2          8            -1.5250             2.10              6.7250
3          3            -4.5000             0.60              4.9750
4          6            -3.7250             1.50              5.8500
5         12            -0.9000             2.70              6.6750
6         10            -2.0250             2.70              5.7000
7          2            -3.7500             1.30              5.1750
8         17             0.9250             4.40              7.9750
9         11            -2.8750             2.80              8.0250
10        16            -0.5250             3.50              7.2750
11        18             1.8000             5.00             10.2250
12         1            -8.8750            -0.30              3.5500
13        13             0.0000             3.50              7.7250
14        20             1.9500             6.90             16.5500
15        15             0.3750             3.60              8.5000
16        19             1.7000             5.70             12.5250
17         4            -4.9500             1.30              5.1250
18         7            -4.0175             2.75              7.0750
19         5            -4.3000             1.70              5.2500

# plot the results #

# load the PRS file #

# merge the PRS with the response variable not transformed #

# check merging #

# calculate 20 quantiles based on the PRS #

# Iterate over each quantile #

# convert results to a DataFrame #

# Adjust quantile for better visualization (1 to 20 instead of 0 to 19) #

# print the results #
    quantile  lower_ci_response  median_response  higher_ci_response
0          8            -0.6600             4.30             12.8600
1          3            -3.4000             2.60              7.3000
2          1            -4.9875             2.10              7.5000
3          7            -4.4050             2.80              7.9450
4          5            -5.1000             2.60              7.6500
5          2            -3.6000             2.30              7.6275
6          4            -2.2750             2.45              7.8750
7          6            -1.3250             3.80             11.8750
8          9            -1.6775             3.75             10.1875
9         11            -3.5800             4.60              7.7200
10        12            -0.2325             4.05             14.4800
11        10            -2.4700             3.25             12.1900

# plot the results #

# finish the plot #

###### manhattan plots ######

## load assoc results to pandas ##
         Chromosome           Predictor  Basepair  ... CallRate MachR2  SPA_Status
0                 1     chr1_858952_G_A    858952  ...      1.0    NaN    NOT_USED
1                 1     chr1_905373_T_C    905373  ...      1.0    NaN    NOT_USED
2                 1     chr1_911428_C_T    911428  ...      1.0    NaN    NOT_USED
3                 1     chr1_918870_A_G    918870  ...      1.0    NaN    NOT_USED
4                 1     chr1_931513_T_C    931513  ...      1.0    NaN    NOT_USED
...             ...                 ...       ...  ...      ...    ...         ...
3189214          22  chr22_50749145_C_T  50749145  ...      1.0    NaN    NOT_USED
3189215          22  chr22_50749470_T_C  50749470  ...      1.0    NaN    NOT_USED
3189216          22  chr22_50749890_T_G  50749890  ...      1.0    NaN    NOT_USED
3189217          22  chr22_50751123_G_T  50751123  ...      1.0    NaN    NOT_USED
3189218          22  chr22_50752652_C_G  50752652  ...      1.0    NaN    NOT_USED

[3189219 rows x 13 columns]

## check we have the correct columns ##

## check we have the correct dtypes ##

## calculate -log_10(pvalue) ##

## convert chromosome to category and then sort by it ##

## sort rows by chromosome and basepair position ##

## Creates a new column called ind in the assoc_results DataFrame ##

## Groups the assoc_results DataFrame by the Chromosome column ##

## make manhattan plot ##

# open the plot #

# Add a title to the plot #

# Define a colorblind-friendly palette #

# iterate across chromosomes #

# set the x-axis ticks and labels of these ticks #

# add p-value thresholds as horizontal lines #

# set axis limits #

# set the axis label #

# Increase font size for tick labels #

# add a legend to the subplot #

# save the plot as a static image #

###### compress the results ######

## remove bed and bim plink files initialles used as input (compressed files already created) ##


## elastic outputs ##


## linear outputs ##

# first raw outputs before clumping #


# then outputs after clumping #







#######################################
#######################################
FINISH
#######################################
#######################################
